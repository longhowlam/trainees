<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Longhow Lam" />


<title>Machine learning in R</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,%40font%2Dface%7Bfont%2Dfamily%3A%27Open%20Sans%27%3Bfont%2Dstyle%3Anormal%3Bfont%2Dweight%3A400%3Bsrc%3Alocal%28%27Open%20Sans%27%29%2Clocal%28OpenSans%29%2Curl%28data%3Aapplication%2Ffont%2Dwoff%3Bbase64%2Cd09GRgABAAAAAE8YABIAAAAAhWwAAQABAAAAAAAAAAAAAAAAAAAAAAAAAABHREVGAAABlAAAABYAAAAWABAA3UdQT1MAAAGsAAAADAAAAAwAFQAKR1NVQgAAAbgAAABZAAAAdN3O3ptPUy8yAAACFAAAAF8AAABgoT6eyWNtYXAAAAJ0AAAAmAAAAMyvDbOdY3Z0IAAAAwwAAABZAAAAog9NGKRmcGdtAAADaAAABJsAAAe0fmG2EWdhc3AAAAgEAAAAEAAAABAAFQAjZ2x5ZgAACBQAADWFAABReBn1yj5oZWFkAAA9nAAAADYAAAA293bipmhoZWEAAD3UAAAAHwAAACQNzAapaG10eAAAPfQAAAIIAAADbLTLWYhrZXJuAAA%2F%2FAAAChcAAB6Qo%2Buk42xvY2EAAEoUAAABuQAAAbz3ewp%2FbWF4cAAAS9AAAAAgAAAAIAJ2AgpuYW1lAABL8AAAAKwAAAEyFNwvSnBvc3QAAEycAAABhgAAAiiYDmoRcHJlcAAATiQAAADyAAABCUO3lqQAAQAAAAwAAAAAAAAAAgABAAAA3AABAAAAAQAAAAoACgAKAAB4AR3HNcJBAQDA8d%2BrLzDatEXOrqDd4S2ayUX1beTyDwEyyrqCbXrY%2BxPD8ylAsF0tUn%2F4nlj89Z9A7%2BtETl5RXdNNZGDm%2BvXYXWjgLDRzEhoLBAYv0%2F0NHAAAAHgBY2Bm2cY4gYGVgYN1FqsxAwOjPIRmvsiQxviRg4mJm42NmZWFiYnlAQPTewcGhWgGBgYNBiAwdAx2ZgAK%2FP%2FLJv9PhKGFo5cpQoGBcT5IjsWDdRuQUmBgBgD40BA5AHgBY2BgYGRgBmIGBh4GFoYDQFqHQYGBBcjzYPBkqGM4zXCe4T%2BjIWMw0zGmW0x3FEQUpBTkFJQU1BSsFFwUShTWKAn9%2Fw%2FUpQBU7cWwgOEMwwWg6iCoamEFCQUZsGpLhOr%2Fjxn6%2Fz%2F6f5CB9%2F%2Fe%2Fz3%2Fc%2F7%2B%2Bvv877MHGx6sfbDmwcoHyx5MedD9IOGByr39QHeRAABARzfieAFjE2EQZ%2FBj3QYkS1m3sZ5lQAEsHgwiDBMZGP6%2FAfEQ5D8REAnUJfxnyv%2B3%2F1r%2Fv%2Fq3Eigi8W8PA1mAA0J1MzQy3GWYwdDP0Mcwk6GDoZGRn6ELAE09H%2F8AAAB4AXVUR3fbxhPfhRqr%2F6Cr3h8pi4wpN9K9V4QEYCrq7b2F0gC1R%2BXkS3rjKWXlfJeBfaF88jH1M6TfoqNzdWaXxZ0NM7%2FftJ2ZpXfzzeVILi0uzM%2FNzkxPTU68Md64GQZ%2Bvfa6d%2BP6tatXLl%2B6eOH8uVMnTxyvVg4fGisfhNfcV0f3luz%2F7Srmc9nMyPDQ4IDFWUUgjwMcKItSmEAASaNaEcFo069WAghjFIlAegyOQaNhIEhQxALHEqIeg2P0yHLjKUuvY%2Bn1LbktrrKrOgUI%2FMUH0ebLc5Lk73yIBO4YeUrL5GGUIimuSx6mKl2tCDD8oKmCmGrkaT5Xh%2Fp6rlphaS5PYp4kPAy3Un74OjeCdTi4nFosU6Qg%2BqRBsoazczLwHdeNqpVx3AW%2BoVjdhMThOo6YkGJTl862RFq5r263bbYSHyuswVrylsSBhHzVQKDU11g6hkfAxyOf%2FDVKJ1%2FHCvgBHtNRJ%2Bb7eSYepeQ4VLZBqAeMjgM7%2FzyJJF1kuGw%2FYFpEq458Xrr65YTUa6VCEKGKVdJ%2B2FoBYYNKCwV1K6B2s1mJnPB7Ww6GtyO04ya%2FHHWPHs5P4J65NyVa5VA0E0LocwPci45b6tvMvohm1BYc1h12Xd2GrbbHVkjB1pzs6IKtOHeYd%2BJYhFasmfs9Zt%2BSZlo9pu8eg0utWZAKB8vjaxBQx7cSbK3Qdr2nBwM27vrXcUHtLolLJyJjK3CAbDcFDo3hsPZ63IH2RrsoWyskdB47jiKitFtcAgqj4wQQxN3PB81RCiCo0Y1jnUVYlOj5JHhJd2JBevIEeSQxDWzTN8PEE3AL90KtP11dVrC5II1L1w331pHFq10vPBGYeyUCFRvB7PAEzMltdubhb%2BlZ4dw9w86yyNfG%2B%2Bu0ZWOBkmsb%2BGrsrKGIN4R0XPQimnAEcj3CI6ZDR35zzHJEZlcW5cQCTMwty4umkB5B4ajHwVNhQDqdMLSAmClnhLScgYgMbQJESALUrtIvjpQz9LVxuIPSiYgQkjusZ01l4BERrPtdO9KfDErKQLne6EUbJlXHqTccNzL163tuES26ickjo5va6FIkCyIyaFEYA%2BlejuqlFxLWIYKmQG9W0tlMe0yXu80wPe%2FOavEJrd8srSFziSal30wMj5H2mH7T6H218RQ93qOFysDEgtLBoRuQUeXjyPQKexdLjoa4vtAQJiBsEXYutEo9T1%2Fm5mUdBMbXFCzIq8Z6Yl5%2B7nyic%2B1mE3xisVatpBarpcC%2FmUs9%2Fs3Csty2GRPfLMo7FrfqcS1KDxIntwVjnkEtjRJoFKEVHWmelIyxd7Y9xlqGHTSA0VfbnBks08M4W21bHczuJBrTiYixiBnsMF7PepCwTAdrGcy8UqZb5uWGvIyX9QpW0XJSrqE7hNzjjGU5u1vgRe6k5DVv4DZvpVnP6Vi0yMKLOhUvPUq9tCzvFhi5mV9KVNMvWpfRJg1bggjEml6Uz6KmiiN92dh%2BGg19OHK4TmOC61TIcAFzsF7DPNQ0fkPjNzr4sMZHaEX5fk7uLZr9LHK9AW9KF2wU%2F%2F%2FBUfaOnlREfyrK%2Frv6Hyn3ISkAAAEAAwAIAAoADQAH%2F%2F8AD3gBhXwHfFRV1vg5974yvZdMQspkSIYkQkgmhdAyIIQQWsSADCLSpajUiMgiAkuJNGmhKyJGDCyybCiyiGBHRGQtyLIuf2UX19UPy7oWyFz%2B972ZBxOE72N%2BL2%2BYd%2Bbe0%2B5p99wBAscBBIN4ACjI4D4oUJEIVAbIL8wPYX4oP1TQ3um3%2B0v5dZz2bj44nsyKLhYPXKkaL1wCAhuuXcQ69dsWyAu7qF5PBMFqQzQRkzQgYvIQCuXleXYHlCXl2x1YZg%2BF7HxMDNAQLQoVetwuKZCZjRUTQqc%2Ff7RjebisqAeuEQJXmpZUdA%2F3KgcgsJA2kL1xDNPDZqCyQAWdXiIy5YOHThUq4%2FKB1XFpgPr5heVtJuSQvJzxOeKB6HfEplzKWCEA4Sc%2BVgqkw8bwIF16K7fg0ttNJr3DajEKBqfT5UlNkwXJKyD4hCRRlFySwU%2BTvTTJkJTh1wkms6l%2FpBWa08Fmt%2FWP%2BNz2AWYcYEez3WwXvU5qECE%2FVB5ylJXl5993Hyc3zw6hkHaPoerldxVjh7eMX%2FF3hYWxu0KF382pcKpXsV%2B9QlS93Mj%2FSz%2FujinsVE1dDTszcEk1u4LpPdjXmDdw6UAsqFlUg7rmf2J%2Bd3aGLmC757GBuEe55mHNXGxifZVrLtuNNUBhwbU6wSQ5IAOyoS2MCxcH7VmpXkHIdZlFP4BPtOvFdvlZZsncL0Kl1pZcS99Iam5eK1erfhFvrkviL9HDKc5X6OV%2FChUq7aGEvw5U6QuFVCbEhOSSZHegODM7WOzxhOzZ2cVFJaXFIbfHK2cH7WlELuK3EnR5vHZJEkzvHZw35S933n0ucur5ky%2FMO7SraN2mrVuqGiNPnIt%2BNnTy6HF4fMkfvf%2B6EEjfkpWPh7rtXrJgp%2BNAk9hzQScj6194%2F%2ByxlZE72Ow0KvcdloMLbPcBiDD%2B2jdSW%2FEk6MENfk55AfQMtwabaPC0aZWZ2a6Nob1NKgxRc3qemb%2FaF0jtk3xZPtkpc4Xjr3KVXE7WDfpi%2BsfVJ1RotwUyJVFVbE4ZV3JUPi0pLsq%2B%2BXMM4A9Vd%2B%2FYcXcVvrtx7bLN61av2oINVTU11dU1NVV4cuPaFRvXrV7xDGPNH6%2BheQJpbMQaHLiz8R9fXb5w8dLl5vO7XnzhD7uef37Xxa8u%2F%2F3ipa9pxpUqrt5AYeq1b8QPxVNg5BQWw13h9k4PpEqB3Lx2eW0DlmxfqkdfUhoy9Y6EnNZgW0t7MZ%2F6smlubka%2BI0NfFckQoDwPkjih%2Bd4yrpTleTdRqoinJE6Ts7AULcTt8mRxQbYjMeLcXMpYwucgMgaCkrrMn668Z97YBwZHJm%2F%2B%2FhnWZ%2FKwOzazl5c2DerS%2Bo2Xth9eshXXd7jTu7NHHeb98%2BVHfqw%2F%2Bz%2FCmp5zhvSZe3e%2FkSOubt2EO3tExnWrrbsy%2F51x94%2BaWFa%2F84V1k%2Fbfx2Z1fWE0%2B2It%2B2zfxGEfAaBiMbBctRiug0CpIBLFUpyK2R%2BOumYgYrZB%2BcZAdoT4%2BTfM0CpsksEggGCxGoNUsV4J5sVpc5SGJE6pwxvIJgM3r97%2B1Kq1S7et2UQKUI%2Fv7znOCn%2F8jpW80ohvKaN24aOatFEFAx8XLFYDFYItR0UbkQMljuIiEgx5HMS0efW2pWtXPbVdGZb9yjruPIInv%2FsR3z%2F%2BEisAhMFkrmCRXGCB9uEUKgoomw16o95qEwxoJiaT2cDtl84CUP5G4XWJOTBmWLK8olOmNOjMKhUpWZWHK5LZgl9279229we2OBUX50kuVjv5QDo7PBwnsvrhWJF%2BYDIuVagZDxeFHOF1MEKbsBMEQS%2BKJjOVdXJ1BKw61EH%2BfeqSTzTz3I7ZA3Zuv%2Bwhshy3sDFL2TjctJR6n2SDsfFJ3A0I5ewXfAgugw7s%2B0XQG0SAfFVWHOEsr6TyphSHW5NHFc9J6Wa%2B7B3Dfp42HguHAUINniPlZCpQ%2Fl0CogDIrW%2F8u85iv7sGv8ZzGzYAxjwV%2FMCxTwobJQCTWU8HRPQeruaaXpRqestVdUOXso7dupeF7px4Z8%2Bed3arKFc44AIg51W9ch4kIIiUEocmSk4sBpCcj15oUDRJXYYExl37RmirrkIv55rLASYJJF%2BS3t0nopeptU%2BE%2BmLrLK%2BlPgQyid3mCBU6UP1rVz8R2n770zc%2FXf7x8s%2FNn9fvaFi3rmFHPfmMLWRP4lycho%2FjNPY4W82Os88wiJ34K4tdAIQjAOQkx8YArcM2PaAOjSZBL8uolzAJFFvGDXd8ej67P2AvKpUkOYghcnK7zl300RBcsExwzJ%2Fhbrd7GuYBwhgAIYtbTx%2F3%2Bd4klJ3gtKCQnGIz9InYZEzqG8EkjSzNavCB%2FcXYlcQshhyMsZrI6PYLWc3lOG%2FvlA4rHr%2F3uTFD3r38%2Fr%2B3fMKOke9W4oJ9G566u7au84CpOz%2Fct5R99wF7W6dIYjjnawrHIAh3hlungFOWgXoyzVKbHOr1eD19Il6vISsrrU8kSzbY%2B0QMGpdjgYh60zDTHJKHoyP4404pw27zB4o1o62gq%2BBLL299am8j%2Bzv774zj995%2FdgTOZsOfWr3rnTWPj2h8qGbo1%2FM%2F%2FkYYvmxfms7TtPrM54E7ns4vwBw0rFy%2FaNJjRRVTet31OgCBPABhongUDOCAzuE0h6gnxChToCJ1ulB0iH0jeqvscFBZotflk%2BhMQ5oJDqhrC%2Fl%2F%2FFxmAUlGYeK5Z6Jl5MDec2yJQdc%2Bl5ViNduL1avoZ805eGll04jy6COKheT8S%2BU6kQwdw%2BlW6nPpXF4qtEoBziwAye3mMnRLkqlPRLqZdQlsKxTcLghkqhzjrLL5M%2BWgUwldSkjbL1HPLrCf51d8MHbv66zu%2FmcGl5Kz0YNZ0%2Bmcf759kbEB29qGGrZiYWop2b2R9fYqnKnlWOVzqXqgNfQIB5LtRr8fQLLT7CyT0ZLaL2K0WFzU5e0TcfmojkckcgvcyhJ4pNlr8Bd63VyEhIbiGhfIBFGTq8R9lqcWB2Dl1G79Rn%2F9i8n08OU3L%2F760UX2E369YuvqVUPrI9VryFR8CXc5V%2FrYefbW7svv%2FYNdxUHv%2FOnFVQ1V8yse2Dde0UcAIY%2FzU4L0sA1FEQg3jJT0jVAJFBlqbOOrALk1dCOmkuHNF%2BmpaKOYunHhldNAlZhEyFGpz4R20C%2Bc47Vmu%2B6gqXo9lewuq5TfXrLnZORk9Ink5JjAlNwvYvJBoF8E5N8qd9nN3jrmj7mOx8OPLDXqolpgwv0zZkpuzaeTynf%2BvWjNvnr22b%2BbsfDJR7%2Be%2BcL6dQ1bXlu3CDvOWfHIMytnrhJPHt7x4L7eg%2F48%2B8C5U0euLuu%2Ff8ozr1xteHTRssdGru8V3kwfeHTMsN937%2FzksLEzFdlO5NQpNsMLWdAtnJlizzQYAAQu26AljUvWZbEQlyuJi1Ymcr8Iaal2jjKNg5qJ9Ctqx02jMyDFKHJw8TpUIvjHKhXZQlZ0%2FIwe1eO%2B%2B6%2FRVHpg2mv%2FuPbBuguPMtfKLU%2BtuXfjkIFraEVzg2tlMuZg6O57%2FvXBP1C3kZ3H9od2PPV81RMVE%2FaNAy3HEcaokRS34Ta%2BLAA8XotzQMRiizkRDVfN87X0JXae6NzkVR6Znehb6J8XL%2BY3IKovXMjn0oEDMrkmmc2iXu9yGm0DIkab6hgTZklwj%2FT6FDccpXsmn6Rjlxv%2BknyrTFMR8%2BU%2FcF9%2BDiRwh%2FUCiChwdeXD58cDhSwsRjeikNNcTo83%2F0AtP2DDKLywji1nhxSezMTjgo9eVHOy3LBbJgIQ0OsEsToiIFRHrIjI4wHOlfxEz6a4ZOTXTLq9eTjdTofW1bEH6up%2Bg5GIBDhGEr2BkRNVlMZTa%2FP3HKVyrMMKrF3H%2FKPYUAWjlGsXaRnXrxTIhrJwqp%2FbMtnphFYWIdgGoLWtddqASGuPzdA7YhNaqFZLvVJSEa48LZwUd4YSN4mJ%2Baq%2FctSSXgtmD6gf2emV91%2F9KNj38bHd9l3PX0tq19dMnzFw3OSsgsWjj%2BzqPXn0w4On3e9nZ%2BNJLYFZ1yqkQ2ITFEM5zzwyA%2B1KLJ1kVwpAjsvSTgx3S%2BrQQeiisxv5Ky%2B9kGbnqUmllmSFEhOP6%2FG4ug6C2nJQUPdSt0td36R1IFMgbsUalrqlQAbw4KK1v1BwIH%2FudKqm8NCQbeMHP2LUtVk3rv7Fb4712N3Tt%2FDeaWvZt3%2B8wA7swe6Y%2F5cvjv3I1rHJn%2BAyhLM44ODVn14%2F7bBUDpq%2Fhpxb8c388XfdM%2BrU3veu%2BTws17Pv7O79aFvzMnvxc3aaHRq8sAZX4jgUsP7CfvYntoNhGYquJiAAAKJNPAIyWLjk0ojFqENR0SwqyILNaiG9I0bRYhFECoKD518xh6iplZYz%2B5W8H0OIlBsz%2FtURB6IHmnaT7itJORvb6A94cnbjGZYvHrnSg0zENwfPGTGddQIKJwCEo9xyW8ALGdA7nO0UUg1Wn89iEGQLjwd01iRrUlXEarWAxVcVsTjAWxUBevt4QnM9%2FgxBMbluwe4SAjxpj%2FmcgN0ef3cCt2IAhVVLsR%2F7%2BTIjjZjU9PTeY1ew4I9%2FOvhn8cCeI%2FNf9BnK2Pk3%2FkZ7TF00%2B6HoquhndauXPAGAMIdb09Oqr8gOu6jFpbdQb5IDekccglHi%2FHK2DL%2B4emRymUNIE3%2BRo3WokKfbtNP37Cs0%2F7rxjQ0X2Cvs2Rex%2FNNLuysbxBB7lX3FPmdvl64rwyU44QusOVSzuj8AUTgmDuEc04FdsYcWQQ8COJyiuSoiUsFSFREct4ppwc9rSBlA%2BZuAPZTBx2Az2Uo2CY%2FhIHysic%2F1z59PI%2FdU5CtWz%2BaJB9gi9gKmYebVKZgHgMq89Bc%2Br1GJWSSDAQXQoWAyS%2FreEUlCQsTeEUKRr3B03DZmUZBwxy%2F6S%2FMZmh%2BdTYZHt5OF4oH1LKc%2BeilhJj0UhpMlAKQ6pAbjTRPxSW45Q0CbAac3asPzwaNfrY9LTuyi2ilOhUvnI8SSohNapUJK7wiAaDLZe0dMgujtHRGdt4%2B8%2FHaphRyV9%2Brq5lT1xe9nfPc0a2IrDuKQL%2F%2F9bve3DrL%2Fso%2FQj0kbVrGXCYuWZWXjUhzzD7xn%2F%2BD6GvYau8Q%2BZe8H8LUY7WK6yuVQ2KdHBJ0giCCaTTraO6LTiQaJoshJV81RgnG%2FQbydi5f%2FDYnpjc2ssZGSRrI3Ws1z7dXkYQC8NoLNxfFqVpwaNht1OotVT4GzFDJj9GrpGI15%2BJJiPpxLMg0v6dVv9AONx9jclFWuR6fyFGvI0TNxvRC%2BUjHmnkjBViRGg4Ix0Yn6RGzLWkgJZRVRDKHw1TvRrzc2NpL1J6JN5M0l0dc5snnk4%2BjCBF0QIT1soQCCJCMFzgtw3EBXxTekkO0%2B0aio0pV%2FbIp9V%2BKIgpPrUZJOFCUev%2FJSmsuNBjuVjDK1gKQgp2DnLbuZlRjwuJUAn2MY4nce4COtZjadZSsCntbhh6zRomMm0bbpo%2Bbh4oGrVQLPOume7Uev%2FBCXo1IDsUG7sFsvcaytVpDB7jBS2aqjKCdypaUI4xPzabNJKZdj%2BWvNn%2BtsW4%2FRVB2xkGeEk582NR%2FnE3ZMwaxy2guAqFp99FZ5bu%2BIXqDW3hHqvLVNiOltBiTmueJRtpW9oZgjHIE9sBOOujo9%2Bv1%2Ffvn5h%2F9Eeb77LHuYa%2B94HIt1bArbxs6yU1iIuRjEAnYqZp%2BE8erqdUBRONnA%2Bc75DE6XQaiKGAySLDuqIjKVEtavhpXmSgW%2FmlplYChutYXx7Ay7tLsRZ5PWUePGL949euKoYPr7t1HOh2jK6mdXrVC5wHaoXLBCCp%2BZp8MeAIEa%2BOqmZtns6x0xC7KTL2yZM%2BMtlRs3J6I2pViG8q258sX7OOxndrH0tpz5ki3rzuqxivyf%2FDnN%2BWMCN1SGs8yIxKS3y0aDQdYTwePVm8EMVRGzmVDK5UepkSi6cntnp2Ku8ktw20SOf5bGNm4BcRXyGdhfcfkJ9jQ7%2FVXTzl2vfEZGRLeJB94%2Fzf4%2BLjqZjFi9cuWqJwDVHIFw29ha4V6a0wSQ5BSFrGxTGvV4uH30CFSfoEoJiY4mt0CGlozy8D%2Bo5jgx%2B6jmBbwy4BEI%2B9d3rHnZ0I%2FGN%2B7usnL1ey%2BxM389WLx%2F1%2BINHRbWXfoDLjz%2B6Z07su%2BYN73vyIFFvd959sV3qtf2nfFA35F3FQw8AoDgABCGcv7JvJ7iABSRUp1epgK3CYLmFeJ5qGYSi7k3IEsbWYFQyQrE9PWqJzjM14yPj2OHrLDdhgYZZafDrqOCmQ8UpzGUuFzsLkUnVHMYs4uij%2F2F%2FcJfFxrfee3ld8QDzf2vsC8wo5nuaa44%2BMabh%2BghQAAA4XW1%2FpMcNqJgMuooCJQqiPLlrxWvQhjgF8%2F%2FSgXTwej3O6M%2FNmF1x8zWHdVaFh%2F5uU3bnwXkmg1yXz6aT6km%2BQwpyW6LRdQn2Q0U9TGTotqUGOKqNclWAjJldKcyenwSZ0h8cyc75y5CT3v2xU42u%2BnL9p6UYpSa0Nne7yy%2B1EQ%2F7PaW6%2Fdbm0N88llHNx18ic5qnrv59RXv0YUK93QAQr1q9QNhhyCJ3ORLiskXFJMvtDT5KhocAz63Yu7rj%2FPIY0oTXmKdjuAkfHg%2F60QWROeQZnI4%2Bgq5M9oX4lybrUY5GWGrIBJRpnoDiChTUeOcJmE%2BqKL%2BGCJdcNEhlrSb%2BQ6T8%2BR887zoCZJPFyv1ZQBBscZ6pWKmQyqDLKBgMIoCNwcUdUrMcuuKmVot8AvlzU6qi9roq82%2F0LSFwoaNC69OAIQGdoRMVnSRY2mRUFAYoxcJlTDIOdBSfeJRD5nMSvEEu4B%2BdkS6svyKX6HWC0A%2Bi1c2Kd5c2XRy3h0mgYbo%2F4spg%2FKNEDuCzdrMFFACSacHOUgFevPMXj5rMb9CfMoLfOrSA%2BKF5b9KyigFJCgExOMgQVJYD1TWiQQEwrO%2BG5rpVFUTC3DfaPxsA1vG9pEg3dQ8jnwV9QJea2Zv0k3XKtUKsJLHIlEqwBgjmU%2FLQUfRp9mbCwCxTjhHHZIf9OA8AILRID2BkJ%2Bs1ZoxwDW1OMStBHU83G1fm5MZ0%2B4QzhUdK3f33F8MRKk50lPCUEXzoVc4K1NnTEvz%2BRw6yqMpYkzrFSFGI7jd1ooIt4LJFRHRA24o%2F98LVH4tX7NllapJZ7zS6LZn8QVeLKsVKjrQrxv43GPPvUychyc%2FVveH0F3HR77xCrNs%2FmPDWy89tOWB3js3Y1%2Bb1GPe7Jq5dxTuORZ11TZuHC3LD00fOhwI7OVWtVZygRPSeVUt0%2BD1Wq2mVGqiGX4zmNwOu8HOhccRljzgqoiArYV5DSXF1SDB1sddEk825YBijeRQiVcrvHAqyJ5Pv%2F3%2Bk0l%2F7GwKzGzQ6Wa811i%2FqXFjfb0wlJ1jP%2FDXxwMGLpdcbNHcsTuWvv7ll29fOPPJXwAQpnMOLxWGxbIaK6VuPU3ySmaOmQ0cHDPPzVmNGM9qlJ1DHgNzu6hmOGTcZXYV9f8d8HTbUOn8QrbvuW11Tz3swiw0oRPvyPQu96Sywe9%2B2mlNGRBlVqGU88fB%2BdM97E%2BVvGCx2CV7ht%2FhtgIgmqhez9mjt1FnRYR6bscerSYTkLTqvTcUDPLPA6osi%2BJOiG7ST%2F%2Fn2W%2B%2F%2B%2BTCTLMsNCxmTzdu3Ny4evOmNS9gNlr5647tA%2Frh0V%2B%2Fmfny%2B4Gv3r54%2Bi%2BfxLF0cN44IRk6hdOTDF4jpdzqtkrxGit4uRskyaUyyqIw6paZQyiRZQ632%2B%2BJsUuivNbh53Kb%2Bx%2F2JYp%2Fe%2F%2B7qFl8eecf%2FzBk65bfb7WQLstc2AZl1GMH9v3fJxx%2Fp2pttp%2F%2Bc%2FeGrS8oUksFoBYpHVxK3cVlMjkJ4UaSuj0GvhQMgKIsVkScspUqq0GtY98IAxWmOZS1p2QNgeJSXkPW3DX3mE%2BzrxreeANH3lObN6LH8KHopW83l9G3%2B3TugmsDC9PnPNkLgEKQuYQCzplcKIVu8HC4a56vQ5YpvYtY4ESnSHIzW6Vn%2BQzd72xlLbYWV0R0nXpFDJm6XKvOqvPk5pJekVxrm%2FJekTY2T7teEU9KnHUa%2Bzj%2F8pXd%2BrzbxD1uragaVBdAqDC%2BjaAUkrJv%2FOXKcGMXmJOnbhQXF%2FF3QsHJVnf87VhB3sSqoa%2Fte5X9jf3r7FdPzMgtC%2FccNOnTtwb3ZPb6ZWdOPLzh7amPD50%2F4z8%2F1T4uVE5ICkzt9ewxXYdBbfPqVx54ddvqMauTndXFnYfmBnY%2B2PS66ypEhs2ZFOn5IO08%2FZFvfn4cEPYCCD24nnuUzM5i0nFz7dF7vEkWvcMhVEQcNgOA3q0Y7xjlCatesVT2mALbtRUfM1P06cfm%2F%2BGZhgadoWD%2FjBMnyJuLfn%2Fkk%2BjrfHXnDOow4N5XP4gWAxDYDoDjxAtAwcr9tZ3PJCDa7Ga5MmImVlQ04%2F3EwqZSIqAJJVQc3NDQ1CG3TceObXI7CJWYU1Zc0qFDaSkAubaKudSxTZAEd4Q9TqPRrNP5kj22yognrLcC1z6ISzW5xSTOhATTljhb3v2det7Zv%2FeNGZnLt9g16B6h%2BaqNHZHv0yaP8TSV89QGJTzetxgMRqNOEkSdYHeYAGw2nY7KRje1xiKGfD5zeUyFyuJsRTUiQi0bdclYkzcER73JeuD5E2zOnB07dKSgy2icydpGlxLpQTZOcjW%2FXTo9NjcO5nNT4GQCoiASQHfca2tMVBjHYVRo6SRfJQGoCAfcdruDiz%2BgdwRo66xWHrfb4RPMPm5p0302p1UPDkUPuCLEt534Igi1bHVIVIgEzfAqepHh1bRDypryyOa1DVNmblnVsDhFl79rIuIAXcHhmYdfJicWLNj3cnSLcv%2Fzx9HjQmV99dDDg8e8%2BheuMZq2cnxdUBBOApeiri69x23S22xcWW02g%2FV2ytpSV72Jmrp7m4JG6NDUt95RNPXwJ%2Bq8d0XUSWM2dhSfU9EknsU6wSyDnOwzeLgds1GbYvxvmcVylSHFilGFxE4PYRT74fKaf%2FwOTZcvobX5lZ3PPffii88%2F10Cy2I%2FswyeR%2FAFNmMfeZ1f%2F8rfzH545p1j5vdyW1apU%2B6E8nOEzCrKsS3foHJkBwQhWq7siYrXprboUaHXDzMdZ0GLBqpaeO2hPAhMUr62Y%2BgRHrThpU8Niry7c%2BPBf%2F%2Bf7yzvryabGFc8%2B6xowcMRg1kUqqh9azT5h%2F1GcNr14%2BGTWl29fevfUeYVXHNNSlVexqMKW6qHJyT6bL8OfnOK1pqalecxOp8wtv80MFRHz%2F%2BY2VT5yJ1l63Ul6r3vQ0njtQyL9GzaIW15cvXnjnI8uf%2FfJ57P0SQsajObpM%2Fd9mHXp3YunT59birloRDO2a6z%2F9T38eEzFCzE9okGOpw1ywy6zXm8wEF4DsZrB4FYtg03rc2nRkaE5IY15ZEfvjt4eRQtfaahz6rrsFoaZNlk%2FfTbaJFSenDQjlrnS6XyW1twOtIplrqLzeuZaEfHYJKq%2Frj%2F5t8pdueG5kbsG25Hfpq50%2Bj%2Fe%2F%2BtjA%2FbXzF82%2BdmN88r%2FevSPL3Z6ftEjj7Yds%2BJ13jSzsaHnpjbt7h4Uvrdr2aAH%2ByzaXLm4R1W3O7p2KO71FCCkX%2FuG7BQrwKPWJlwu3jPioEKS1%2BC0OXtFLGGbVeaCkj1xU3kqIVjV5ONWqo52xVGXhtxKNuHyEMcdA5NSJuSy17ZurRiBXdlrw2vN8lyzHQeQZdU9%2F83mRWePngiAsIOvrjKhElx8fh86ZZPJ4DS4PSaz2aZzWdVV7TFqEbMS%2F4daVmW0rJcrhBY127EvX9TPNNQl6UP7Z7zztlAZLeMO6GMSvnpozV2Dj54hp7RcjgiVau%2BHAQ0ms6hHK6jhiJZl%2BNX0NFTicIYQt7ER%2B76ptuiMte%2FtYyP4oI%2F8o0cx9iPtrx6K5UpSgI%2FWinsblz4lNc3rsZipYBZ0yQ7ubnTuxCyYK7c2A1U2Z2Rlk8LhUHSq1BmbsoRPKeSfcBbp2qSdPsY%2B3jNxsk5nLHCcaHqjg0snBF7dzc6QBZ3OvHR%2FdK5QyUaz6j5l%2B4tJbXTp7trW9eRvHClACAIIOpXGzLBdFiVAUWlxQZ3RLaD1pnQ4ngmjmhUfYgteQT9m%2FJktwFVH2Cn27hFSQLxsGO6IfhU9jUdYD0AgfL1LfHw3z%2FsVMqnHK5jB7OBLO0UHfIJCVam1GRJo46KKOdrSUrLvuwFOnfnuS%2FtYTsWfl%2FStKu2xq3cXzuCVn9wf%2Bpn87mrGy5vtC03HtkAsZ6YPCZW3yJl7RUQr6npF0P2%2F5cz0oeZ%2FksHR0%2BTL6D5y31Q6eN685sPxrixetlPl5%2FYlJxu9AFbZRbmnpqlpTq09K3F7TdV%2FbpXcPJZTfEtxCddDvj7d3EK4ZLfHjedrpx794PFH58%2F49MClCxdM44aRZaRxE%2BaPjywnw0Zg4ebdS6Xj7NzZoCl4FhAvMxuZrfluorSo0RSABN%2BtlHzx8nKeJv3cDAiV7Ijaw5Oq4OwWDQ4H8UFqqsXiE2laujso0QScEzYFFXSDxYr7U7DPVNCV5Dj2pcRw4eKhDx%2BZ%2F9jjp45OnvHwVFIePIvB49LSPRvZ%2ByPvJcsjvOq5cRenZNg4zJn2qEvdpyXVQg6tAS%2FXAzu1JvkcpuoIdVglCaojEuTngS3pjfw38rSkOlOZT8nQVNOmbD9lKoU5HFg8t2TMUz2mRrqPyi95omTcisrHK%2FsMJSfuLFn%2FUKvsVinhsvqH%2FRkZSeoOPFuKdcJwrcuYCALV8343AGpSu4xtNPOWXcZcCQNO1%2FXt0PNKk%2FGszp3Ly0IVZPfVC2Lfxb3C5ZVhQDjK7fd5dVemazjNozNTahCARxo62irVJxKnwUz4SzDKgg%2B07k9ljt9sw2apra1KOJCldLR6NAOuqD89OWHNwpPHcdniPisKChY%2BtHv7My8sX%2FFdifTO%2Bxlov4LNXXfvoH7vstCH5z462QkQypUYSDzBpV4Zzk5y6s3mZI%2BdGD1OMS3dlORL6h%2FR%2B3xOcNr6RpxJIPa5uRWkRdPQzZ6Nm29lf5Lfinl2ypuduEqQxqONXTatnD0HG9jQblU05erVU2%2B99f%2FEEzUL%2B%2F1uGTs397MxS%2B7YtDz%2FxwtzsfO%2BU4psZqMkeIVtnHNByAibW0GmBSxtctLd7iwZeNSYn1gJchaVBku9il8r9co82Ja9clCxDnKwNLs0IXQ6VLV4%2BOLx8%2BeOq7t%2FUVXVgmF14%2BYuGrN42MKqeVtnzHh627QZW8mHj01aNmxh794Lhz059ZEFD%2FCHvfj7JZN%2BN2XbM1Onbd8BiscDEJT9Fw8MDrdzWGSj0WYS9URPTS6LW%2FYmGSwW2So5HBScbqsz3UmsTqvThG7JlATlWg%2B33RHrzL7lpjuGUOGj1uaovjBEKnH2HjYCJfY6dmGv72BvYGd%2BARu7j1wgZ5vZ3Ma57Ec08RslQBKsgaxUVYkkUR726QUqUDlmFjgmiYqtbgjFLYRiI5p%2FYebmnxVpXPuF1kupUABdeGdcdiE4pdy0Dj5fmkmCgNS13E07lbRqK%2Fn1%2FmCviN%2Btt%2FWK6OGGznh%2Fs4t9I39VVFmLztSUlwuwZdCiRC2l%2FKk33lG0dHD%2FqprTbw5%2FZmTxqMV9Z8yYvelw%2FcCqjf%2F%2B6K9P9H9t4KLl7R%2BcvmJR99W%2Ff6Ggbs3LPQbRnMF1WW0mD5q1NDW4IJjSKdy5prTH%2BklDl%2BfctXrZxm5rs9r27dWuY8e8oqHTRvWb0MVZPfnuKWXOMUCwWLTQ8eKH6u5TWpiTanKAI8lnpW495N90QCAhzctKeI%2FFxVnZpaXZWcU4pzgrq7Q0K6tYnFrUrl1RYUFBYfwOQGEM7xzvEdt5hxKeSwWDXmrNT0936a1esbSDZAKH1ZRuIuCwOYjJYXKk5AWcoRQByhNPBdhblgFRMxHuG90bnN2obu8KDjc3eYHM1py5DiFU2NqhNXTQOXMWz10weE77sRWvffDZq0880vHB5vXv4PB3les1tv2D02z76xP2YNvdezD3pT3s7N497JOXhMCeTTu3t%2F2dq9X3n575qfMjIXZI%2FQ7b%2Fu6brOGD0zj0rT%2BwD%2F%2BwB3P2xr8GQKCCushU8W1OdzqUhlt5pRQDokeJazP8rQwGh88D1EYJNTvSOakf3feGku9qVGpqG4xTV8ojfbXWGSt18iYUtdZJXEnDlt0%2FedPztWvHjM%2BbtnB%2BHauecmLUlAeov2bk6HHjJkhCcGFoRIcJs1jnI2OaCgRBqd8NhFraSI%2BCBGbICTupxI21YNTrBbMkWKwmUYegHGS5WbPRiyhjVuw2EAfPVEriM1kjLsUhtexzTK9lO0kQ1%2Fdk29mzvXB9yo23qh9EHfeDXhAhJWwiKKAki0J1RCSQr20nattixUJOXfM71Bv9Hhc%2BCdeuaV3LRAIbAAjXdUoX16r7wqGgF3iOLui5Zpn1JodXKu1gsnFoi9Pi0DmtjnQHAR63E4fT4bythikCCP22ZKVVoUS%2Bhp0Bqm51Fnr%2BL2UjHz5YPXLwfRNx36B%2Bl3eeXrwWxYbNVy%2F8n%2BpGrtwd7tNtSfXsNFaLo9jTdPZ89ub%2FpXB47YrkEiRpzW3r%2BoJ09UfBJLnmAoG5dBi5LJ5U83Z%2F2GIGp7L7nGwzHPNQhS3J7yWaAKe27LkytvA6c%2FfPn39g4Oqa%2Bfun195VPX3qwLunC2vmH9i%2FoGZlTdOCgdOm3l0zdZoiv%2FGASic8yQYLAMhwBiA6Q93NqCLLub9OUmpcstOLaHGCwAsItnQvZqjyadHEUVx6cz%2B0JMt%2Bsjy645vIQH91edGont0XbPj9msiaPXiIVI2%2FNHhk35IePbMLh0yeP6V6%2FZPPA4KflKlzBqAsnGkVRaCONIPUOstxn%2FMhJ%2BnrRKMzxUmcTl2yP92s88eVhKvIfTe2KDHRmKtlyd%2F2PpPpA3vsPbRzw4w1sz%2F8snbmA6Or7%2Bw%2BpUPP8mXDl2wVvqx%2BwJu%2F%2FYmVHWb32L5q0oAeXXrkBYa2LZl5056LnkfvwhP6xD0X5YAIN3pyAOvaT85494494cnCD133dnN3O1oEqNZDegiV4IHicLJoMOhs4HS6dC6%2BLeC2ulLMRKks6LWkMWHX6XqfaELKyMnTOhsGs13PNCxJNkz%2BZ%2F0Qg6GhAeewK698pKaNLwyr2caOScrsU1mzMEJygRWCYYcgIoBopDa7TidSq4jaQa%2F8RJkG7MortqVTEvILI6Z9PL1rzacn%2F%2Fov0pY1S3t%2FraYhx5WrKDBA2ED6Yh0dqvitsEECMJuofkCEQsyAJOqq2jzatUOseZR82L1nz%2B7xMwlZzIVNAOBQIge7xQhgUfrILXa7jtog%2F71CzQq3qDNoZYbSkOzBpo31obZtOw24a8BDQx4ubWIXRk7UT9S1Kckrtu%2BbHgSEvqQKP1d3kPleHwFKDSZuX2mGBGlK3sc5EGO7FpnEzw8MXLlQ8pQsvpNv4K4ld9471NP2%2FhFAoDt1kaPi26q3zgo7lONnEnBvHfMfbr3iP964r4XTTjgzJSYsWHJ0V%2F3qF3eu3%2FB8lN07fsKwYRMeGCZM3nHw8LPP7T%2Bw%2FTH%2Bb%2FYjjwCBau4hdsY9BF%2BZRr1AgMrEoJdu5R%2F4fBhELEUxdqM72c5aTGef1%2BIQVnvjPTGxCb3wfhzek01IufGW24c%2BAOIZzq8gnCYLACAbHrsGKMNHNDV6EPR%2FosTBA8ziYuCw7Tjs%2BThseQz2CwV2Ou3PYeV9xMZBVchkAMkvnuAQM34FFf4CxEZ9KD5qXmxUIBBiM2mNMBxSoY3Sba1zpQWwlbVVwCXk5EIqmmhqKj93lzEgkm2zG3tH7IEWecP9w%2B9rGZ4ohslCYnXDUm9MGF2J0ihbnJBfkf59Rs7q4vv9Y9X1ozq9%2BdbRTwPhSMnYbk2zOnXtXqqkXKHH1tZM7NOvw5ip2e0XjzjcWDEhMjB%2FyIz70jFvcU%2FeGRvmVKrdoPJ0bltbq9R1v%2FYaDgTdn4hNzIa84ltA1MLCGETS7SCOQSAGkdoSIv86xGsg3HKMrOsQE6CUQxiaKGmtgtyAkWIwIMNxKIN5QK4xAIk3MIIVnNA%2FfAdPM%2BwIOhPaRNEtuvROycm7kHm7iMHM7wabASUqOtByowkglmHm5an5G8bOiYau9y%2FSAF7vYVQ2zqR5UUeUXdxLDtMT0SMkNXqR9Lhag0cfURpetbZG%2FAvZr2jRHOZSOkc5ztkqzrMIAf55rM9N5VmbON8PqhxBs8aRmyFqoTwG4b4dxLFrV2MQyS0hsq5DTACHylWC%2FhhXgUA%2BgFip9id54Z5wod3t1glmAKcgCUk%2BrogS11erXC6%2FJJ%2BWL8jcIsuyoNfbqiJ6Kri17tNEXW55EDWhHZV7uVhLarxnM5QhVqpNqbM3bcJ9eBf%2Bbn%2F07S9xNlt4lIyKtaWSunqyntWxHSQcba5nhhhNYrmqS%2B3jurSmJdWx7jiVLwUx3sKsmLb5bgdRi4YYhP92EMegKQaR3RIiX4PgeGy65RhZ1yEmwMdxnW4b5z7CQrQJJmEDGMEX1st6ino0mXXgy0%2B0x2rMHLeOu0ewbTh8BHua7RiLw9m2MThS2DCa%2F3fbaLyfPTsaR%2BCIsWwrAOXzv877434CJ6RAQFkZnnRvmsAPExtcAA6rqFMCF0%2Ba32f2945YHTpRoDazQHnjnES1lrm3%2BFq4%2BYgL%2Fygm0lglwc7fxSoM1BZEj3qKzovZ1zsLv1479tEH9ykddGe2jnx04rGmh6Mjpu%2F9zy%2FNwbFk68SdWpPhmOUDNr2FDyl9dMMXV699l61D26bmvgOVZjp2ZRN9qTc7xVdOrI9LlUxpXLoVMfk7Nb7fDFELp2MQKbeDOAZzYhAZLSGyrkNMgA3xlRNMtEfCbHWUTvF5CmKjOFSQeO%2FfrHjvH9%2BpMOtFUbKDBB6vWeALiC8fs96sl2LdkZoVarkRrHVH8v9lCDcaJGexM%2BzzQ42NZ9GHnuYrO3mL5LvvUdvFy4zXWq%2FB6ei%2FV%2B5Y9yQAqv0oW6R0aK94ppxcMTUAXpMJUu25YkGhw5Hbrl12RaQd5LrV3S5tj%2Bvm0xpaZCBL2vZIQjWCo6Q2%2F2lnOTKUqE%2F1UYJv5ZAOKb36Lxv32p%2BOTCrfUnn27ofnjujZq094yVz2TcPf%2Fv7%2B58IPi6dX3OnPyC0L3b917LZdPTcF8w%2F0mVQxcHZN%2BcTisqHF1YMuXO0r7Nv3562c52pXkOTnPL8TACXovgLUVWlXOH6L57V56vN2t3t%2B7FP1eajFc%2FGz689fe%2BUW3xc%2FvP58whegruiOKsCNGRZehzj%2BcwyiTQwCqAIhKbtXOVDENWdkOJQLre3tedlIaF%2BWlJTe3ghi5y4pbYNtKyK%2BAqGgV6RD66BdECyZQU%2BxzqKriLgsNtBaO9R97viBxZsNL1corarUot3Jy%2F%2BqHSkOv7bLFExMz5TiAMaaVIb%2Fwg7NmPnUc0VVb4%2Ba%2F3xO8a6Hj%2F0reqcOO967tWbwurHswpy73lz03Mt7Jg1ZtfPpwzvoK7OWGon8BOY%2F%2ByddrEUqp%2Fie%2B4eMYP%2F9%2ByRWGwjyVpav5k5sXH9%2F5MVNo2XdQ6Sw4ektO5V1zXc4lW4kzreeMU%2BJFaqnVDtxVIn1ikl8vyqRVppEbn5e21993vp2z4%2F9rD7PafGcS1R7PsEQk1d7TaLX%2FgqAo9URXolZHHYXKGOgqI3xIgApTICovZYRgzDHIa79iUMMSoA4xl6IQTg0iG84RDrHQ4OYwA4CqBbHZ9d89VRlx1zyq6euqsJ5fsnUqhXwYN5jsTttkj7YRp9eETFSj91nsfLIR0%2B9LqSttY3QmLJw6%2F3b430QyITiIlAqxdlBMcj%2FlHpUk%2B6gRVqnV4kwil39%2Be%2FsK5T%2F9sUYXdkp9n3vr4YN77ll3OW%2Bpzc8v7NpC3vppe0vPUtC7Ev2FzR%2FcQmlWcInr25%2BcGHXgtrefZ6cNHMlm8b%2BtaaRbXjh4Aku21jXgbraqmOrzaLyJC1RNqNUrt0Vk%2F1HquySb%2Fe8drD6PPN2z4%2Bp45Ngi%2Bd8fu35a9%2Ff4vtcJtrzCSkx3Wh3fS2Ph2YhR9gJVO1CD4WTPAaDTSACKjsZTifKZjMqJ%2FQQ8tX1yhOfG8nPjUN6iccXE96Pp8ejezqVFHXsFCrqot3J8iefZP%2Fq3KW8Y1m4nPwYfwOUY3tEGCUsjvv7PvxEa3orl8vQ6iZn76u47uxt1M%2Bb2Kjnf3P2ZWVxBdGcfXw7QXSpTl4Si1SnX6L2X2yaUjNt%2BDw0Xd40o6Z25NzmV4rxTJ9pvAljfYjl95r63Iuxboyetf0XbEBQGjL6zuy7cMOvu8aRRcWffLRjTHRO6DzXjNjutSq5e2KSf0PVDI8mmZuf107VNOfWz4851OeBFs%2B5ZLXnE%2FyxtZarrfrYDqw6wr2xGWIjpKsAWu%2BI2t%2BVyXex0jOkFJfNZpfsrQMOsKeYPHqqT%2BNdjB7q5euvRZPnb3oYUWsXUUomXo%2FW9JUVbx7J4HugOKR748Sz333%2Fyd8fMwk63mSElTs38OYRzF9LmyID2Efsvwpjn83sV86KdcDaFQ1NOXQi58u3ce%2FZMxo1nF6Nmgn7Y%2FTmxejV%2BpuEyuv9TaJArLfsb%2BIw6gkU6UvxFLggHe4Ot0uSrE5nKpjtqZKY4bc6eDxpBaOR51hGGj%2BVwg8UUAc4b5zk4det2ia1fWVJO2TlvZF9aafq7NnSl1EYN4y9zJ7BYRgeN5RaonxdR8%2BRfs09fmXXEH%2Becs89LqzDiTgeF3ljSZmwlZ1m55QTGn6hNi32qy1yujAU0iAXCmBQuG26zkI8nqx8t7tVlk4oDOW1Mbbh0RHvSCKixdiunWg32pIyxcyKCIieFj7YoVjVRAeseV9R9a0q5rdyvYktTFkxnyvWs%2FNzup6pu8B%2BROnrBae6djz2%2BInL0aAOq4Y%2Fe8%2BQDVf9G154buPm5xvWCb3mrjKRjN%2B7vp4xEwtQh3q8Y%2Ba0KbPYz19MYDO5tw1mkLIPz3985rOPP%2F10x9NP7wBEE68Q7pH8YFF6wGWwWXmN0KJs3CSfKkwsE%2FIgzx1QzhIE0DR3nLfB89CcmUMWLuFF2u%2BWPJGTu3C%2Bt3TBoiIAgpP5iG2lhdp%2BkEMyxSpMejflw753u9KSrHUfcfpp29njxj46a8zY3z3YPRTq3rmsqJu4b9TM2lGjps8c3qFLlw78AkQdn%2Bk78TN1N5wPn%2BSzg2gC%2FnKrZc73En4mKLYb3o4vKU6BwvQ0olRTQpJEXXkDB%2FTOLAxZRpmn39tucP%2FKjIL21tHmqcL5rLZZnbvMquO3Tl1n1aldEci5Ff%2FFEyCCePMvngykw%2BK%2FeMIh5f8VUtYgffQ49lB7%2BR0HUNTpQenhP6WBBkscHEs5y%2BQZ1WF29yx63DMUTVyicNM3RdTpRZly061Rq55Od5RisXIk%2FbGKDPGARzmLjqmfcouq%2Fe4LkcAKAEQZizSpY1khOWwS0KwXbHbQUZP2M1%2Bx3pUgbyrhA%2FvjeGG9tcNjs9M6maNnb2B4FnXTeR1Tw7TF6DZldL0ZRcHuMIs2WRn9LW10DWe%2Fei9JQJ4ELUkjOsxJ7m6%2BQYbnXvbTY2Ow6D6FHh%2F7lTTBZZSVLOtqB8g4iCCHzeZK%2BdC1Y38ymWJ3vb5SBnteXszG7cAfyXB6EYzgPBD%2FURrIP3Wr6u%2BOqQ9OmDF94qRp5JtZj%2F9u9sx5C%2Ficym8TiHvgB8gGOwAEwU4c%2FM4nELJA1RaoJelK5ZPTbBAIlYikk0WuCInpvPM3e2CJ%2B16ASv2UpGqjUBAIkMRRWhRNSeqtK6QAyGYBkJXxUyYgEkE7ZYLxAQJIVjbPWkkXx4%2BZIJRzr1gnnuT0TQ2Xp3rTPZ5kI5Hl5NZ2wZDslYJtjN4kb%2F%2BILklMTUvtHyFp1rT0tPw0qqdJaUlpzsxM6BvJlJ0W3iDhg5ZN3bwwdMsfKruRW2ZQbuRlt9evdcorVpPyolGwuJT%2FdUDsCHUKOz4AWfRHQvA065Z1snHLxtW7%2FoddaNewgZANO4LY%2Bn9OPN%2BrQSxmD80rC7ed1%2FRm9%2FpuaEacl3tH9TwUsfXIpYPVzprl6o4iBXdYT0AUtDAtYc3y%2BEuJtrjkUwGEVlI650ylKvE%2B5ABA%2FHNTwuf9lc%2BBgItUcf0%2FAgZwQedwuks0ypTyaYjSqY%2BiqLe60l3E5aIWOZ1mxPuV70toergeGwR4g0v8V2eKi0otVJZJ05xV7GHcsHQO%2B0ESk9LSjDup6913x%2FKzVKdeX9THFGzb1v5TDDfpQ45bECoJ9%2B43cBcf0nCXXr%2FF8%2F43notvxJ6rVEnqc1TWG05X9cp%2BAAQRKWiHl2Knck80KgqljCAC4Aq1QvJpPHP6XaxCImp1FiUv6pwAUXstt2Ud9NrbHGJCAsQx9ufEKktsFtJBzroOMYF9EK%2FV%2BGK1mv8PflNJUQAAAAABAAAAARmahXJJOF8PPPUACQgAAAAAAMk1MYsAAAAAyehMTPua%2FdUJoghiAAAACQACAAAAAAAAeAFjYGRg4Oj9u4KBgXPN71n%2FqjkXAUVQwU0Ap6sHhAB4AW2SA6wYQRRF786%2B2d3atm3b9ldQ27atsG6D2mFt2zaC2ra2d%2FYbSU7u6C3OG7mIowAgGQFlKIBldiXM1CVQQRZiurMEffRtDLVOYqbqhBBSS%2Fohgnt9rG%2BooxYiTOXDMvUBGbnWixwgPUgnUoLMJCOj5n1IP3Oe1ImajzZpD0YOtxzG6rSALoOzOiUm6ps4K8NJPs6vc%2F4cZ1UBv4u85FoRnHWr4azjkRqYKFej8hP3eqCfDER61uyT44DbBzlkBTwZD8h8%2FsMabOD3ZmFWkAiUs5f4f2SFNZfv6iTPscW%2BjOHynEzEcLULuaQbivCdW5SDNcrx50uFYLzFHYotZl1umvNM1tgNWX%2BV%2F3gdebi3ThTgVEMWKYci4kHZhxBie3TYx3rHbGr%2BPdo7x4dIHTKe5DFn%2BO%2Fj%2BW2VnE3ooW6isf0LIUENvZs1gf%2FLHojJwdpplCP5gn%2F5gi26FoYa19ZVFOJ6Sxuoz%2Fq2Ti20IKVJdnqvYJwnhfPH%2F2f6YHoQF30aZaK9J8T026RxH5fA%2FWPW%2F8IW4zkpnIfoFLifGB86v0ffm5nbyRs5iaHR3hNBD0HSfTzoPugRM%2BhdN0x052KoHLBS0tdgpidAiEesDsgWYO73RWQz2LWIwjqnMe%2FuYISQtlbyf2NlT9Q9PoBcBnrO6I5ELoMeyHkNnIXGdv809H%2FDXNOTeAEc0jWMJFcQxvFnto%2F5LjEvHrdbmh2Kji9aPL4839TcKPNAa6mlZUyOmZk6lzbPJ3bo56%2F%2FCz%2BVaqqrat5rY8x7xnzxl3nvo%2B27jFnz8c%2FmI9Nmh2XBdMsilrBitsnD9rI8aiN5DI%2FjSftC9mIf9pMfIB4kHiI%2BhWfQY5aPAYYYYYwpcyfpMMX0aZzBWZzDeVygchGXcBlX8ApexWt4HW%2FgLbzNbnfwLt7DJ%2Fp0TX4%2BUucji1hCnY%2FU%2BcijVB7D46jzkb3Yh%2F3kB4gHiYeIT%2BEZ9JjlY4AhRhhjytxJOkwxfRpncBbncB4XqFzEJVzGFbyCV%2FEaXscbeAtvs9sdvIv3cjmftWavuWs2mg6byt3ooIsFOyx77Kos2kiWsIK%2FUVPDOjawiQmO4CgdxnAcJzClz2PVbNKsy2ZzvoncjQ66qE2kNpHaRJawgr9RU8M6NrCJCY6gNpFjOI4TmNIn36TNfGSH5RrssKtyN%2B59b410iF0sUFO0l2UJtY%2F8jU9rWMcGNjHBEUypf0z8mm7vZLvZaC%2FLzdhmV2XBvpBF25IlLJOvEFfRI%2BNjgCFGGGNK5Rs6Z7Ij%2F45yNzro4m9Ywzo2sIkJjuBj2ZnvLDdjGxntLLWzLGGZfIW4ih4ZHwMMMcIYUyq1s8xkl97bH0y3JkZyM36j%2F%2B58rvTQxwBDjDDGNzyVyX35Ccjd6KCLv2EN69jAJiY4go%2Flfr05F%2BUa7CCzGx10sYA9tiWLxCWs2BfyN%2BIa1rGBTUxwBEfpMIbjOIEpfdjHvGaTd9LJb0duRp2S1O1I3Y4sYZl8hbiKHhkfAwwxwhhTKt%2FQOZPfmY3%2F%2FSs3Y5tNpTpL9ZQeGR8DDDHCGN%2FwbCbdfHO5GbW51OZSm8sSlslXiKvokfExwBAjjDGlUpvLTBY0K5KbiDcT672SbXZY6k7lbnTQxQI1h%2B1FeZTKY3gcT2KvTWUf9pMZIB4kHiI%2BxcQzxGfpfA7P4wW8yG4eT%2FkYYIgRxvgb9TWsYwObmOAITlI%2Fxf7TOIOzOIfzuEDlIi7hMq7gFbyK1%2FA63sBbeJtvdwfv4j28zyaP8QmVL%2FimL%2FENJ5PJHt3RqtyMbbYlPfQxwBAjjPEN9ZksqkMqN6PuV7bZy7LDtuRudNDFwzx1FI%2FhcTzJp73Yh%2F3kB4gHiYeIT%2BEZ9JjlY4AhRhjjb1TWsI4NbGKCIzjJlCmcxhmcxTmcxwVcxCVcxhW8glfxGl7HG3gLbzPxDt7Fe%2FgY%2F%2Begvq0YCAEoCNa1n%2BKVyTUl3Q0uIhoe%2B3DnRfV7nXGOc5zjHOc4xznOcY5znOMc5zjHOc5xjnOc4xznOMc5znGOc5zjHOc4xznOcY5znOMc5zjHOc5xjnOc4xznOMc5znGOc5zjHOc4xznOcY5znOM8XZouTZemS1OAKcAUYAowBZgCTAHm3x31O7p3vNf5c1iXeBkEAQDFcbsJX0IqFBwK7tyEgkPC3R0K7hrXzsIhePPK%2F7c77jPM1yxSPua0WmuDzNcuNmuLtmq7sbyfsUu7De%2Fxu9fvvvDNfN3ioN9j5pq0ximd1hmd1TmlX7iky7qiq7qmG3pgXYd6pMd6oqd6pud6oZd6pdd6p%2Ff6oI%2F6pC%2FKSxvf9F0%2F1LFl1naRcwwzrAu7AHNarbW6oEu6rCu6qmu6ob9Y7xu%2BkbfHH1ZopCk25RVrhXKn4LCO6KiOGfvpd%2BR3is15xXmVWKGRptgaysQKpUwc1hEdVcpEysTI7xTbKHMcKzTSFDtCmVihkab4z0FdI0QQBAEUbRz6XLh3Lc7VcI%2FWN54IuxXFS97oH58%2BMBoclE1usbHHW77wlW985wcHHHLEMSecsUuPXMNRqfzib3pcllj5xd%2B0lSVW5nNIL3nF6389h%2BY5NG3Thja0oQ1taEMb2tCGNrQn%2BQwjrcwxM93gJre4Y89mvsdb3vGeD3zkE5%2F5wle%2B8Z0fHHDIEceccMaOX67wNz3747gObCQAQhCKdjlRzBVD5be7rwAmfOMQsUvPLj279OzSYBks49Ibl97In%2FHCuNDGO%2BNOW6qlWqqlWqqlWqqlWqqYUkwpphTzifnEfII92IM92IM92IM92IM92IM92I%2FD4%2FA4PA6Pw%2BPwODwOj8M%2Ff7kaaDXQyt7K3mqglcCVwNVAq4FWA60GWglZCVkJWQlZCVkJWQlZDbQyqhpoNdAPh3NAwCAAwwDM%2B7b2sg8kCjIO4zAO4zAO4zAO4zAO4zAO4zAO4zAO4zAO4zAO4zAO47AO67AO67AO67AO67AO67AO67AO67AO67AO67AO67AO63AO53AO53AO53AO53AO53AO53AO53AO53AO53AO53AO5xCHOMQhDnGIQxziEIc4xCEOcYhDHOIQhzjEIQ5xiEMd6lCHOtShDnWoQx3qUIc61KEOdahDHepQhzrUoQ6%2Fh%2BP6RpIjiKEoyOPvCARUoK9LctP5ZqXTop7q%2F6H%2F0H%2B4P9yfPz82bdm2Y9ee%2FT355bS3%2FdivDW9reFtDb4beDL0ZejP0ZujN0JuhN0Nvht4MvRl6M%2FRm6M3w1of3PVnJSlaykpWsZCUrWclKVrKSlaxkJStZySpWsYpVrGIVq1jFKlaxilWsYhWrWMUqVrGa1axmNatZzWpWs5rVrGY1q1nNalazmtWsYQ1rWMMa1rCGNaxhDWtYwxrWsIY1rGENa1nLWtaylrWsZS1rWcta1rKWtaxlLWtZyzrWsY51rGMd61jHOtaxjnWsYx3rWMc61rEeTf1o6kdTP%2F84rpMqCKAYhmH8Cfy2JjuLCPiYPDH1Y%2BrH1I%2BpH1M%2Fpn5M%2FZh6FEZhFEZhFEZhFEZhFEZhFFZhFVZhFVZhFVZhFVZhFVbhFE7hFE7hFE7hFE7hFE7hFCKgCChPHQFlc7I52ZxsTgQUAUVAEVAEFAFFQBFQBBQBRUARUAQUAUVAEVAEFAFFQBFQti5bl63L1mXrsnXZuggoAoqAIqAIKAKKgCKgCCgCioAioAgoAoqAIqAIKAKKgCKgCCgCyt5GQBFQBPTlwD7OEIaBKAxSOrmJVZa2TsJcwJ6r0%2F%2B9sBOGnTDshOF%2BDndyXG7k7vfh9%2Bn35fft978Thp2wKuqqqKtarmq58cYbb7zzzjvvfPDBBx988sknn3zxxRdfPHnyVPip8FPhp8JPhZ8KP78czLdxBDAMAMFc%2FbdAk4AERoMS5CpQOW82uWyPHexkJzvZyU52spOd7GQnu9jFLnaxi13sYhe72MVudrOb3exmN7vZzW52s8EGG2ywwQYbbLDBBnvZy172spe97GUve9nLJptssskmm2yyySabbLHFFltsscUWW2yxxX6%2B7P%2BrH%2Fqtf6%2B2Z3u2Z3u2Z3u2Z3u2Z3s%2BO66jKoYBGASA%2FiUFeLO2tqfgvhIgVkOshvj%2F8f%2FjF8VqiL8dqyG%2Bd4klllhiiSWWWGKJJY444ogjjjjiiCOO%2BPua0gPv7paRAHgBLcEDFOsGAADAurFtJw%2Fbt23btm3btm3btm3btq27UCik%2F1sq1CH0I9wl%2FDTSONInsjxyKcpGc0VrRNtGx0dXRF%2FFpFiV2KbYl3j%2B%2BJz4vkTaxKjEgcSXpJzMm6yb3ALkAnoCV0ARLAcOBjdCAJQJqgWNhJZDT2EbbgTPhz8h%2BZFJyDbkFSqgVdGh6Br0BhbFFCwHVhNrj43DXuH58V74WcIkahHvyDRkLXIGeY18SxWl%2BlMHaIVuSc%2Bh3zHpmNbMJOYuy7DF2E7sFvYMJ3Clf%2B3DHecNvjm%2Fm38g1BYmioxYS5wqbhZ3S0Wl2tJkab50U04pl5CHy9vlmwqlZFJaK4uVnco55YlaUK2kNla7qEPV6epi9aMW01jN0zJohbRZ2mptj3ZWu6e91wE9vT5LX63v0c%2Fq9%2FUPRiZjprHS2GmcNG4ar8yIOcycZC4yN5mHzMvmE%2FOrhVq6NcCaYC2wNlgHrAvWQ%2Ft%2Fe6w9115r77XP2fecrE4xp65zwM3lNnZnuBfdZ17E071sXj6vrTfP2%2BHd8F74lJ%2FeL%2BHv86%2F6D%2F23Qfogf1A%2BqB10CAYGk4LFwdaf2C%2BJfQAAAAABAAAA3QCKABYAVgAFAAIAEAAvAFwAAAEOAPgAAwABeAFljgNuBEAUhr%2FajBr3AHVY27btds0L7MH3Wysz897PZIAO7mihqbWLJoahiJvpl%2BWxc4HRIm6tyrQxwkMRtzNIooj7uSDDMRE%2BCdk859Ud50z%2BTZKAPMaqyjsm%2BHDGzI37GlqiNTu%2Ftj7E00x5rrBBXDWMWdUJdMrtUveHhCfCHJOeNB4m9CK%2Bd91PWZgY37oBfov%2FiTvjKgfsss4mR5w7x5kxPZUFNtEoQ3gBbMEDjJYBAADQ9%2F3nu2zbtm3b5p9t17JdQ7Zt21zmvGXXvJrZe0LA37Cw%2F3lDEBISIVKUaDFixYmXIJHEkkgqmeRSSCmV1NJIK530Msgok8yyyCqb7HLIKZfc8sgrn%2FwKKKiwIooqprgSSiqltDLKKqe8CiqqpLIqqqqmuhpqqqW2Ouqqp74GGmqksSaaaqa5FlpqpbU22mqnvQ466qSzLrrqprs9NpthprNWeWeWReZba6ctQYR5QaTplvvhp4VWm%2BOyt75bZ5fffvljk71uum6fHnpaopfbervhlvfCHnngof36%2BGappx57oq%2BPPpurv34GGGSgwTYYYpihhhthlJFGG%2BODscYbZ4JJJjphoykmm2qaT7445ZkDDnrujRcOOeyY46444qirZtvtnPPOBFG%2BBtFBTBAbxAXxQYJC7rvjrnv%2FxpJXmpPDXpqXaWDg6MKZX5ZaVJycX5TK4lpalA8SdnMyMITSRjxp%2BaVFxaUFqUWZ%2BUVQQWMobcKUlgYAHQ14sAAAeAFNSzVaxFAQfhP9tprgntWkeR2PGvd1GRwqaiyhxd1bTpGXbm%2FBPdAbrFaMzy%2BT75H4YoxiYFN0UaWoDWhP2IGtZtNuNJMW0fS8E3XHLHJEiga66lFTq0cNtR5dXhLRpSbXJTpJB5U00XSrgOqEGqjqwvxA9GsekiJBw2KIekUPdQCSJZAQ86hE8QMVxDoqhgKMQDDaZ6csYH9Msxic9YIOVXgLK2XO01WzXkrLSGFTwp10yq05WdyQxp1ktLG5FgK8rF8%2FP7PpkbQcLa%2FJ2Mh6Wu42D2sk7GXT657H%2BY7nH%2FNW%2BNzz%2Bf9ov%2F07DXE7QQYAAA%3D%3D%29%20format%28%22woff%22%29%7D%40font%2Dface%7Bfont%2Dfamily%3A%27Open%20Sans%27%3Bfont%2Dstyle%3Anormal%3Bfont%2Dweight%3A700%3Bsrc%3Alocal%28%27Open%20Sans%20Bold%27%29%2Clocal%28OpenSans%2DBold%29%2Curl%28data%3Aapplication%2Ffont%2Dwoff%3Bbase64%2Cd09GRgABAAAAAFIkABIAAAAAjFQAAQABAAAAAAAAAAAAAAAAAAAAAAAAAABHREVGAAABlAAAABYAAAAWABAA3UdQT1MAAAGsAAAADAAAAAwAFQAKR1NVQgAAAbgAAABZAAAAdN3O3ptPUy8yAAACFAAAAGAAAABgonWhGGNtYXAAAAJ0AAAAmAAAAMyvDbOdY3Z0IAAAAwwAAABdAAAAqhMtGpRmcGdtAAADbAAABKQAAAfgu3OkdWdhc3AAAAgQAAAADAAAAAwACAAbZ2x5ZgAACBwAADiOAABYHAyUF61oZWFkAABArAAAADYAAAA29%2BHHDmhoZWEAAEDkAAAAHwAAACQOKQeIaG10eAAAQQQAAAICAAADbOuUTaVrZXJuAABDCAAAChcAAB6Qo%2Buk42xvY2EAAE0gAAABugAAAbyyH8b%2FbWF4cAAATtwAAAAgAAAAIAJoAh9uYW1lAABO%2FAAAALcAAAFcGJAzWHBvc3QAAE%2B0AAABhgAAAiiYDmoRcHJlcAAAUTwAAADnAAAA%2BMgJ%2FGsAAQAAAAwAAAAAAAAAAgABAAAA3AABAAAAAQAAAAoACgAKAAB4AR3HNcJBAQDA8d%2BrLzDatEXOrqDd4S2ayUX1beTyDwEyyrqCbXrY%2BxPD8ylAsF0tUn%2F4nlj89Z9A7%2BtETl5RXdNNZGDm%2BvXYXWjgLDRzEhoLBAYv0%2F0NHAAAAAADBQ8CvAAFAAgFmgUzAAABHwWaBTMAAAPRAGYB%2FAgCAgsIBgMFBAICBOAAAu9AACBbAAAAKAAAAAAxQVNDACAAIP%2F9Bh%2F%2BFACECI0CWCAAAZ8AAAAABF4FtgAAACAAA3gBY2BgYGRgBmIGBh4GFoYDQFqHQYGBBcjzYPBkqGM4zXCe4T%2BjIWMw0zGmW0x3FEQUpBTkFJQU1BSsFFwUShTWKAn9%2Fw%2FUpQBU7cWwgOEMwwWg6iCoamEFCQUZsGpLhOr%2Fjxn6%2Fz%2F6f5CB9%2F%2Fe%2Fz3%2Fc%2F7%2B%2Bvv877MHGx6sfbDmwcoHyx5MedD9IOGByr39QHeRAABARzfieAFjE2EQZ2Bg3QYkS1m3sZ5lQAEscUDxagaG%2F29APAT5TwRIgnSJ%2Fpny%2F%2FW%2F%2Fv8P%2Fu0Bigj9C2MgC3BAqKcM3xgZGLUZLjNsYmQCsoGY4S3DfYZNDAyMIQAKyCHTAAAAeAGNVEd320YQ3oUaqwO66gUpi6wpN9K9V4QEYCquKnxvoTRA7VE5%2BZLemEvKyvkvA%2BtC%2BeRj6m9Iv0VH5%2BrMLEiml1XhzPdNn3n0rj6%2FEKn2%2FNzszO1bN29cv%2FbcdOtqGPjNxrPelcuXLl44f%2B7smdOnjh09crhe279vqrpXPuM%2BPbmzYj%2B2rVws5HMT42OjIxZnNQE8DmCkKiphIgOZtOo1EUx2%2FHotkGEMIhGAH6NTstUykExAxAKmEqSGMFl6aLn6J0svs%2FSGltwWF9lFSiEFfO1L0eMLMwrlT30ZCdgy8g2S0cMoZVRcFz1MVVStCCB8raOD2Md4abHQlM2VQr3G0kIRxSJKsF%2FeSfn%2By9wI1v7gfGqxXBmDUKdBsgy3Z1TgO64b1WvTsE36hmJNExLGmzBhQoo1Kp2ti7T2QN%2Ft2WwxPlRalsvJCwpGEvTVI4HWH0HlEByQPhx468dJ7HwFatIP4BBFvTY7zHPtt5Qcxqq2FPohw3bk1s9%2FRJI%2BMl61HzISwWoCn1UuPSfEWWsdShHqWCe9R91FKWyp01JJ3wlw3Oy2Ao74%2FXUHwrsR2HGHn4%2F6rYez12DHzPMKrGooOgki%2BHtFumcdtzK0uf1PNMOxwDhN2HVpDOs9jy2iAt0ZlemCLTr3mHfkUARWTMyDAbOrTUx3wAzdY%2BniaOaUhtHq9LIMcOLrCXQXQSSv0GKkDdt%2BcVypt1fEuSORsRUwgrZrAsamYJy8fu%2BAd0Mu2iYFhexjy9FIVLaLcxLDUJxABnH%2F97XOJAYQOOjWoewQ5hV4Pgpe0t9YkB49gh5JjAtb880y4Yi8AztlY7hdKitYm1PGpe8GO5vA4qW%2BFxwJfMosAk2X9n9X2cVVfnA36pzHNHJGbbITj75NTwpn4wQ7ySKfAu9u4kVOBVotr8LTsbMMIl4VynHBizBEJNVKBAfMNA9867j0InNX8%2BranLw2s6DOmqIHBIbDfQR%2FCiOVk4XBY4VcNSeU5YxEaGgjIEIUZOMi%2FoeJag4mEB3PUOweCaG4wwbWWAYcEMGKn9mR%2FsegY3R6zdYg2jipGKfZctzINQ%2FvxkJa9BOjR44W0OpTKAskcnjLTcKyuU%2FSVIWSKzKSHQHebYW9mfGYjfSHYfbT3%2Bv877XhsIwGzEUaleEwITyE2u%2F0q0Yfqq0%2F0dMDWuicvDanKbjsB2RY%2BTQwOnfvbMUhiNPFyDCRwhZhdjE69Ty6FjoOoeX0spZz6qKxxu%2Bed523KNd2do1fm2%2FUa6nFGqnkH8%2BkHv94bkFt2oyJj%2BfVPYtbzbgRpXuRU5uCMc%2BgFqEIGkWQQpFmUckZe2fTY6xr2FEDGH2px5nBcgOMs6WelWF2lmiKEiFjITOaMd7AehSxXIZ1DWZeymhkXmHMy3l5r2SVLSflBN1D5D5nLM%2FZRomXuZOi16yBe7yb5j0ns%2BiihRdlFbd%2FS91eUBslhm7mPyZq0MNzmezgspUUgVimQ3kn6ug48mntu3E1%2BMuBy8u4JnkZCxkvQUGuNKAoG4RfIfxKho8TPoEnyndzdO%2Fi7m8Dpwt4XrnSBvH45462t2hTEX4Bafun%2Bq8jIzK%2FAAEAAgAIAAr%2F%2FwAPeAF8egd8lFXW9zn3PmX6PNMnPZNJMRRDMkzmDYgZMRRDCEmMMUPJIgZEepHlRYyIiNhRUdYuS4ksy9reLDYsdOmLLC%2FLy7L2CgKrrCJkLt%2B9T2YyYPl%2BD8804J5zT%2Fn%2FzznPBQKbACSTvAEoqJAdtUhUJpQYjBJVAUrKSkIOJ1ZUOEKOUGkfV8ARiPB7E72m87WJZF58ibzhXPVE6QsAAnMufI4H9XXsUBh1UpOJSJLmQNWqNsasLkKhsrKnA%2FT1HCF9PQzSAPYtD5V5PW4lmFeIK86EcCRbObLp2lGjGxpH4%2Bf0wLkjjU3NDSNGxYSMxbSdDkzomhE1SypQalCISvniob1lDuTL7injC1O%2BMr%2FxmeJtxeRt%2FiJviJ8mmrjFOr0BJCZ3QAbkQFu0ypCZ45HcRqNJQkiT%2FLKsOO02s2Ryudze7CxVUnw%2Bv9%2BtmKTcgEEymzPRlgN2e5rHaeOXyeeiisnJFagMOSsqSkr45kL8Tr450SfM5%2Fy1V66pGvBwTV1BcYcDEX67QjQkbo8cigTplyVI2OHh%2F6zdXHO4%2BiR6SjoxMPzo8O21h2tPx7O2lmylNV%2FtY5Nwubj3fXUA%2F8BuFveBr74CoNB84V6pSnFCLhRCL7g7OijfR7Oy3FalR49AcXYRFBnsQUcgkAYO6H15j6wiAGu%2BI%2BAo6pleFDAWKJZMX%2BaImNunWOpiskIVH796ewAqEzvV9gqX9nQ4Qd8S%2F1V%2FScSM%2FrmsTP9FfNUNIvzuVlRPMFxY5PB6fY6iwsJw3%2FJIOOTx%2BlT%2BWzaR%2BxYWecrR7fWFFanqi%2F33nnn9%2Bv%2BMvXr7mk933%2Fv5Gy3PrN6yZjg7WFV1D5s2oGoh7nx%2Bk2vvTrkeDT0HKlieXvvakkfecj%2F5uKnhm6iNHRk27a6bevTL%2BclH3ulVkX3cBTJUXjip%2FCDvBiO4wQ95PB6qo%2Flen0%2BWTRpofo8nLa04mB3UgpeX5PbMLEzzKz4%2FtapOlXt5a1llpXhN7FF7r8zJ37o%2FiN15Q2XhvsE8RdajOqwFyrwFGETXr%2F0F9u9dNnZsWW9869X1azow9qe%2Fkpc7D52mPRf%2F%2FHcJFrR1npvf9sWX336EO7%2F9x7lqeUMn6frt8y%2B%2F%2FZD%2FJjzecOGEAnxvWdzjpTAzWtHbGjRhlhdMXqvLVZSWnl5kpSoChLJVtcwXSPea8vNLSrT0dEnTegyPaZIUqIlJLnSKhAV%2FpfBuhb9EbE53bYVIM%2F3S45hfiZ%2B7th8IFPHN5QuXcscms1vF8kiAZ2qBsEEEFQX7FnJDeNy%2B8nIF2JLZ7%2F77DPtk3rJhVV9vefPD%2B57CzCF98cr82%2Bs631s4%2FvbxrKPf1XjT0Iqrh%2F%2BuafTMxR%2B9e%2B%2BmxqZnxzzx5l8embstxo7PeX0Ju3DjoqYJA7C611hyd3hAtH%2FzpD5jAAVm4DM6Zjj5C5WIAIu9DuxCIB0kuvEBAKGBbSTz%2BL%2B3Qm7UZjaZqCSBqtrN%2BVQgmAMTua3joeaMhBTicTt9wULS8PSj5x58eNk9Z5c9RUrRiPte3MTKzvyHRd5Yh9vFygP4yq3JlfmyfHG%2Bso1LyP%2F5yqgRNVjuDPclRSGvk7Q%2B%2FejZJY89%2FOA5sTT7ifVb%2Bzru%2FOEM7tv0EisFhErSJGUpbrBBOOo3ms0ypVZUVc0umUyqilarYrDxpN1aJrKQuykJwvwz%2FyPMUOCTXSqlRa6CiEzJy8U4J8DWf%2FjpM%2FeeOMZeLMKpxYqbPTyx088Oz8MKtnMuFqefm4gzAKEZPpUqpG1g5qivGRSjkSKAxWo2giJRKOFCysqS4vjNhQXCAa4Bxz1HEI%2ByNlx0FBextqOk9SjezW49yhaIHbGzuBtOggKe1wgFWVapDCXbdSNt5ghfoNCgMxLA3X1v%2B%2BdV%2Beg%2FvIsdR9MJYWVcS5rISqDg%2BCuVQQLkSiTc7QoHPANIGq49dw6wi7GwgmvujZoUrrSRNsaMLqjsmfjnkYu4aU6SlJZ28xECNyqt0mMrM2pBricBidueiNS5iDcRA0ir4h%2By4yQgGJP%2FDwLVF05IQ%2BW9XLoPLou6LYoTFPCnGT0jYkaV2kfEaBok8y%2B1kkYCeeDQnIEyQI2nUrlDE3kkDT3PzsfZhXMoxZHGw2OmTRl7w%2BSpLeQoW8gexttwNi7C6ewO9hD7%2FusTaELr8eOAMA%2BA1nJtTNAj6jJKAAZEs8WgqihJRgX9wJHOkYoXkf8iwR2RiKKqRRiitWw3lYdnr30cDzNae%2F8Tw%2F1L3sS5gFALINXpKDQgmp1pQxW86M3O8aoqMTlNtTGnSjATM2tjXEgCYfS3hKyuCkFHkzBeScI6WKhFVxLuD%2BEQLt4TkOo6CU5f1drrhvrrVly%2FdspDayfe%2B8EtQx7fuJG0HcbZLyyc1r%2B5qXbojtE1xa0dt4x%2F5c31r9hA6MYtP5DrVgijoiV5Po6KKs3MBOCVStFlgez8bG57v8%2Fvq4tZ%2FGilfr8pX7VqJm1EzJQGeg3j5%2FxX8ruWMbrG4oduFyXxMEFyQlkpkMeJTvhKbCMY1j%2Fo2ykPlEmSr335KxvYPvbZydev29P65KNrX58%2Bc92zfxv6%2BKil76PnU1Sl6fe%2Bl694%2F%2FzIweMjUO1ZPnH2TU3fxqa09%2Bl%2F6OHXAQgEAaSZuhddMDiaZ1epkRAzpTKAxyVzrnGh7JLreGi7qF1VqO5WvoGQ0DwF584uo3cpz4sCBzc9T9SAQPKgoqI082X2QfxhshCzXmZ5Jmoo6MvOYAk7gCWH6cudN5%2B98oSroZZNBoRWbuEw1ygDmqI9OZ36aJrbbTPYqIFmZrldRpdFA27ONADF4%2FHXxjyKYhkRU9LgYsIJ6e%2BpgHAkGUjkgUhLSBg2N9w3IMwpylMaKScT%2Fn6efcC%2BPLN8xActmMGOhu%2B4bH6EpsV%2FyAgOoO0n9%2F%2BHnR2B5h7hr455LAPJ1%2Bwc%2B1i1AYGhXOs6eQf4IR%2BuigYUp8WSlweZTnAWFNpz6mJ2u4d60kbEPGnUwENEvUTbVJbqTCjIAQJlPo8IXEUNdQEJcCAhMvd%2Fgvy8Q3E6TmsbErv%2B%2BZ2tRuuN%2F7f1X%2BzsNyv%2FvYhoN066sbVlcRuZiq%2FiWvuP7rEb%2F7LuhyPfsFPLMffdxfMnz7%2B1fu5qEc0RPdM6QIHLo14FgCDKRFYNMiWU1MaoAsLfupYpQwobhpDby4OfkoJ4iZQWPyy9jNLm8wLSdEtUyzvBB3lwOVwbLXYqnl6U%2Bo3%2BQo%2FHnp1ttBtL%2BihOZyBQXGwBS0Z9zJIGwfoYXGwTYYlLnVeWdKFwoCSqAj0%2FLqoW8qk7kShFiku3kK9cfCPVHyDedt%2FqpeyLL06zk4uXtU1DyfXfE2fPmrng0Ccjbhg%2Bflxtq7zz3ZUzXhrU%2FO6sjqN73mrbXD2iY%2FKzm89vbBp7Y%2F3VcwaOI3vqq674XdnlYysH1Ym8GajvcgekQQFURnOzZJfFEgyCCwqLtNy6mKZRrzd9RMyrUkMdR%2BNfdbfu7DIBzCIaw0J5kS16edcXuNOdBXwbyU1J1ewxtvTOqxtHP%2F3%2BJIOl3xOz3v0nmr9Y%2Bf2d8VNjp4xrbbm7jQ5mdazJdtYzasufW2r%2B83%2FH0fEE%2B3DTXbdNum1%2BHfd4stOSZuvMURh1OXnyAPjtnsaYXeumMPAnaOwXTOb4NVYT72PqU%2BxG7xcf6mPNQAQX6%2FIUcHKmcllV1UUlBRXFZdIaYyZNUjgzJ6Rpm8u6mKrApzM0vUgYbrTrbF2SFHbS18Xa5GhSmF5P7JYqZODSiqKajIK%2FVYNEqQIEZRigFxShVFwJURhGD6JU0ZlDP443kvW7ccNSPH2abWFfCns140peoYDeNeZHHSqlRgkMcp00ViJSV30QKhkjagSue7JMQH4304%2FFkrTgKC9Tjh69VLueUScBrhFPNVAUJJTKEur6Ce0u1dCFuorNZH28UayJb2IaDjjNtKWsWmioXPicrpB365FYFc3LTU9PA%2BB2dlqdhUV2QCMFCAazGmNBl900ImaXkg7mVCR4KJVkyfpRJFR5F86oRckaXOFoe0m%2F7W6YevPVY5uWvzf1w3P7vm99YGyIHU4139VjH6ob1tLvqqpxR9u2r5m2onVI9RVXsHUX9eMTLkxQdnCc6AuVEIv2VCsq3G5XOGzt77rMZaWBtEDvNOgN0au8hkhEMg3QTPzqkVUq5feAklS7rOucMleiPU7ivc6kQtuiYCqrfNTdlVF8fxLxCKgtj3iUQC44%2BjrzOa06UfyDSESH3x2j106vnpWmTXnhlT1o%2BUfT%2Fqt9NdGau79%2FZhf73%2BexCP2T2Pz%2FZefZXez6I%2FgIyv%2FEkRs7Yf3IFpM1FG27n5x%2B%2BNQ9Q%2FotPPTGQSQBH%2FPd%2F9Yf%2Fvjjne1sx152gh0p6f3eKHwYW3%2FEZZ93sA627uCCpcfMzwj7AIC8WN4IKljh6miAWKkBQZHNZgqip6CSZLOSmpjVSs0yBZocIpTouZRiZWGortKL8gsDiITjI5Uik%2BLHJ7FXiYTziRJnywoMgWdwNFstbzxXRcbikdvy72CqiPvXAaQznI%2Ft4Idczsm9VLdbktKzzeY83vfZ7QGDlqalDY9ZNLRSTbODPb0mZneCvyYG9BLcSxY9KQVDSTe5ArmSp7voCQYwWfE4HPqnwOu4AyOYNn%2FC%2FfPZh2fjx7C84%2FaZ8xev2nXHraxT3vDKpkVrHaacdQ%2B%2B%2FxGdXTuy8Zr4NrZo3PgNgDCXI%2FUBnh9eKI36VZeLN%2BNWnxscUBNzSKpskmtiJleyNBOvSfVEKuQRD2%2B0Iw4l2BUdoTI%2BZiikBS%2B9h9OfOtrxL7aJvdiOkQOHDrc2tEs72U%2FHmW846xyGi3DSZ3j9azd1FvUDImwoz%2BE2NIBd1OtGAIdVkjTZUhOTqWTlLbMzaamUcEELnGVzAbVA0BHKleew8ew2Ng534wR8gL3Dxq5ZjO%2FxGuQP7A55A7ubrcHDnUMBdY8RLs0Mg6L5BgnAqphMiBbFWBOzKNxLAnII3zehaKqJofOXXkp5iCsitPAkbol0bqDV8RN4ijmIm4tl7zK2BLqkUsalGqFvNN1AqVkBQDQJoSl5QlZS0MVSLhaCX7P9dHD8OHKMEwKWxLu8KBdxL6ZDTbQo3e8nNquVEFemy2DIsGlmjQdbOr9BNkt%2Br%2BzlsmTu1FB3wd0z5VlnstgW8BBwKLpv9YJL5RlPdMKNOALkU1L14E93sr%2ByVfg43vTxgZtW%2FGXnd1vevKGVHafhuOnyAlyMU3AcPjDybB377rOT591Y2mUHeYJu%2FUg004jIzW%2BQJFm2GGhNrMaABoNsUijK3QmbMnfKFN2XPIHtjr%2FNdmE5uRrDZG78Xj5t2EIGAOCFiawBT%2BozgRw%2BbSAGXiPLwM0MRsr79e4NCw4Rxa5IJL6kRnJurq0bOKEZy79hDV4k7gVL5JHn1l4AdgYS%2BtfxVS0wMJpjIcRkNiOAzUBl2cq%2FUrNZoXwP3VtwpgBXF1eWAOXEQAdVfSMRDKBcx1awhYvEZm7FB7CZETKxJf4D39CN6%2FHf8XkJ6VIlly6LPUkqBVCQArccJKJUl6GXoPq6r3PD1MsbzldfSPxvRcyR3dAvmukGo9nI1bbxUPHKisdJjEQxq9QGilBcN36X0mUp6hA6Y9DpEYujXuXykscVRBpkK4wudhzbcaSC07GdfUgtRrZEms9Wzok3cw1WSi3nqklH6R3oPr8kYcedOm6WR9NMYETFagVwUFlRVM1MVW5RVLtHv11adI%2FEnAKwL1KEcM%2FJO9nv43fpSiwh81U7%2BqQGdrQtXseFv4FZvycdQPQ8%2BVKfDHgE0jgAfBZF8RpdNTGjRO01Mer6daQROSBexQQy16Hxpkj%2Bkj3BXubXE3gz1vNr%2FPlDb76Bs9nSNzaSY%2BxxdivejVP5tZCj0mP%2FOYvf4smfoAvtpHU62rkEFkhGowdsNrvdbQXBV3ZNM9TENGr%2FTSzoRn%2FZLXHoEyAo4ckJSx%2Bau%2BBBspEdYacX8yA6iCb0UGXmlKkTd504Fz8rb%2FgchAXYat0CdkjjEZynUFmSCDVIJg9AhmYypVOVEwBXRFK5UWSV22N7Ev4uHU92T9OQe%2BLX7PPaKziWzWZnfL9pJMZW1bO5OPS3LSUP1S3lg9poocvnk0ySppm8njQw8cTzu4wWMA6PAZgtFm40C%2FWaRcikzJbSWfPzuXKqQ0sxKLdfgl3BF0A82brsgaXLW7gB12EPzH7oTqxuZWvZKtp73M0Tm%2BPz4vvlDUeOLdxZwVwPk1KRVS2cQX0ce4s4n%2BRlpKcHICC7LeCGy4rdAbAELNlGX3ZNzCdRYyq%2BuhvwVHHWrRpn%2BIvGGoVFl%2FMhDadWMcJP9LZen9cr%2Bdin7JuOx%2FZeN2FqnzFL7767DtWvZu2f2TrnyermlsJrn977BC7f%2Flkz5g4srx3e8%2Borqypveeqmzf8qL%2F13n8KGgcUDKqrHbRP6FwNIYiqrimdLCgBFNBhVKlHOuxSdv3y2lARgcoLtYrOlOn53IGEMEF7k%2BdXC13JCQdThQHSbDQaX08hRhsdSYuuXVBAOtyLx4BHI6%2B6CYLnlEXbyLfYFex%2FD9zz7BAf0ztqVZ%2B7EwHn6YufCPz33%2FDraBqjXfyHBI2K%2BRonRKAOiVZYkC3BDJ%2Bq9VNpUJOaj%2BsXtVx6h57CC2dmLTMMKdPlKFXO0a4DY%2BdTwvZeN%2FqJLhrqRy8gSsx%2BT0e52yQh%2Bv2ynlszMrKwci9mcnemSzdRvt6NJiOSi%2BEtCbgo1UyM3WkiKOMKJUtMlGvCIi78nPihD2fPbzWFJ6WPdxqngfix9q9Sr9HQdwoJDth5mUy%2Fnm1hKoRixV%2FmpUJxwVT85trLi1EAa6twb%2BaS%2B9uuhNBsStmnSbVMVzTXLnPpUo6oYTYpJ0C2VLGYDkWXJqFCUkhDL9evG%2BooUZ3VpjZj8Izex59h6fnXg56wfNmF%2FDGMtC5Pi%2BGHyHdka%2F47Y4j27dJCYyF2B7wZVlZEQEERvNFFF4QqiSgVDdslOjEH5Z65AarLLowIDZAGWchEZbA%2FLwDo6mozsXBTfQUqoXleVJiZ0RugfzTJISFUVEExmlYuSRP1I0IAGUcZdOgxNpl1qFqqPbALSzPPvkbfjTVJ6vIrs30m%2FRXi%2F0ykkLWUbyWw9T7KjVgXRIIFRJlTBfN2EuvH0BNZX4iUpmc0y8bOPPmIblXMHz60Xa1gA6MDkVFt%2FZIKYnGpfnBa6sUmAHY9%2FmJhqI4S4fJ%2BQL55xoKIY%2BVYNoOZTiaaCvQtCfCFHMMy1CH34IX7GMmfKjQd%2FUoR8AzFIA%2BR3QIHeUTdBWVYkSTznFd6SVJko0DW%2BxLKLeyTRZYcwiGjADQ%2FjqVO8uP6KGOiGzmqyKN4maq1OtpHWXhja9SRIRonoRhEaJZ5K0NrOFyl%2F%2FvMAAGKNdIQ%2BqATAwK1gBjVKRVTIdwCUpB%2FrioP0XWLww7EvHPD6PGRL5ZkqbKpcLx3ptW2gZ%2Fz7GYIdmjju9pfm6E8Zq6OFTovBQvLy%2FP78LIMhaEkbFrNYZLfbPjjm5jWdnDM4JnvBk0Az%2Fy%2BZVYSeXlcUJWdMvMcN9%2B1u8h0omny9N6YT%2BhuGr1r0xzd%2BOr%2F5xbv%2FOn7T8Y9PswO%2FX3znY5MWPHHDsNfXvfono1K6rn7f%2BK3vx32E27h55MJbxwOBFVznDsUNTsjh7BvIojRg1Mw2n89szrWA2WPUFFDSh8QUL7iGxEC7mCz83SHi7H5mUeZ0aISzRVANCgTlw1AfH9d2D8WobftHX%2B7YNsMT%2BhpLLZbJM2ZOJJNvaZk%2BQ5rNdrPv2XH2t6XzFTdbPuiJ9jP3rwh0PPOXNWvWAMLoCyfoMWk2eDi6esRYymclxCubh8RkDexcM%2B%2BlZZJuOTk32SdwmnJoYkjgUBQyIf4DZqJx81Mjh9525cmTzcuHVf%2FBTQZgFvauOZFVwBH49ZIydr4kH4iQK81M2CcaDRi9Gi%2BobTZhqFy7xwIOIyi6fTTdPt5ft4%2BoT4Q%2BecShOXlPGioU%2FBLkji3iOnVPiAnZ9vHnOw9ON%2Fmw7Jv%2B1omT5kyVp7dNmDnLjWVoRx7zq9vG4YSfTjyy5vt7ViWNk9BynD61y%2BDMEKROSUpzOLKcJlOm3%2BOkzuoYFVUUVMesmuoZHFNTel5aloiry3bI3RbgrbNeR4XKwOMJ6AVAxMMtOP2GaQZcT2aVs%2B%2FY3zDt7LdoiJfID985vmNc3Qb61PyZM%2Bd3NmAPdGAahth3Jx%2B789Eel5%2B4rCjB7nSOkgMeuCKa7SZElSn1%2BqwAPhndyHVz283akJgZqJ4bgp8v7QVDiRwWFgxH9KfOeieocBWpiZ1l%2B9eu3bj%2Fufm1o2uv6ocGOq9zCZ23rKHh3ZdLPsoafsVgoKAwtzSV26sYyiEKd0SrzFlZAwZIfRwOUqzmSkGUpIHpPXr4fJFg8Kp0K1jRqlj7qv2GxYy5Eke5wr7FpDpWXFxYWDksVqi5e1fH3BkXz%2Bn4pxIOWz79gRHv0LneqJs2FQ76ewKfPao%2BpSsqEvmsj%2BykQFfCF6ZeRcGFyUQK8v26El%2F4WGzqS33OfxjpXbL2ndc3sTfYvm9%2BvP3WksHVg5tvOnmsZKGTFc2buvrNabOfa5w5%2Fdrrmura10otT%2FceNqZjJ5Xzew187smt%2F1i1bPw9We5Roeh1xYVrZ732vkM6L1UOHVlb2WcEHT5q0qRRuwBhBYC0lmeDB8LRdATw2Y0Wg8Fo9Nolp1MaEnNqJkCjR6D%2FJfU5336yUOPaKqJJEuCQeFQirWX7O%2B6YxfZjqapqE%2F61bQ958LsXt8S%2F40CwpeDekav%2Fvh0ILAPAD7lsA1jEZFcyGsFksprtJg9Rr4kR6DJ%2FZWoO7uobKtNnnyJUlrW3X3ttO14phMgLHn98yIjzPqkFgFxoY259XSt4oSTqd%2FL0JgaDT%2FNcE9PAaBctOk%2FsjOTEKYEwCRGJxwB6tajQpMDBcxoHXzN8CJbum6GLZe60066mRmnd%2BeJXN6mThXRIWPMH%2FUn%2BNdGgxLmTUKrIsmYzWa0Gg8lkN4P41WCzUcXkofbu2oTf3cjSZdpuokXRuGOyi1dx22KswGZWhYd5AffOIrF9jYxdh40sI74Et93MVivueDXr0gYPcG0ouF4DRIkAevQioLvExgPivyvuhO7qQJ5BQRgeLXS7XPrsKDMzI6PAajSaTPkuq9WRKzu46XwOzWzPRJNH7%2BG7krl7%2BOC8ePqbjJDCRIiEfKFykdziVfBd8q%2Bke9n%2B%2BuvnTGL7vy529F437Xwso%2FdL097ZwvbVXz9jOnlw3rz12%2BLfSS1Lh1%2B%2FurZpy%2BF4kfhtxYuQjGCut1tMFxHAq6vrscoOoatQFU0Xx29SyV%2FXLRG8TS0ierkyof%2BZtWWXEPbn7boC9dce3JHE5yf0pzhpostXLJYMcLnSvcYhMa9mp0Nidu8vu%2FxUrvPeVQMOCCQs6MzrxGVT5986ecr8W6dQmX3ELvzxh7swGyl%2FI6Xt6%2F70Qnv7mhfYKbbnQTS8jE7s8wA7B4LrOep1cC1ckMMn1Hl%2BRVFNlKpZmqrlcuQEq9U9hBOEwa5mQEaKzBKmSBWoSQVlTvPepDFCnPndRKFJtuemosq2GZrG9p%2FtaZv8wfaPbt58TGf7vePdSx%2Fwsv5K9SPtbB87%2FT%2Fs7H10mU722JDgM67pTN1euaIq8dIsyh%2BTpOUZ%2Bfg6PcNnz%2FZanE5V4I0FhsQsv8m6iSfIBUmS5S2dL8HBXl8ook%2BLIkFBaLdMkafPPzxZ2v7R5zsmPXeFIQMJ22e1lq48uri9oOMZ9uLa9lNYiho3Z9%2B6xqU%2FbcBDAybXN3ZFFJ3LddVEh0mcejw5BCxZZVnUS7wGFxqlMrTMRy%2BJIqpdWewrCD%2B6iu3%2Fsre97yvSbCP7xLR8SXyH1LKxZTYkqp%2F1XIZ4dpmjpLktAEU5bnchWNw5lhxTli9rcMynUdPgGPX%2BvJ2%2F2BgiqPTHK2HB5clePsGgXCkPt082oetPnbx1%2FbDrDtW395oycuG8yJd%2F3%2FXu6MZHa5Zcv2zRrf2wZn1HILfzsvKx%2Bb0rCstHz73%2B8VXN%2F8y%2F%2FJriK%2FqHR%2F%2B30LeE6xuRa8AjToRYDHa7y2UyEIfB4fWZnHbn4JjVYrfL3HVyQt3QpktOVnRhgnBcxKOXvoLpIyFPwCO6cjK3bsas9tdeeHRt8xasYDuu%2BTD4aeiNN0jGwgknTn4e%2F%2FyqK4UOT%2FGc4zM%2BcENZ1E8cDrfby3t%2Fj9NoJ7JNtumyPcmJ1sVDgItr7tQYgH%2BgrxdrpR2zt72PpSLjsXRp7XUHt5Mj8dki4Ynt%2FEpI9JkPcrlm6BV1m0GWiYgIK0G0GNEuC5llKWndDU1X%2Fx0SbTfiOtaElf%2FINyryZYexkjVJLfFF86aMXUzaumS4AZRtXEaWOMsoSyaOIVng81ETVTMyMjNzVEXJ9plMVLbbMxQ7yDqidR3RdPz2LIDSIO1WQ8wBsin%2FpGskRZpuUfew19lm7LMwJ1eRcrT7sG6R5NCsqBgvN92NPdk7uARPdt4vtTDH4m9q1lxH%2FPGvvE03jMkcer4XnuKKI5gApOW6bWqi%2BYoMaKSUSAQlGWWzQVWtfIZmMSoUAA1mj4T2S2cBqaROkYZeq3KlhdkClOu%2FmD2BI48cxZHsMWxja46fYO2kPwmyZ7A1fiy%2BDRewhcJLzK17ycs1KTC73ZrXK0koahm%2FJgob%2FpNT8no0p9XJMTHDAFyVskQJkKKvhBlTUzxHyokifvTqgNsSaw9mmBRz7n4cwoqu%2BvcfR9RErqqfl%2Bfkfr2%2FYcZNo8ic866XXnR8Z72xNZI450HXce2MIn%2BoKqkIYDYgmvQhAm8c7YR%2FMwyOoefSIULSSMJGySlCWEwR6LrOB4nC0uhAZiCmDrLp6%2B3xekDI4T38Id7D54ipCHUbcnIcfn%2BuNTMzIFGXy8qjKd9qSbTzYosp2hbbF7bnuBrm%2BREWRw08Coc18VTQ4xFQ6%2BEJhDmL2m6%2Fc%2FOZG4cpn31T3XpmM9quH32qucGAVz7Z9jEdXMUObcyzBF8xskNVg%2BknbU8BIO5gJWSlYgMK7tcIpZJMAaCyhONDYlbqCOKOo0cV29lA1ylOauB7yBN7yOHlOmgGQ75bkoI52TabW3Z7qCzl%2F3%2F2IIuHzuFynuSi2BZnlftyiBSnzxyCyzwcrImh4e0Xbhz2%2B9mfKtWtL7xTP39x26LeM2aFPyFVQ7CnuWmyw5K3EXsOrqIfh2dPY5tNjY2nGm7QTxGQIqmCtoEHIlG%2FAg4zmKnd7qNeu82mSJSaHQ5QoCRU1lYi9ElBdqqp5pwa1sv%2FRAMmELwQB0baym968pqFwxaOC99ePv7pgf89chFZcXX5l1NzcyPRii%2Bnphf8lzhBwpbiQanl0rP6Dg26zurbad4v56mukCugE0Wi7Vh7JsTasSV5lIO0dJbKBcljHAhLOdJqfN6cwad7QYchPV3OyCA%2Bn4mYMrPSXCNiBtuIGMiGNH4pGWmKygXqpwH4S8%2BePzvOII575nOCTh4R15lS69q26gmSEBt94OCr7YtF6z7vlm8b7mpdcN%2BrL%2FfHcyhjZk77c8arjmflv%2FBn9kZObzbAuFFEB4A0ST%2Bd2BztZXeaidFqTfd6iV%2FzO51ado7Fn%2BavjxnT0sDFqcleG3P6QR7xs%2BNNXUfUIJTSVqjbjT%2BpBpRfbpXXFSKawsFwiBuQbNyyZcyzs2sbcS679w9k3%2Fmvbhr%2B6qufy7sbvojGrt10dOm6WtZ5ttes1keObtl5BAjMBCYFpHXcnkW8R87TLC6j7EsnBrDZ8jIhM%2FOyYp9LSycWo2xQPZ4ctYBHz%2FYyHc11H2qb9S%2BiA4oURXyC3SM%2B0WGqPrVIoJJaFCmMXFRdbixfuGzBqEk3j1qwfGE43Pbogt%2BNn93Y9siC8v1T6%2BqnzxxRO50cnPC7BcsWhCMLly6MTZs8uu2RtlBo%2FiNtYyYOnz6ttm7aDBHpCoDEp%2BPghZnR%2F7I53U6Plce2UaYyMYkJqxeRED%2FHBp%2FidDkbYkCRuuwmm93WEFPtdgt6FMsl5xX9mtiW3kNfypcpEhAfkgPKkCfoEXdAGF7cGCBD0YAVbOGWH374gX38448%2FvsOW4BViZBv3vHrfq8eO8RdyHMhFiKNCMGoniiKGmUaJSlTVsUcEbCpFdAhyJGBIAFHnAbag8wAAgUm89lnw%2F0o5D7g2jvTvPzOzu9KCJNSFaAKEBMYHAokSuQpiY04OODjYsWxCcjbkNaluuPdyiXuaS0jHpPfeE0N68fVO%2FObSe%2B8uy39mVlqEzr76oeyi%2BbG7U3bK83yfkUZBGZwCMyKlaRaXRRTLC6E4JyfkAld4DKmpsbkrK0ttpSafxzc15nHqTVNjepQycUvmivi5NiuyMYtA0qyNo3NOVr9OFfZJmt75WUW7VMhOWtE4fsubj9zRP33SzuaW6LxFB3rWTJj4xSuvXdHyYsOAb%2Fbpj257c%2BOS5s4tvmrim7appHXPputbn8kPlVdURssit194%2FxklXdGr7p3261Hh7uKKUGH0uu2nzi8Pxya1V5qmAUYu4UfygiRwVi0%2FYrQaWIvIdGcQ4pBB7dzU9snCdpLZJF%2FSOXJNjdRPPa0uMhVd2TKurqk5Mq5FXFPXEB0%2F7ucNExvqGieOb6wDIIw7lSbR99oBPqhmvm9ikm0mm7%2Fc7yzPc%2BbV1IrpYEmnX1mlhbZglpActKMVbEo36zBrHWyifBGnSASrw44ZvIhr6bwgFCxiuH4R45HIul%2Bc91p4c3j55tf%2FfvilPddGFx5b8zJqf5X9DCi9v%2Fm10vvcrj6U09uHsg%2F0Ke%2F29invHSBfX7VJ%2BTAv99nwkcNvfNd82xjlI%2F4%2FSu%2BrLyi3%2FObXaPaLTJb0b6xlBfCX%2BDHKMLqgAOoieZk65HLlmXXU56PLK%2FRmGI2e9HQbys4GEGweShSEA0F1mAtak3BQbR1SPGxVVo3K6irbp3YM1ToJV3pGr452r7n58XnrWi6tr79h3tY9yqTy%2FKbYvMvxsYvGRLrPu%2FBCWegef0l%2BcNcmpeGP%2FqIz6oqkNPas06Fd6BEEkMAIbZHRaUaDTKd2RMKCgERqGDdkGNkrBpBGCE4XBIMoIpOMsR4lWko4kLBqJI%2BK5j8Faab66Q897w8yR4ALIR3yqYfpaPGg8hFyDSo70RG06A12%2FoayC49HL1E%2Fs9K3DL2QNXzKGb8fhTCZCCJkRZgzSkcQkogAAdYJoQTf6LXQWZQQHjx2hLz1I7pgEIaGErEHWAIzAAhaezTEW%2BS5kUqBYFHUgcViJEbamxB9uT%2FROLFE8QLBIegdsp5%2BnaSN8spKbara53ErgY4FlFnoIwadmhP5X7VaYcvuz5QHAu8h%2FcO3K%2Bs89eFTJuceP%2Bdft9utd0xUFqDpyj3kqh3K1%2BH6uhrlzX%2FZctHQEckuSNLhJG8MjPTGCNLRbwWDZH%2BFr%2F6Jm7D5hAmyIDMiQ0ZGTrbVkMkqRQ3FUq17vL06HSowmDyctbXd2N5201ln3XjW5a88G6uvnz2nLjJHWMg%2B7W0766bZL10emd02YWJ7G%2BNFAYSwiCGdcx%2BZGTqdRB35BoSomd9sMRrSZYQkAYOKeoYC8S5MM5WnxriwyfZwnAs9I2%2Fh3kG0RVlFY12UNylYiiCAo%2FgZTriVRKwOA5LAgiyuTNnkwQ4Hyucer4lJXb96j39EPHUF%2BJnjK%2F5%2BbriipGXeqiuf3np9%2B4YudA6O3jbYEQv6S2bt37Cle8be7rMBwVgcxo%2BIr4APJkRy7enY7QbIl%2FLTzVK65C8mdrvDIed4PSa5IIE5pbQ8dlABTRX6S6xu1DgHrezj3QjuuaN9%2Fn1P7N541ards5oXtJ3REgwFWsOdE%2Fb9v3W9wlu7a432i6at2N7wzOzzq6tvrAr76ePuDExYn%2BqLI0JEDyCnCdwXdyjui3uFjR%2FVNMjMIUk6ao6YiGZWHZ0i%2FDX75U5H1aEgAOK2LmrkhkxmMUmXJFnOsjrBQR%2FdrXNlOGl7yiCq4Y2Z%2BzTTkbYwT8qwtv73xo0CxS6XhZtDZ7WvpVaAD0ZnlC6fNWF%2Bvigy%2Byj67YoVdz%2FPrAF7Z8wo%2F9mM65SDUhQQLFSOCbslO2RAIOJINwsiAoTMFr0emUykKWYSWc8XiHtk4gMlbe5qgAb7UsMIa0IFwu6bbumd0PqX1%2F72IW5Tjkmn%2F3QfCVmPHEWCwiKd8Cj0e7KGEUURmUU6Ebk1RiCQCHSypSLhfEr%2F%2B2Eqe2hQsaNeALBCVcRlNjI7Fh1Y7Gaz0W60ySYW9pXNXt9QQI0EXB1%2F3PjAIiZPQYprQ3RWgnr3Xd88KXuOu%2FGW5v7s6Kwj6xc5btOZJpzh7hmf2cktXDiKGxPRSYI8MjopD%2BWfMDoJeePRSb4QbvyciNkVzReismdxFD2z4Oyi0vHr6MwOwnTUfEt8ic9KPBFjIvYqgzhkDw%2FxTGK3kxc9YlKPgt969IarH3%2FwwP4nFG9dY%2BPEiY2NdULbnf0v3Hr7wAu3dHR2dnTMm5cy6s2OlKZTy49OL2AW1Ib01FNiGh70BD7YIdHEB79%2FOej1B9UBL%2B6NL0aoFonqQehRdg4ip%2FLxIFqsSMPn2KuMXYbaUNsyJZw1fMrGrnIA6Qpa2n5Y%2BTuAYvg1fgUA6eAP5Nrjj4L8IMFW%2BuJUVye0D51Au5h8T7W6B7CZSZlyNlXeJ75ClUs8XEnM8as%2BEb9qmXpVwDBeWUH%2BLLTzNU5DpKiQug4YJk0jh0pMoyDbnI1lQp0JPk9rzJdhoRy8xZvKwaN4g9Cm5HHsnddbrUub3bCVWHLF4ldiF1wYPjM27aFzzp37w3lvHP3F7rOrUcnw6jY6d1dT86yJ4eiY0sOnTO6%2F%2FYLru%2Bj0cyyamXhHhoZU2lu3GPuhiOexHiQ0HfQPYqfoh9HVJ1B0w2%2F%2FheIgzFQV2SMV52iKgYTCOlIxU1N0cUXaQwR7uWRYkxbXSNDfPYvXhpfEa4MpdD7OPtrg4sg4yUbMNmIRLCjNZEJsvgbgEETRbiYUvqb4syENGQkj%2FJFkkzkxTAQrMmlscsKiQLvUAAeUNb8G7yQ062PCs0QKkEYsI9rR6nzH9imOvcoLeLew9%2FghbKIUT%2BhoLlq5jiPvcYqZDnXNrC6WKXZGjNP8%2BVlGYAXOBfY556p5%2BZaodTT0KC89ZE%2BUXqqiG9pSFPdShT1JcXDoO1XhHnmNmZqia%2BgnXgMYFag1wGbucZ7cAJnQGCmivUCW3ep0GlBamtthAIqVWwGovcRJi9eKLYy8TgmP0%2BBgddahWmkscQqUlpiPo4MhBwPPA1tV5FzFz7cKwm9%2Bd%2BCzzzahATIdd1Du%2FG5GoOPWnR9%2BofQoyl1qHsRXeDuriLez36eUA%2BdUeTlUxtt7N1fgvJMpulHDv1AchOdUhXek4hxNMZBQZI1UzNQUXVzB2vvoeGkj2IAMglnogXTIjaRLBGTZYORGZXcgqMUn8260FqnLBlSM7lL%2BuB%2BVocqr6Rhetkf5tfL7vfj3qKxH%2BSMavZf%2B%2BVuaSiUAhD7DLeIHkgA2yIZCCEdyXJ4cuz0tB9LAW%2BTMK3Ab3QxXJQWpdOWImbyK8arGGFaJqpEG2V2IO%2FyqihEFV1Wm94Xts3tnv8iA1RevaL1x1sDRP56CjrR2UWL1%2FZBiOG0%2BWqzyvXWXXHDpANrEwNWGNfM3DSi%2FfHYJ%2Frbsp%2B8e6j5uKR4aUmlIXgO18Vocrdaz1uOkKrqR6V8oDkKPqsgfqZipKbq4gr0RJcl9kqDwq4yNv3kb1KtYuCSJSmbrqZpIDiOjjbIoSpJTMDbFZEdTTJAFWdIRyZowKGrdjOZBjePIDroW0tZGwh2UUz1yNcPaH1CQ4fikjst3rbt0NcHv%2FagMUij5c2Vc18rz5%2FNZJM3JfMkD1dAaGU3tegXFxQDlWSZTbXkgUGPKKtBBcbEui2SWhkqnxEIQcFgyozFLwnGq7ZUx0g03TH%2FaTYLqcnOkuuX8iaFL8zhXsVAn4a3SSDRSWl1%2FRVfoo3fmXTau%2BubIbfnTo2vnNjQ0TVjXsWQjbb4%2BhL9FfuGvkV%2BcNqai1JldVTJn7srmu%2B7JLfy6KLhqVGhcaeOylsh5lbWnl49r6TrnKPVMv%2FLO%2FazH5ASbVEBr5VQ%2BUtQfAPb2jbbEazY1vfvCE6Xna%2BkHfxhi6RUj001a%2BkAasPTikemClt4lAX%2B3T%2BGCYcUDmqJ%2FlKrwqwogTCEpQjeUQBBOgS2RydU1JDM%2FP2g3GoNBuabG7%2FGMKZPlsC%2FfW50fjVVXsyDp7OxQNJZtNo6aSoF3p%2BS0NFDHPHgbYiBJgQZGv%2FERLZmZ0t5q6wkJKnqMhzBz8MufZG0ZXsZRzHYYrWJk1TDShwoZfiVWbn2rce4L19%2F03NdfPRtr2nHzvKc%2Femdx%2Fd3LDyM4XkaJq%2Bcfm%2FbY8bqFq1fv6FyOvX%2B1oHvwefbOru7Y0zcz5q91cn3Tq52bInXKZx9RCGvWp8UlOEsQzpxD6T%2F05acLVrNap952xtZhP0xWx0%2B0iY%2BfnCrjtT1FbQ2389oqStRWanr34n%2BeflDP00eNTBe09C6rWpeVidoeugYAvcGv8LTaXynTgF0DGRLXuBwA%2Fy5J0T00eaRi6JdU8UmS4qDyuqqwJBTvUMXlkqApuriC9Vdu9UkSBIfk5fPVpZGx4MYuV46oJ%2BkEY0tOTnr6qEKLpcQNmZh%2BSJ2ImdjppB56CnnSKS02%2BRpiJifBU2MEnYC8izsQ2clwI9I%2B1YYLf3Gtkw8SVgdtm4XAwyNdtX46hDAvXCL2GCmnN3ZetuitjjuuvUr5%2F0PfKX9DwuFDDfpT17zfga0rz19x8fIFq84TXdXF99Wdtr1n%2Fm5lz4fKh8pLyPrJR8gyV%2Bhdtuva4%2FMv2Lj1ih27%2Blg74MwMf2tPV9%2FaEPAZUHI97ucl3KK2k5t4PReeOJ319ZfAyRW8pRiS%2BgUt3aSlD6jpeSPTBS29y6C2pIDWK8yCw0JYeIl7wbKhNGJ1pqWZBQEIyYUcNwVKAXHz0vPBYdBQiw8WTxJRTWOGj2%2BK1tf%2FPFpXNzVaf2ojO%2BKOwcEvTpva%2FPOG6c1EmNrUMqWhpRkIfcaHKAN0OZ81eEfOGnzxWQOjb0jBFAZx%2FC%2BzhmCNsJ9hQWsvOLVn0n5GBm1eUrt%2FzK5jR21o%2FOiJKy9AhwzKa%2F6alefjSoYJlXV2dVyL7IwUqpp%2BQes1ytH2RjTouvnWlnFKMOP2oSGVpeD1c2ZST4ByefGmpvMavgVOruA1XMnTC0emC1p6V0B9A0u1np977PkV5qi9zXh%2BBQ8XJOgmziYWsLhqD%2B1vHQZzli2Dxi8VWsCcbXDIRM6dEpOdxEnL%2BCQocxLLTDtnDWdWTT4Wyh0nAU7ot8Herhf%2F%2FuZLf5xv0ulUfvGjOONEDrXMYEgzK%2BCtE9qVsXpQVixvbB7mnLQ8CVqeut5Qc%2F0zNdcJKk9oH6byMk5M5VGJGk2mO108BE7wQmekxuJwGFF%2Bvs6WAeDL0umKLHa6drMgI7HQX0YznaWSNBddcwhCLotpRQ5tBcd%2BThplmiAy%2BBMMx2M6XcOLuERnVGvx%2B3WnH9vn31Wm9Cv3oTPQhPGbvaRDW9Q9dstdd%2FXVrfR7t8jpaBvqQuejTSZZXeCR145%2B8%2B1PDivZbnPyN%2BhT3SphMXhgNARhQWRMoMKEHQ6%2FX19RkWu3V%2BXr9aEchzvgiMYCATCbfxaNmc3YJNDOmfLEZnDT4VwQvFNiQupwHj45Cp00iOdT56kG4bniI7dDo6KTeT2fSk%2BLtyhf7dl5pPfHLSgb4QUvT7nsi2%2BR%2BbhTt2fL%2BU90tDx99FwN5Pu4fbWMBnC3%2FZprdiD9%2FciByqY1XcvYaf26naXlbOCeHGf7BhavuJhFHD0h%2FFXwSAVgZP0Zi5ozAMh6jE0ZWF4vsh39sg5pyx2NKqQzEZ2XGU%2BdFNAgrdc1Ne977elTUafn6kbhr2ed0XJ29tMLqh5sYBENqFX4M4lKD8Q9ehmS1eqmkUWyR8ay7CDxvRTYHVKNZ7qk8YhEdy1YcOklCy%2B67Pqa0tKaiorSGvGlCzavv%2BiCDZu7ykKhsrKqKkDwa%2BHPgkEygQuqIm4KNEUEQjLdBhvobPTrYvM6MzavFyCQ9fpZmoNENQebXw6qkISXvbF5mNVHiE23yjF6xRM27knfvXTUtKZoET%2B%2FfAk7F%2Buray7vKyjOr%2BKHAr4bGHqI3IN7%2BG5S%2BAS7SU0nbeih999Xlbp%2FqtQllG7Sj%2Fp4jIw7kiaIOqTTySBou5KZB5gLq7jGWhvCumKTs7N6sN5L%2Bp1zkG2h8t3HkHQFCVwRmQhIknSCRC8wvD8WUrffQHtNwbWDkz3iI84XlPdRySFI3luLeVIwEfnuWhIEtNuffHstwOzeZBl%2F%2BgzwRczUIGsiggSSZNFlkHRtI0Z%2BoT8E%2BbOoWSnwxY%2FoUzVPdILhSZyRP8ezp2Vz%2BE4SGJn%2FndpNDXwrMFMaMYjsRi%2BqN9Luoz60qB5QH885cqO31JNM8Ua1DBJFgVlJkOt5SRihMGIaeQcIpN7Ap91gROGgt0eWkkvbi2wunXrfKIyCdLA9wszuRplAgHssUq3uc6%2FavnXvvku37cGf9hzou3r%2FLbcAELbTizQXhfm75mXsYF6m6kEvys4gbKuXAofMQuS5LUhtbJnmP9AJy8gdX3yp56m7v%2BAps89kZzPacGPqPmctKUf%2BVkA7vpHbtCsijrgDV9RLQAg9pa0JI9VZmsxW0W%2FVN5vqlE12xKZeO24nRzp2bfoHPRPEf7z2SBs4vvHEBm8ApCxj83oe25YVSSeAEcaCFtqW8B8j5EX48mN%2F%2FIKMjge2AeK7BW0S%2B6EYdkQaJaL3%2BXI8RW5ntmywWIrSafaLika5cnP12dklBpdLzpRy83Knx0heRt66PJxOMvMy82yFPiiEabFCndlkMzXHbNp2YiNNoxZenyxzKUghO%2FCtQOhvro%2FH5DgKdA420DrVfS4oWELdb%2F7qWvq7BuL7XXhXXu9CVyrtGKN5yj0hZNq9ecn93ynPj9q6VMBLtvjQpG%2Be6ps7ebnwys5f3ucNFDzwTXgIxqK0Tx5wFVff9zVyT%2F%2FQ4%2BXsWgfzjp%2B0n6MTYDbdHRriMbs%2FSh7wQyNfQ04lboD45x8nfd7MPgcMBhzF34tPQRpYGbthFXUmWnBEBixim90k62TJikTRaiW6PJLPDTwBLSYu4RpNwn%2B8DhpfWI1CfA%2BzWrZnHP5%2BzefKBrTh0zXKHkmuzliH39q3rwfXHT%2FUN3Nu1gWuZ9Wn05u0pyuGRuJWn14KAMTT4QTpzcPp0q6k3PF0dS8BvtMDAcsjIIiIQGKXQLYPAt8FgTU2uvZ8EQDruB3sL%2FEV7krVDmZIWNNupYoPkxTdQ3NGKoYYgS4mKQ4q76sKS0JxHADfqZupKbq4gq9wuaT6%2FwCVeR0IAAAAAQAAAAEZmiehT9dfDzz1AAkIAAAAAADJQhegAAAAAMnoSqH7DP2oCo0IjQABAAkAAgAAAAAAAHgBY2BkYODo%2FbuCgYGr9zfPv0quXqAIKrgJAJZXBsIAeAFtkQOsGEEQhv%2Fbnd272rZtG0Ft27ZtW1G9dYMiamrbZlgrqN17M89K8uVfTna%2FoRs4AwCUGVBCU0zQl7DAlEIZWoPOfhXUs0BbVQAL1CG0ZepQd9STPdUW9dQ61FGN%2BU5LpOW1pswUpmU0hZj%2BTGOmWnQ2lPNyV2rEoO%2FA%2BmUw0CwATG8cNjkwyXzEYZrG9Of5NUyy%2BXBY7Q4Hm9a8tgCH%2FWU4bOcwPfmsjc7GvDcYPWk7StjU2G8qAf5xwHQE6D%2BzHRXUbqzi96bmrEQNEeim4V965jWnB%2Bho0sNRHnTn7E5H0V3nQAlaAGsawqkxWKfGhDPoO2Ts%2FGdwsk5fIecd011vh9O%2FOaegHO9toBWAfYLM5JBSxvoNquliyEeDvUucbeXvMd55vIqRtTGMJTnzAkP5bdnsXvTX6VGOPkbfYe%2ByRgh%2F6xHoLms6QDmmlvyFPThTB2PEtbczfMbr3XUu1JD7fmqUjaYre68jzpPD3wJIH6QH0RyQ5L6Ui%2FGeGFqDOZLiPj7iXnpkDsKJ5%2BTwO3LmEe8JYecb2fcazoXMC%2FEd4z0J7EFS3MdH3EuPJJX07gom%2Bff4%2FDMcpS1ee85bBLQNGO84cgiqPerpVcghUBEeK%2FS1jzBBfUZbwUv5X%2F7bkOlslqCEwJ5TBw4lBFsBJdRuHA4vYk%2Fown8RLYvLrQAAeAEc0jWMJFcQxvFnto%2F5LjEvHrdbmh2Kji9aPL4839TcKPNAa6mlZUyOmZk6lzbPJ3bo56%2F%2FCz%2BVaqqrat5rY8x7xnzxl3nvo%2B27jFnz8c%2FmI9Nmh2XBdMsilrBitsnD9rI8aiN5DI%2FjSftC9mIf9pMfIB4kHiI%2BhWfQY5aPAYYYYYwpcyfpMMX0aZzBWZzDeVygchGXcBlX8ApexWt4HW%2FgLbzNbnfwLt7DJ%2Fp0TX4%2BUucji1hCnY%2FU%2BcijVB7D46jzkb3Yh%2F3kB4gHiYeIT%2BEZ9JjlY4AhRhhjytxJOkwxfRpncBbncB4XqFzEJVzGFbyCV%2FEaXscbeAtvs9sdvIv3cjmftWavuWs2mg6byt3ooIsFOyx77Kos2kiWsIK%2FUVPDOjawiQmO4CgdxnAcJzClz2PVbNKsy2ZzvoncjQ66qE2kNpHaRJawgr9RU8M6NrCJCY6gNpFjOI4TmNIn36TNfGSH5RrssKtyN%2B59b410iF0sUFO0l2UJtY%2F8jU9rWMcGNjHBEUypf0z8mm7vZLvZaC%2FLzdhmV2XBvpBF25IlLJOvEFfRI%2BNjgCFGGGNK5Rs6Z7Ij%2F45yNzro4m9Ywzo2sIkJjuBj2ZnvLDdjGxntLLWzLGGZfIW4ih4ZHwMMMcIYUyq1s8xkl97bH0y3JkZyM36j%2F%2B58rvTQxwBDjDDGNzyVyX35Ccjd6KCLv2EN69jAJiY4go%2Flfr05F%2BUa7CCzGx10sYA9tiWLxCWs2BfyN%2BIa1rGBTUxwBEfpMIbjOIEpfdjHvGaTd9LJb0duRp2S1O1I3Y4sYZl8hbiKHhkfAwwxwhhTKt%2FQOZPfmY3%2F%2FSs3Y5tNpTpL9ZQeGR8DDDHCGN%2FwbCbdfHO5GbW51OZSm8sSlslXiKvokfExwBAjjDGlUpvLTBY0K5KbiDcT672SbXZY6k7lbnTQxQI1h%2B1FeZTKY3gcT2KvTWUf9pMZIB4kHiI%2BxcQzxGfpfA7P4wW8yG4eT%2FkYYIgRxvgb9TWsYwObmOAITlI%2Fxf7TOIOzOIfzuEDlIi7hMq7gFbyK1%2FA63sBbeJtvdwfv4j28zyaP8QmVL%2FimL%2FENJ5PJHt3RqtyMbbYlPfQxwBAjjPEN9ZksqkMqN6PuV7bZy7LDtuRudNDFwzx1FI%2FhcTzJp73Yh%2F3kB4gHiYeIT%2BEZ9JjlY4AhRhjjb1TWsI4NbGKCIzjJlCmcxhmcxTmcxwVcxCVcxhW8glfxGl7HG3gLbzPxDt7Fe%2FgY%2F%2Begvq0YCAEoCNa1n%2BKVyTUl3Q0uIhoe%2B3DnRfV7nXGOc5zjHOc4xznOcY5znOMc5zjHOc5xjnOc4xznOMc5znGOc5zjHOc4xznOcY5znOMc5zjHOc5xjnOc4xznOMc5znGOc5zjHOc4xznOcY5znOM8XZouTZemS1OAKcAUYAowBZgCTAHm3x31O7p3vNf5c1iXeBkEAQDFcbsJX0IqFBwK7tyEgkPC3R0K7hrXzsIhePPK%2F7c77jPM1yxSPua0WmuDzNcuNmuLtmq7sbyfsUu7De%2Fxu9fvvvDNfN3ioN9j5pq0ximd1hmd1TmlX7iky7qiq7qmG3pgXYd6pMd6oqd6pud6oZd6pdd6p%2Ff6oI%2F6pC%2FKSxvf9F0%2F1LFl1naRcwwzrAu7AHNarbW6oEu6rCu6qmu6ob9Y7xu%2BkbfHH1ZopCk25RVrhXKn4LCO6KiOGfvpd%2BR3is15xXmVWKGRptgaysQKpUwc1hEdVcpEysTI7xTbKHMcKzTSFDtCmVihkab4z0FdI0QQBAEUbRz6XLh3Lc7VcI%2FWN54IuxXFS97oH58%2BMBoclE1usbHHW77wlW985wcHHHLEMSecsUuPXMNRqfzib3pcllj5xd%2B0lSVW5nNIL3nF6389h%2BY5NG3Thja0oQ1taEMb2tCGNrQn%2BQwjrcwxM93gJre4Y89mvsdb3vGeD3zkE5%2F5wle%2B8Z0fHHDIEceccMaOX67wNz3747gObCQAQhCKdjlRzBVD5be7rwAmfOMQsUvPLj279OzSYBks49Ibl97In%2FHCuNDGO%2BNOW6qlWqqlWqqlWqqlWqqYUkwpphTzifnEfII92IM92IM92IM92IM92IM92I%2FD4%2FA4PA6Pw%2BPwODwOj8M%2Ff7kaaDXQyt7K3mqglcCVwNVAq4FWA60GWglZCVkJWQlZCVkJWQlZDbQyqhpoNdAPh3NAwCAAwwDM%2B7b2sg8kCjIO4zAO4zAO4zAO4zAO4zAO4zAO4zAO4zAO4zAO4zAO47AO67AO67AO67AO67AO67AO67AO67AO67AO67AO67AO63AO53AO53AO53AO53AO53AO53AO53AO53AO53AO53AO5xCHOMQhDnGIQxziEIc4xCEOcYhDHOIQhzjEIQ5xiEMd6lCHOtShDnWoQx3qUIc61KEOdahDHepQhzrUoQ6%2Fh%2BP6RpIjiKEoyOPvCARUoK9LctP5ZqXTop7q%2F6H%2F0H%2B4P9yfPz82bdm2Y9ee%2FT355bS3%2FdivDW9reFtDb4beDL0ZejP0ZujN0JuhN0Nvht4MvRl6M%2FRm6M3w1of3PVnJSlaykpWsZCUrWclKVrKSlaxkJStZySpWsYpVrGIVq1jFKlaxilWsYhWrWMUqVrGa1axmNatZzWpWs5rVrGY1q1nNalazmtWsYQ1rWMMa1rCGNaxhDWtYwxrWsIY1rGENa1nLWtaylrWsZS1rWcta1rKWtaxlLWtZyzrWsY51rGMd61jHOtaxjnWsYx3rWMc61rEeTf1o6kdTP%2F84rpMqCKAYhmH8Cfy2JjuLCPiYPDH1Y%2BrH1I%2BpH1M%2Fpn5M%2FZh6FEZhFEZhFEZhFEZhFEZhFFZhFVZhFVZhFVZhFVZhFVbhFE7hFE7hFE7hFE7hFE7hFCKgCChPHQFlc7I52ZxsTgQUAUVAEVAEFAFFQBFQBBQBRUARUAQUAUVAEVAEFAFFQBFQti5bl63L1mXrsnXZuggoAoqAIqAIKAKKgCKgCCgCioAioAgoAoqAIqAIKAKKgCKgCCgCyt5GQBFQBPTlwD7OEIaBKAxSOrmJVZa2TsJcwJ6r0%2F%2B9sBOGnTDshOF%2BDndyXG7k7vfh9%2Bn35fft978Thp2wKuqqqKtarmq58cYbb7zzzjvvfPDBBx988sknn3zxxRdfPHnyVPip8FPhp8JPhZ8KP78czLdxBDAMAMFc%2FbdAk4AERoMS5CpQOW82uWyPHexkJzvZyU52spOd7GQnu9jFLnaxi13sYhe72MVudrOb3exmN7vZzW52s8EGG2ywwQYbbLDBBnvZy172spe97GUve9nLJptssskmm2yyySabbLHFFltsscUWW2yxxX6%2B7P%2BrH%2Fqtf6%2B2Z3u2Z3u2Z3u2Z3u2Z3s%2BO66jKoYBGASA%2FiUFeLO2tqfgvhIgVkOshvj%2F8f%2FjF8VqiL8dqyG%2Bd4klllhiiSWWWGKJJY444ogjjjjiiCOO%2BPua0gPv7paRAHgBLcEDlNxQAADArI3Ydv7Vtm3btm3btm3btm3bD7VvBoIgLXVVqCf0ztXT9dzd3j3cvcX90CN5Snmae%2Fp45np2e356gbeH94HP8Q3x3feH%2FX38NwJwoHigQ2Ba4GBQCK4NfgxVDE0OnQr7w1nCI8P7wi8jdqR4ZGzkRDQSLRmdH%2F0UqxTrEVsbux%2FPHe8b3xh%2FlgglzESJRJfE6MS6ZChZJzkj%2BRouCA9GJKQuMhI5hsZRHR2A7kZ%2FYZWxldhtPDPeFd%2BIPybyE0OIy2SIrEy2IneSX8mvFKB6UpfodPQYeiOTjmnK3GOzsCPYpexaLjdXiRvBHeJ%2B8BX5Lvxe%2FqOACmWEnsJ60SsyYjqxiLhE3CoeE6%2BLL8RvUlRqJXWThkszpJXSbjkq83JaOZ9cXm4gd5IXKZACK4qSSSmiVFWmq0lVUtOr%2BdXyagO1oxbRSM3UsmnFtOpaC62nNkqbo7M60HPppfXaemu9j77X4IwUI49RxqhrtDWOGzeM92Y985lFWWWtcdZia4d10%2FpiU3YZu6%2B91j7rME5xp5szGVAgDcgBioDhYDpYDjaDE%2BAmeAW%2Bp8R%2FA5ajfCcAAAABAAAA3QCKABYAWAAFAAIAEAAvAFwAAAEAAQsAAwABeAF9jgNuRAEYhL%2FaDGoc4DluVNtug5pr8xh7jj3jTpK18pszwBDP9NHTP0IPs1DOexlmtpz3sc9iOe9nmddyPsA8%2BXI%2BqI1COZ%2FkliIXhPkiyDo3vCnG2CaEn0%2B2lH%2BgmfIvotowZa3769ULZST4K%2BcujqTb%2Fj36S4w%2FQmgDF0tWvalemNWLX%2BKSMBvYkhQSLG2FZR%2BafmERIsqPpn7%2ByvxjfMlsTjlihz3OuZE38bTtlAAa%2FTAFAHgBbMEDjJYBAADQ9%2F3nu2zbtm3b5p9t17JdQ7Zt21zmvGXXvJrZe0LA37Cw%2F3lDEBISIVKUaDFixYmXIJHEkkgqmeRSSCmV1NJIK530Msgok8yyyCqb7HLIKZfc8sgrn%2FwKKKiwIooqprgSSiqltDLKKqe8CiqqpLIqqqqmuhpqqqW2Ouqqp74GGmqksSaaaqa5FlpqpbU22mqnvQ466qSzLrrqprs9NpthprNWeWeWReZba6ctQYR5QaTplvvhp4VWm%2BOyt75bZ5fffvljk71uum6fHnpaopfbervhlvfCHnngof36%2BGappx57oq%2BPPpurv34GGGSgwTYYYpihhhthlJFGG%2BODscYbZ4JJJjphoykmm2qaT7445ZkDDnrujRcOOeyY46444qirZtvtnPPOBFG%2BBtFBTBAbxAXxQYJC7rvjrnv%2FxpJXmpPDXpqXaWDg6MKZX5ZaVJycX5TK4lpalA8SdnMyMITSRjxp%2BaVFxaUFqUWZ%2BUVQQWMobcKUlgYAHQ14sAAAeAFFSzVCLEEQ7fpjH113V1ybGPd1KRyiibEhxt1vsj3ZngE9AIfgBmMR5fVk8qElsRjHOHAYW%2BQwyumxct4bKxXkWDEvx7JjdszQNAZcekzi9Zho8oV8NCbnIT%2FfEXNRJwqmlaemnQMbN8E1OE7Mzb%2FP%2F8xzKZrEMA2hl3rQATa0Uxs2bN%2B2f8M2AEpwj5yQBvklvJ3AqRcEaMKrWq%2F19eWakl7NsZbyJoNblqlZc7KywcRbRnBjc00FeF6%2Fenoi05EcG62tsXhkPcdk87BHVC%2BZXleUPrOsUHaUI2tb4y%2F8OwbsTEAJAA%3D%3D%29%20format%28%22woff%22%29%7D%2A%7Bbox%2Dsizing%3Aborder%2Dbox%7Dbody%7Bpadding%3A0%3Bmargin%3A0%3Bfont%2Dfamily%3A%22Open%20Sans%22%2C%22Helvetica%20Neue%22%2CHelvetica%2CArial%2Csans%2Dserif%3Bfont%2Dsize%3A16px%3Bline%2Dheight%3A1%2E5%3Bcolor%3A%23606c71%7Da%7Bcolor%3A%231e6bb8%3Btext%2Ddecoration%3Anone%7Da%3Ahover%7Btext%2Ddecoration%3Aunderline%7D%2Epage%2Dheader%7Bcolor%3A%23fff%3Btext%2Dalign%3Acenter%3Bbackground%2Dcolor%3A%23159957%3Bbackground%2Dimage%3Alinear%2Dgradient%28120deg%2C%23155799%2C%23159957%29%3Bpadding%3A1%2E5rem%202rem%7D%2Eproject%2Dname%7Bmargin%2Dtop%3A0%3Bmargin%2Dbottom%3A%2E1rem%3Bfont%2Dsize%3A2rem%7D%2Eproject%2Dtagline%7Bmargin%2Dbottom%3A2rem%3Bfont%2Dweight%3A400%3Bopacity%3A%2E7%3Bfont%2Dsize%3A1%2E5rem%7D%2Eproject%2Dauthor%2C%2Eproject%2Ddate%7Bfont%2Dweight%3A400%3Bopacity%3A%2E7%3Bfont%2Dsize%3A1%2E2rem%7D%40media%20screen%20and%20%28max%2Dwidth%3A%2042em%29%7B%2Epage%2Dheader%7Bpadding%3A1rem%7D%2Eproject%2Dname%7Bfont%2Dsize%3A1%2E75rem%7D%2Eproject%2Dtagline%7Bfont%2Dsize%3A1%2E2rem%7D%2Eproject%2Dauthor%2C%2Eproject%2Ddate%7Bfont%2Dsize%3A1rem%7D%7D%2Emain%2Dcontent%3Afirst%2Dchild%7Bmargin%2Dtop%3A0%7D%2Emain%2Dcontent%20img%7Bmax%2Dwidth%3A100%25%7D%2Emain%2Dcontent%20h1%2C%2Emain%2Dcontent%20h2%2C%2Emain%2Dcontent%20h3%2C%2Emain%2Dcontent%20h4%2C%2Emain%2Dcontent%20h5%2C%2Emain%2Dcontent%20h6%7Bmargin%2Dtop%3A2rem%3Bmargin%2Dbottom%3A1rem%3Bfont%2Dweight%3A400%3Bcolor%3A%23159957%7D%2Emain%2Dcontent%20p%7Bmargin%2Dbottom%3A1em%7D%2Emain%2Dcontent%20code%7Bpadding%3A2px%204px%3Bfont%2Dfamily%3AConsolas%2C%22Liberation%20Mono%22%2CMenlo%2CCourier%2Cmonospace%3Bcolor%3A%23383e41%3Bbackground%2Dcolor%3A%23f3f6fa%3Bborder%2Dradius%3A%2E3rem%7D%2Emain%2Dcontent%20pre%7Bpadding%3A%2E8rem%3Bmargin%2Dtop%3A0%3Bmargin%2Dbottom%3A1rem%3Bfont%3A1rem%20Consolas%2C%22Liberation%20Mono%22%2CMenlo%2CCourier%2Cmonospace%3Bcolor%3A%23567482%3Bword%2Dwrap%3Anormal%3Bbackground%2Dcolor%3A%23f3f6fa%3Bborder%3Asolid%201px%20%23dce6f0%3Bborder%2Dradius%3A%2E3rem%3Bline%2Dheight%3A1%2E45%3Boverflow%3Aauto%7D%2Emain%2Dcontent%20pre%3E%20code%7Bpadding%3A0%3Bmargin%3A0%3Bfont%2Dsize%3A1rem%3Bcolor%3A%23567482%3Bword%2Dbreak%3Anormal%3Bwhite%2Dspace%3Apre%3Bbackground%3Atransparent%3Bborder%3A0%7D%2Emain%2Dcontent%20pre%20code%2C%2Emain%2Dcontent%20pre%20tt%7Bdisplay%3Ainline%3Bpadding%3A0%3Bline%2Dheight%3Ainherit%3Bword%2Dwrap%3Anormal%3Bbackground%2Dcolor%3Atransparent%3Bborder%3A0%7D%2Emain%2Dcontent%20pre%20code%3Abefore%2C%2Emain%2Dcontent%20pre%20code%3Aafter%2C%2Emain%2Dcontent%20pre%20tt%3Abefore%2C%2Emain%2Dcontent%20pre%20tt%3Aafter%7Bcontent%3Anormal%7D%2Emain%2Dcontent%20ul%2C%2Emain%2Dcontent%20ol%7Bmargin%2Dtop%3A0%7D%2Emain%2Dcontent%20blockquote%7Bpadding%3A0%201rem%3Bmargin%2Dleft%3A0%3Bfont%2Dsize%3A1%2E2rem%3Bcolor%3A%23819198%3Bborder%2Dleft%3A%2E3rem%20solid%20%23dce6f0%7D%2Emain%2Dcontent%20blockquote%3E%3Afirst%2Dchild%7Bmargin%2Dtop%3A0%7D%2Emain%2Dcontent%20blockquote%3E%3Alast%2Dchild%7Bmargin%2Dbottom%3A0%7D%2Emain%2Dcontent%20table%7Bwidth%3A100%25%3Boverflow%3Aauto%3Bword%2Dbreak%3Anormal%3Bword%2Dbreak%3Akeep%2Dall%3Bborder%2Dcollapse%3Acollapse%3Bborder%2Dspacing%3A0%3Bmargin%3A1rem%200%7D%2Emain%2Dcontent%20table%20th%7Bfont%2Dweight%3A700%3Bbackground%2Dcolor%3A%234CAF50%3Bcolor%3A%23fff%7D%2Emain%2Dcontent%20table%20th%2C%2Emain%2Dcontent%20table%20td%7Bpadding%3A%2E5rem%201rem%3Bborder%2Dbottom%3A1px%20solid%20%23e9ebec%3Btext%2Dalign%3Aleft%7D%2Emain%2Dcontent%20table%20tr%3Anth%2Dchild%28odd%29%7Bbackground%2Dcolor%3A%23f2f2f2%7D%2Emain%2Dcontent%20dl%7Bpadding%3A0%7D%2Emain%2Dcontent%20dl%20dt%7Bpadding%3A0%3Bmargin%2Dtop%3A1rem%3Bfont%2Dsize%3A1rem%3Bfont%2Dweight%3A700%7D%2Emain%2Dcontent%20dl%20dd%7Bpadding%3A0%3Bmargin%2Dbottom%3A1rem%7D%2Emain%2Dcontent%20hr%7Bmargin%3A1rem%200%3Bborder%3A0%3Bheight%3A1px%3Bbackground%3A%23aaa%3Bbackground%2Dimage%3Alinear%2Dgradient%28to%20right%2C%23eee%2C%23aaa%2C%23eee%29%7D%2Emain%2Dcontent%2C%2Etoc%7Bmax%2Dwidth%3A64rem%3Bpadding%3A2rem%204rem%3Bmargin%3A0%20auto%3Bfont%2Dsize%3A1%2E1rem%7D%2Etoc%7Bpadding%2Dbottom%3A0%7D%2Etoc%20ul%7Bmargin%2Dbottom%3A0%7D%40media%20screen%20and%20%28min%2Dwidth%3A%2042em%29%20and%20%28max%2Dwidth%3A%2064em%29%7B%2Etoc%7Bpadding%3A2rem%202rem%200%7D%2Emain%2Dcontent%7Bpadding%3A2rem%7D%7D%40media%20screen%20and%20%28max%2Dwidth%3A%2042em%29%7B%2Etoc%7Bpadding%3A2rem%201rem%200%3Bfont%2Dsize%3A1rem%7D%2Emain%2Dcontent%7Bpadding%3A2rem%201rem%3Bfont%2Dsize%3A1rem%7D%2Emain%2Dcontent%20pre%2C%2Emain%2Dcontent%20pre%3E%20code%7Bfont%2Dsize%3A%2E9rem%7D%2Emain%2Dcontent%20blockquote%7Bfont%2Dsize%3A1%2E1rem%7D%7D%2Esite%2Dfooter%7Bpadding%2Dtop%3A2rem%3Bmargin%2Dtop%3A2rem%3Bborder%2Dtop%3Asolid%201px%20%23eff0f1%3Bfont%2Dsize%3A1rem%7D%2Esite%2Dfooter%2Downer%7Bdisplay%3Ablock%3Bfont%2Dweight%3A700%7D%2Esite%2Dfooter%2Dcredits%7Bcolor%3A%23819198%7D%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23a71d5d%3B%20font%2Dweight%3A%20normal%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23795da3%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%230086b3%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%230086b3%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%230086b3%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%234070a0%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23183691%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23969896%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<section class="page-header">
<h1 class="title toc-ignore project-name">Machine learning in R</h1>
<h3 class="subtitle project-tagline">Trainee sessie 02</h3>
<h4 class="author project-author">Longhow Lam</h4>
</section>


<div id="TOC" class="toc">
<ul>
<li><a href="#predictive-modeling-technieken"><span class="toc-section-number">1</span> Predictive modeling technieken</a><ul>
<li><a href="#lineare-regressie"><span class="toc-section-number">1.1</span> lineare regressie</a></li>
<li><a href="#splitsen-in-train-en-test"><span class="toc-section-number">1.2</span> Splitsen in train en test</a></li>
<li><a href="#logistic-regression"><span class="toc-section-number">1.3</span> logistic regression</a></li>
<li><a href="#decision-tree"><span class="toc-section-number">1.4</span> decision tree</a></li>
<li><a href="#random-forest-met-ranger"><span class="toc-section-number">1.5</span> random forest met ranger</a></li>
<li><a href="#xgboost"><span class="toc-section-number">1.6</span> XGboost</a></li>
<li><a href="#predictie-en-validatie"><span class="toc-section-number">1.7</span> predictie en validatie</a></li>
</ul></li>
<li><a href="#the-h2o-package"><span class="toc-section-number">2</span> The h2o package</a><ul>
<li><a href="#h2o-automl"><span class="toc-section-number">2.1</span> h2o automl</a></li>
</ul></li>
<li><a href="#unsupervised-learning"><span class="toc-section-number">3</span> Unsupervised learning</a><ul>
<li><a href="#k-means-clustering"><span class="toc-section-number">3.1</span> k-means Clustering</a></li>
<li><a href="#dbscan"><span class="toc-section-number">3.2</span> DBSCAN</a></li>
<li><a href="#hierarchisch-clusteren"><span class="toc-section-number">3.3</span> Hierarchisch clusteren</a></li>
</ul></li>
<li><a href="#market-basket-analyse"><span class="toc-section-number">4</span> Market basket analyse</a><ul>
<li><a href="#interactive-mba-graphs"><span class="toc-section-number">4.1</span> interactive MBA graphs</a></li>
</ul></li>
<li><a href="#deeplearning"><span class="toc-section-number">5</span> Deeplearning</a><ul>
<li><a href="#een-simpel-model"><span class="toc-section-number">5.1</span> Een simpel model</a></li>
<li><a href="#convolutional-model."><span class="toc-section-number">5.2</span> Convolutional model.</a></li>
</ul></li>
<li><a href="#tiijdreeks-modellen-met-prophet"><span class="toc-section-number">6</span> Tiijdreeks modellen met prophet</a></li>
<li><a href="#the-mlr-package"><span class="toc-section-number">7</span> The mlr package</a><ul>
<li><a href="#specificeren-van-technieken-en-hun-opties"><span class="toc-section-number">7.1</span> specificeren van technieken en hun opties</a></li>
<li><a href="#imputeren-van-missende-waarden"><span class="toc-section-number">7.2</span> Imputeren van missende waarden</a></li>
<li><a href="#het-aanmaken-van-een-task"><span class="toc-section-number">7.3</span> Het aanmaken van een task</a></li>
<li><a href="#variablen-hard-uitsluiten"><span class="toc-section-number">7.4</span> Variablen hard uitsluiten</a></li>
<li><a href="#sample-schema"><span class="toc-section-number">7.5</span> Sample schema</a></li>
<li><a href="#uitvoeren-machine-learning-becnhamrk"><span class="toc-section-number">7.6</span> uitvoeren machine learning becnhamrk</a></li>
<li><a href="#vergelijking-machine-learning-modellen"><span class="toc-section-number">7.7</span> Vergelijking machine learning modellen</a></li>
</ul></li>
</ul>
</div>

<section class="main-content">
<hr />
<p><br></p>
<div id="predictive-modeling-technieken" class="section level1">
<h1><span class="header-section-number">1</span> Predictive modeling technieken</h1>
<hr />
<p>In R kan je veel verschillende predictive modellen fitten. We behandelen een paar in deze sessie. Lineaire regressie met de functie <code>lm</code>, logistische regressie met de functie <code>glm</code>, decision trees met de functie <code>rpart</code> en ensemble van trees met <code>ranger</code> en <code>xgboost</code>. Ook <code>h2o</code> zullen we kort aanstippen. Ik zal deze functies apart behandelen maar we zullen later in de sessie zien met het package <code>mlr</code> hoe je op een meer uniforme manier meerdere modellen kan proberen op een data set.</p>
<div id="lineare-regressie" class="section level2">
<h2><span class="header-section-number">1.1</span> lineare regressie</h2>
<p>We beginnen met simpele lineaire regressie, bruikbaar voor voorspel modellen waar de Target variable continu (numeric) is. We nemen als voorbeeld huizen prijs data die ik gescraped heb van jaap.nl. We willen de prijs van een huis voorspellen basis van een aantal input variabelen/kenmerken.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">jaap =<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;data/Jaap.RDs&quot;</span>)

<span class="kw">library</span>(ggplot2)
<span class="kw">ggplot</span>(jaap, <span class="kw">aes</span>( kamers, prijs)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()
<span class="kw">ggplot</span>(jaap, <span class="kw">aes</span>( Oppervlakte, prijs)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()

<span class="co"># some obvious outliers</span>
jaap =<span class="st"> </span>jaap <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>( prijs <span class="op">&lt;</span><span class="st"> </span><span class="fl">1e7</span> )</code></pre></div>
<p>Een tweetal simpele modellen.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">modelout  =<span class="st"> </span><span class="kw">lm</span>( prijs <span class="op">~</span><span class="st"> </span>kamers               , <span class="dt">data =</span> jaap)
modelout2 =<span class="st"> </span><span class="kw">lm</span>( prijs <span class="op">~</span><span class="st"> </span>kamers <span class="op">+</span><span class="st"> </span>Oppervlakte , <span class="dt">data =</span> jaap) 

modelout
modelout2</code></pre></div>
<p>Modeling functies in R retourneren objecten met van alles er nog wat in. De functie <code>lm</code> levert een object af van de klasse lm.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">class</span>(modelout)
<span class="kw">names</span>(modelout)
modelout<span class="op">$</span>coefficients

modelout
<span class="kw">summary</span>(modelout)
<span class="kw">plot</span>(modelout)</code></pre></div>
<p>Iets mooiere diagnostische plots uit <code>lm</code> objecten krijg je met de library <code>ggfortify</code> die weet hoe je lm objecten moet interpreteren voor ggplot. Dan kan je met de functie <code>autoplot</code> uit ggplot mooiere diagnostische plots maken.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggfortify)
<span class="kw">library</span>(ggplot2)

ggplot2<span class="op">::</span><span class="kw">autoplot</span>(modelout)</code></pre></div>
<p>Je ziet dat er wat outliers in de data zitten, die kunnen we nog eens er uit filteren</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">jaap =<span class="st"> </span>jaap <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>( prijs <span class="op">&lt;</span><span class="st"> </span><span class="dv">1500000</span> )
modelout  =<span class="st"> </span><span class="kw">lm</span>( prijs <span class="op">~</span><span class="st"> </span>kamers               , <span class="dt">data =</span> jaap)
modelout2 =<span class="st"> </span><span class="kw">lm</span>( prijs <span class="op">~</span><span class="st"> </span>kamers <span class="op">+</span><span class="st"> </span>Oppervlakte , <span class="dt">data =</span> jaap) 

<span class="kw">summary</span>(modelout2)</code></pre></div>
<div id="formula-objects" class="section level3">
<h3><span class="header-section-number">1.1.1</span> formula objects</h3>
<p>Modellen in R kan je specificeren met zogenaamde de formula objects. Hieronder zie je een aantal voorbeelden.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## gebruik eerste cijfer van postcode als een locatie variabele
jaap =<span class="st"> </span>jaap <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">PC1Positie =</span> stringr<span class="op">::</span><span class="kw">str_sub</span>(PC,<span class="dv">1</span>,<span class="dv">1</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span><span class="kw">is.na</span>(PC1Positie))

f0 =<span class="st"> </span>prijs <span class="op">~</span><span class="st"> </span>Oppervlakte <span class="op">+</span><span class="st"> </span>kamers 

f0
m0 =<span class="st"> </span><span class="kw">lm</span>(f0, <span class="dt">data =</span> jaap)
<span class="kw">summary</span>(m0)

f1 =<span class="st"> </span>prijs <span class="op">~</span><span class="st"> </span>Oppervlakte <span class="op">+</span><span class="st"> </span>kamers <span class="op">+</span><span class="st"> </span>PC1Positie
m1 =<span class="st"> </span><span class="kw">lm</span>(f1, <span class="dt">data =</span> jaap)
<span class="kw">summary</span>(m1)

## interactie termen
f2 =<span class="st"> </span>prijs <span class="op">~</span><span class="st"> </span>Oppervlakte <span class="op">+</span><span class="st"> </span>kamers <span class="op">+</span><span class="st"> </span>PC1Positie <span class="op">+</span><span class="st"> </span>Oppervlakte<span class="op">*</span>PC1Positie
m2 =<span class="st"> </span><span class="kw">lm</span>(f2, <span class="dt">data =</span> jaap)
<span class="kw">summary</span>(m2)

## interactie termen
f3 =<span class="st"> </span>prijs <span class="op">~</span><span class="st"> </span>Oppervlakte <span class="op">+</span><span class="st"> </span>kamers <span class="op">+</span><span class="st"> </span>PC1Positie <span class="op">+</span><span class="st"> </span>Oppervlakte<span class="op">*</span>PC1Positie <span class="op">+</span><span class="st">  </span>Oppervlakte<span class="op">*</span>kamers
m3 =<span class="st"> </span><span class="kw">lm</span>(f3, <span class="dt">data =</span> jaap)
<span class="kw">summary</span>(m3)

##  interactietermen
f4 =<span class="st"> </span>prijs <span class="op">~</span><span class="st"> </span>Oppervlakte<span class="op">*</span>kamers<span class="op">*</span>PC1Positie 
m4 =<span class="st"> </span><span class="kw">lm</span>(f4, <span class="dt">data =</span> jaap)
<span class="kw">summary</span>(m4)</code></pre></div>
<p>Als je verschillende model objecten hebt gemaakt kan je de functie <code>anova</code> gebruiken om ze met elkaar te vergelijken. Dit gebeurt met behulp van F statistics.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(m0, m1, m2, m3, m4)</code></pre></div>
<p>Nog een paar voorbeelden van formule objecten.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">##  termen weglaten
f5 =<span class="st"> </span>prijs <span class="op">~</span><span class="st"> </span>Oppervlakte<span class="op">*</span>kamers<span class="op">*</span>PC1Positie <span class="op">-</span><span class="st">  </span>Oppervlakte<span class="op">:</span>kamers<span class="op">:</span>PC1Positie 
m5 =<span class="st"> </span><span class="kw">lm</span>(f5, <span class="dt">data =</span> jaap)
<span class="kw">summary</span>(m5)

## een target en de rest van de variabelen als inputs
f6 =<span class="st"> </span>prijs <span class="op">~</span><span class="st"> </span>. <span class="op">-</span>PC6 <span class="op">-</span>PC
m6 =<span class="st"> </span><span class="kw">lm</span>(f6, <span class="dt">data =</span> jaap)
<span class="kw">summary</span>(m6)</code></pre></div>
<p><br></p>
<hr />
</div>
<div id="oefening-1" class="section level3">
<h3><span class="header-section-number">1.1.2</span> <strong>OEFENING 1</strong></h3>
<ol style="list-style-type: decimal">
<li>Importeer de AllCarsGasPedaal.Rds auto data set</li>
<li>Maak een paar linear regressie modellen om de VraagPrijs te voorspellen, probeer wat verschillende variabelen al dan niet mee te nemen.</li>
<li>Als je Merk meeneemt als input dan zijn er eigenlijk te veel merken. Kan je alleen de top 32 merken meenemen.</li>
</ol>
<hr />
<p><br></p>
</div>
<div id="buckets-linear-constant-en-splines" class="section level3">
<h3><span class="header-section-number">1.1.3</span> Buckets / linear constant en splines</h3>
<p>Als een input variable niet linear is m.b.t. de target kan je deze niet-lineariteit modelleren met buckets (linear constante stukken) of met splines.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(splines)

## In een scatterplot kan je wellicht enige vorm van niet lineariteit zijn.
jaap <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(prijs <span class="op">&lt;</span><span class="st"> </span><span class="dv">1000000</span>, Oppervlakte <span class="op">&lt;</span><span class="st"> </span><span class="dv">1500</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>Oppervlakte, <span class="dt">y=</span> prijs)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>()

mybreaks =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1000</span>, <span class="dt">by =</span> <span class="dv">25</span>)
jaap =<span class="st"> </span>jaap <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">OppervlakteBucket =</span> <span class="kw">cut</span>(Oppervlakte, <span class="dt">breaks =</span> mybreaks))


m1 =<span class="st"> </span><span class="kw">lm</span>(prijs <span class="op">~</span><span class="st"> </span>Oppervlakte, <span class="dt">data =</span> jaap)
m2 =<span class="st"> </span><span class="kw">lm</span>(prijs <span class="op">~</span><span class="st"> </span>OppervlakteBucket, <span class="dt">data =</span> jaap)
m3 =<span class="st"> </span><span class="kw">lm</span>(prijs <span class="op">~</span><span class="st"> </span><span class="kw">ns</span>(Oppervlakte,<span class="dv">6</span>), <span class="dt">data =</span> jaap)

<span class="kw">summary</span>(m1)
<span class="kw">summary</span>(m2)
<span class="kw">summary</span>(m3)</code></pre></div>
</div>
<div id="predicties" class="section level3">
<h3><span class="header-section-number">1.1.4</span> predicties</h3>
<p>Met de functie <code>predict</code> kunnen we nieuwe huizen scoren, dat wil zeggen de prijs van andere huizen die niet in de training data set zaten voorspellen.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">NieuweHuizen =<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">Oppervlakte =</span> <span class="kw">seq</span>(<span class="dv">20</span>, <span class="dv">250</span>, <span class="dt">l =</span> <span class="dv">100</span>)
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
   <span class="dt">OppervlakteBucket =</span> <span class="kw">cut</span>(Oppervlakte, <span class="dt">breaks =</span> mybreaks)
  )

<span class="co"># Bucket predicties</span>
prijs2 =<span class="st"> </span><span class="kw">predict</span>(m2, <span class="dt">newdata =</span> NieuweHuizen)
NieuweHuizen<span class="op">$</span>prijs2 =<span class="st"> </span>prijs2

<span class="kw">ggplot</span>(NieuweHuizen, <span class="kw">aes</span>(<span class="dt">x=</span>Oppervlakte, <span class="dt">y =</span> prijs2)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()

<span class="co"># Spline predicties</span>
prijs3 =<span class="st"> </span><span class="kw">predict</span>(m3, <span class="dt">newdata =</span> NieuweHuizen)
NieuweHuizen<span class="op">$</span>prijs3 =<span class="st"> </span>prijs3


<span class="kw">ggplot</span>(NieuweHuizen, <span class="kw">aes</span>(<span class="dt">x=</span>Oppervlakte, <span class="dt">y =</span> prijs2)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y=</span>prijs3), <span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
<p><br></p>
<hr />
</div>
<div id="oefening-2" class="section level3">
<h3><span class="header-section-number">1.1.5</span> <strong>OEFENING 2</strong></h3>
<ol style="list-style-type: decimal">
<li>Als je een spline model voor KMStand gebruikt, maakt dat veel verschil vergeleken met gewoon KMStand?</li>
</ol>
<hr />
<p><br></p>
</div>
</div>
<div id="splitsen-in-train-en-test" class="section level2">
<h2><span class="header-section-number">1.2</span> Splitsen in train en test</h2>
<p>Het is gebruikelijk om een data set random te splitsen in een train en test set. Op de train set wordt een predictive model getraind. En het model dat we getraind hebben testen we op de test set.</p>
<p>We gebruiken hier een copy van de titanic set omdat we de data iets wijzigen.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">perc =<span class="st"> </span><span class="fl">0.80</span>

## maak een categorische kolom van survived
myTitan =<span class="st"> </span>titanic<span class="op">::</span>titanic_train
myTitan =<span class="st"> </span>myTitan <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(
  <span class="dt">Survived =</span> <span class="kw">ifelse</span>(Survived <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;N&quot;</span>, <span class="st">&quot;Y&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>as.factor
)

## haal missende waarden weg, we gaan ons hier even niet vermoeien met missende waarden :-)
myTitan =<span class="st"> </span>myTitan <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(
    <span class="op">!</span><span class="kw">is.na</span>(Age)
  )

N =<span class="st"> </span><span class="kw">dim</span>(myTitan)[<span class="dv">1</span>]
train =<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>N, <span class="dt">size =</span> <span class="kw">floor</span>(perc<span class="op">*</span>N))

TTrain =<span class="st"> </span>myTitan[train,]
TTest =<span class="st"> </span>myTitan[<span class="op">-</span>train,]</code></pre></div>
<p>Dus we hebben nu een train en test set en we zien een verhouding van survived die verschillend zijn in train en test omdat we hier redelijk kleine data setjes hebben.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(TTrain<span class="op">$</span>Survived)
<span class="kw">table</span>(TTest<span class="op">$</span>Survived)</code></pre></div>
<p>Bovenstaande code was op de oude manier in R een train en test set maken, er is zijn packages die dat voor je doen, een van die packages is <code>rsample</code>. Dit packages is veel algemener en kan ook gebruikt worden voor cross validation splits.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rsample)
myTitanSplit =<span class="st"> </span><span class="kw">initial_split</span>(myTitan, <span class="dt">prop =</span> <span class="fl">0.8</span>)
TTrain =<span class="st"> </span><span class="kw">training</span>(myTitanSplit)
TTest =<span class="st"> </span><span class="kw">testing</span>(myTitanSplit)</code></pre></div>
</div>
<div id="logistic-regression" class="section level2">
<h2><span class="header-section-number">1.3</span> logistic regression</h2>
<p>Een logistic regression is een van de simpelste predictive modellen om mee te beginnen als je classificatie wilt doen. We gaan uit van een binaire Target (Y /N). We gebruiken de <code>TTrain</code> data set die we zojuist gemaakt hebben om een model te fitten.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">out.glm =<span class="st"> </span><span class="kw">glm</span>(Survived <span class="op">~</span><span class="st"> </span>Sex <span class="op">+</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>Pclass,  <span class="dt">data =</span> TTrain , <span class="dt">family =</span> binomial)
<span class="kw">summary</span>(out.glm)</code></pre></div>
</div>
<div id="decision-tree" class="section level2">
<h2><span class="header-section-number">1.4</span> decision tree</h2>
<p>Een decision tree genereert op basis van een algoritme regels die je kan gebruiken om te classificeren. Het is een eenvoudig algoritme dat per variabele kijkt hoe deze te gebruiken om de data set in twee stukken te splitsen (kan ook meer, maar gebruikelijk is twee).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rpart)
tree.out =<span class="st"> </span><span class="kw">rpart</span>(Survived <span class="op">~</span><span class="st"> </span>Sex <span class="op">+</span><span class="st"> </span>Age <span class="op">+</span>Pclass, <span class="dt">data =</span> TTrain)

<span class="kw">plot</span>(tree.out)
<span class="kw">text</span>(tree.out, <span class="dt">use.n =</span> <span class="ot">TRUE</span>)

<span class="kw">fancyRpartPlot</span>(tree.out)

### larger trees with complexity parameter
tree.out2 =<span class="st"> </span><span class="kw">rpart</span>(Survived <span class="op">~</span><span class="st"> </span>Sex <span class="op">+</span><span class="st"> </span>Age <span class="op">+</span>Pclass, <span class="dt">data =</span> TTrain, <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">cp=</span><span class="fl">0.0005</span>))
<span class="kw">fancyRpartPlot</span>(tree.out2)</code></pre></div>
<p>Met visNetwork kan je nog een mooiere interactieve decison tree maken.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(visNetwork)
<span class="kw">visTree</span>(tree.out, <span class="dt">height =</span> <span class="st">&quot;800px&quot;</span>, <span class="dt">nodesPopSize =</span> <span class="ot">TRUE</span>, <span class="dt">minNodeSize =</span> <span class="dv">10</span>, <span class="dt">maxNodeSize =</span> <span class="dv">30</span>)</code></pre></div>
</div>
<div id="random-forest-met-ranger" class="section level2">
<h2><span class="header-section-number">1.5</span> random forest met ranger</h2>
<p>Een random forest is een zogenaamde ensemble model. Het is de combinatie van (veel) verschillende decision trees. In R kan je met verschillende packages random forests fitten. Het package <code>ranger</code> is hier een van.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ranger.out =<span class="st"> </span><span class="kw">ranger</span>( Survived <span class="op">~</span><span class="st"> </span>Sex <span class="op">+</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>Pclass, <span class="dt">data =</span> TTrain , <span class="dt">probability =</span> <span class="ot">TRUE</span>)
ranger.out</code></pre></div>
</div>
<div id="xgboost" class="section level2">
<h2><span class="header-section-number">1.6</span> XGboost</h2>
<p>Extreme gradient boosting wordt de laatste tijd ook veel gebruikt in Kaggle competities. Zoals bij random forests is een xgboost model ook een ensemble van decision trees, maar de trees zijn nu niet onafhankelijk van elkaar. Eerst wordt een tree gefit, daarna een andere op basis van de eerste, etc.</p>
<p>Met de library <code>xgboost</code> kan je in R extreme gradient boosting modellen fitten. De aanroep is anders dan wat we tot nu toe gezien hebben. De <code>xgboost</code> functie moet een matrix met input variabelen worden meegegeven.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(Matrix)
Titan_Inputmatrix =<span class="st"> </span><span class="kw">sparse.model.matrix</span>( Survived <span class="op">~</span><span class="st"> </span>Sex <span class="op">+</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>Pclass, <span class="dt">data =</span> TTrain)

## hoe ziet zo'n input matrix er uit? eerste 15 rijen
Titan_Inputmatrix[<span class="dv">1</span><span class="op">:</span><span class="dv">15</span>,]

## nu kan je de xgboost aanroepen met input matrix en label
xgboost.out =<span class="st"> </span><span class="kw">xgboost</span>(<span class="dt">data =</span> Titan_Inputmatrix, <span class="dt">label =</span> TTrain<span class="op">$</span>Survived, <span class="dt">nrounds =</span> <span class="dv">25</span>)</code></pre></div>
<p>Bovenstaande aanroep fit een regression tree, dat is niet wat we willen we willen een binairy classificatie, label moet dan wel numeriek 0 / 1 zijn.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">param =<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">objective =</span> <span class="st">'binary:logistic'</span>,
  <span class="dt">eval_metric =</span> <span class="st">'auc'</span>
)

xgboost.out2 =<span class="st"> </span><span class="kw">xgboost</span>(
  <span class="dt">params=</span>param,
  Titan_Inputmatrix,
  <span class="dt">label =</span> <span class="kw">as.integer</span>(TTrain<span class="op">$</span>Survived) <span class="op">-</span><span class="dv">1</span> , <span class="dt">nrounds =</span> <span class="dv">25</span>)</code></pre></div>
<p><br></p>
<hr />
<div id="oefening-3" class="section level3">
<h3><span class="header-section-number">1.6.1</span> <strong>OEFENING 3</strong></h3>
<ol style="list-style-type: decimal">
<li>Gebruik de auto uit de AllCarsGasPedaal data set weer, train nu ook een decision tree en maak een interactieve tree plot</li>
</ol>
<hr />
<p><br></p>
</div>
</div>
<div id="predictie-en-validatie" class="section level2">
<h2><span class="header-section-number">1.7</span> predictie en validatie</h2>
<p>Met een test set kan je bepalen hoe goed een model is. Gebruik het model object van een modelfit om een test set te scoren en de scores met de ware uitkomsten te vergelijken.</p>
<div id="predicties-1" class="section level3">
<h3><span class="header-section-number">1.7.1</span> predicties</h3>
<p>Voor binaire classificaties is het handig om response kansen uit te rekenen. Voor logistische regressie met <code>glm</code> gebeurt dit niet automatisch, we zetten extra <code>type = response</code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred_GLM =<span class="st"> </span><span class="kw">predict</span>(out.glm, <span class="dt">newdata =</span> TTest, <span class="dt">type=</span><span class="st">'response'</span>)
<span class="kw">hist</span>(pred_GLM)</code></pre></div>
<p>Voorspelling van de decision tree, en random forest ranger.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## LET OP! prediecties van tree zitten in een matrix
pred_tree =<span class="st"> </span><span class="kw">predict</span>(tree.out, <span class="dt">newdata =</span> TTest)
pred_tree[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,]
<span class="kw">hist</span>(pred_tree[,<span class="dv">2</span>])

## LET OP hier is argument data ipv newdata en je krijgt een lijst terug 
pred_ranger =<span class="st"> </span><span class="kw">predict</span>(ranger.out, <span class="dt">data =</span> TTest)
pred_ranger<span class="op">$</span>predictions[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,]
<span class="kw">hist</span>(pred_ranger<span class="op">$</span>predictions[,<span class="dv">2</span>])</code></pre></div>
<p>En voor xgboost moet je ook de test set als matrix veranderen</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Titan_Testmatrix =<span class="st"> </span><span class="kw">sparse.model.matrix</span>( Survived <span class="op">~</span><span class="st"> </span>Sex <span class="op">+</span><span class="st"> </span>Age <span class="op">+</span>Pclass, <span class="dt">data =</span> TTest)
pred_xgboost =<span class="st"> </span><span class="kw">predict</span>(xgboost.out2, <span class="dt">newdata =</span> Titan_Testmatrix)

## hier zitten de predicties een vector
<span class="kw">hist</span>(pred_xgboost)</code></pre></div>
</div>
<div id="variable-importance-in-trees" class="section level3">
<h3><span class="header-section-number">1.7.2</span> Variable importance in trees</h3>
<p>Als je een tree of ensemble van trees hebt getrained kan je een idee krijgen welke variabelen in het model belangrijk zijn geweest in het trainings proces. Laten we voor de tree modellen die we hierboven getraind hebben de variable importance zien.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># enkele decision tree</span>
tree.out<span class="op">$</span>variable.importance

<span class="co"># ranger random forest</span>
ranger.out<span class="op">$</span>predict

<span class="co"># xgboost</span>
imp =<span class="st"> </span><span class="kw">xgb.importance</span>( <span class="kw">colnames</span>(Titan_Inputmatrix), <span class="dt">model =</span> xgboost.out2)
imp
<span class="kw">xgb.plot.importance</span>(imp)</code></pre></div>
<p>SHAP (SHapley Additive exPlanation values) per feature</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">xgb.plot.shap</span>(  Titan_Inputmatrix, <span class="dt">model =</span> xgboost.out2, <span class="dt">top_n=</span><span class="dv">100</span>)</code></pre></div>
</div>
<div id="lift-percentages" class="section level3">
<h3><span class="header-section-number">1.7.3</span> Lift percentages</h3>
<p>Als we geen model hebben kan je de overall survival kans uitrekenen (op de test set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## if you know nothing :-)
TTest =<span class="st"> </span>TTest <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">target =</span> <span class="kw">ifelse</span>(Survived <span class="op">==</span><span class="st"> &quot;Y&quot;</span>,<span class="dv">1</span>,<span class="dv">0</span>))
TTest <span class="op">%&gt;%</span><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">target =</span> <span class="kw">mean</span>(target))</code></pre></div>
<p>Als we een goed model hebben zullen de survival kansen hoger liggen voor mensen met een hogere score. Laten we de GLM (logistische regressie model) predcities in tien stukken (decielen) opdelen, en per deciel de survival kans uitrekenen.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">testpr =<span class="st"> </span><span class="kw">predict</span>(out.glm, <span class="dt">newdata =</span> TTest, <span class="dt">type=</span><span class="st">'response'</span>)
TTest<span class="op">$</span>predictieGLM =<span class="st"> </span>testpr

### deel de GLM predicties op in tien even grote stukken
TTest =<span class="st"> </span>TTest <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">percPredictie =</span> <span class="kw">cut</span>(
      predictieGLM, 
      <span class="dt">breaks =</span> <span class="kw">quantile</span>(
        predictieGLM, 
        <span class="dt">probs =</span> (<span class="dv">0</span><span class="op">:</span><span class="dv">10</span>)<span class="op">/</span><span class="dv">10</span>
      )
    )
  )

## if you have a score!!!
TTest <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(percPredictie) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarise</span>(
    <span class="dt">N =</span> <span class="kw">n</span>(),
    <span class="dt">target =</span> <span class="kw">mean</span>(target)
  )</code></pre></div>
</div>
<div id="roc-curves-and-hit-rates" class="section level3">
<h3><span class="header-section-number">1.7.4</span> roc curves and hit rates</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rocResultTEST =<span class="st"> </span><span class="kw">roc</span>(Survived  <span class="op">~</span><span class="st"> </span>predictieGLM, <span class="dt">data =</span> TTest , <span class="dt">auc=</span><span class="ot">TRUE</span>, <span class="dt">ci =</span><span class="ot">TRUE</span>)
<span class="kw">plot</span>(rocResultTEST)

## HITRATES 
TTest <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(predictieGLM, target))  <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">slope =</span> <span class="dv">1</span>,<span class="dt">intercept =</span> <span class="dv">0</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;survived rate rate on test set&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">scale_y_continuous</span>(<span class="dt">limits=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</code></pre></div>
<p><br></p>
</div>
</div>
</div>
<div id="the-h2o-package" class="section level1">
<h1><span class="header-section-number">2</span> The h2o package</h1>
<hr />
<p>H2O is een schaalbaar machine learning platform die je vanuit R kan bedienen. Het bevat veel machine learning algoritmes, en voor grotere sets waar gewoon R moeite mee heeft kan h2o een uitkomst bieden. H2o heeft een eigen ‘executie engine’ geschreven in java. Bij het opstarten van h2o vanuit R wordt dan ook een apart h2o proces opgestart waar je data vanuit R naar toe moet uploaden om daar de algoritmes op los te laten.</p>
<p>Als je h2o opstart is er ook een eigen GUI, daar kan je naar toe localhost:54321 (standard 54321 port).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(h2o)

<span class="co"># initialiseer h2o via R</span>
<span class="co">#h2o.init(nthreads=-1, port=54323, startH2O = FALSE)</span>
<span class="kw">h2o.init</span>()</code></pre></div>
<p>Upload een R data set naar h2o: titanic train en test voorbeeldje. Zorg ervoor dat categorische kolommen in je data van het type factor zijn als je ze wilt meenmenem voor modeling.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">TTrain =<span class="st"> </span>TTrain <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate_if</span>(
    is.character, as.factor
  )

TTest =<span class="st"> </span>TTest <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate_if</span>(
    is.character, as.factor
  )

TTrain<span class="op">$</span>Survived =<span class="st"> </span><span class="kw">as.factor</span>(TTrain<span class="op">$</span>Survived)
TTest<span class="op">$</span>Survived =<span class="st"> </span><span class="kw">as.factor</span>(TTest<span class="op">$</span>Survived)

### breng R data naar h2o. (Er is nu dubbel data in R en in h2o !!!)
ttrain.h2o =<span class="st"> </span><span class="kw">as.h2o</span>(TTrain) 
ttest.h2o =<span class="st"> </span><span class="kw">as.h2o</span>(TTest)

### Je kan ook direct text files inlezen in h2o met 
<span class="co">#h2o.importFile(path=&quot;C:\een file.txt&quot;, sep=&quot;,&quot;)</span>

### welke files zijn er in h2o
<span class="kw">h2o.ls</span>()</code></pre></div>
<p>Er zijn diverse modellen die je kan trainen, we zullen hier een aantal laten zien, neural netwerks, boosting en random forests.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## model op titanic
NNmodel =<span class="st"> </span><span class="kw">h2o.deeplearning</span>(
  <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">5</span><span class="op">:</span><span class="dv">6</span>),
  <span class="dt">y =</span> <span class="st">&quot;Survived&quot;</span>,
  <span class="dt">training_frame  =</span> ttrain.h2o,
  <span class="dt">validation_frame =</span> ttest.h2o,
  <span class="dt">hidden =</span> <span class="dv">5</span>,
  <span class="dt">epochs =</span> <span class="dv">250</span>,
  <span class="dt">variable_importances =</span> <span class="ot">TRUE</span>
)

<span class="kw">show</span>(NNmodel)
<span class="kw">h2o.varimp</span>(NNmodel)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">GBMmodel =<span class="st"> </span><span class="kw">h2o.gbm</span>(
  <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">5</span><span class="op">:</span><span class="dv">6</span>),
  <span class="dt">y =</span> <span class="st">&quot;Survived&quot;</span>,
  <span class="dt">training_frame  =</span> ttrain.h2o,
  <span class="dt">validation_frame =</span> ttest.h2o
  )
GBMmodel

RFmodel =<span class="st"> </span><span class="kw">h2o.randomForest</span>(
  <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">5</span><span class="op">:</span><span class="dv">6</span>),
  <span class="dt">y =</span> <span class="st">&quot;Survived&quot;</span>,
  <span class="dt">training_frame  =</span> ttrain.h2o,
  <span class="dt">validation_frame =</span> ttest.h2o
  )
RFmodel

<span class="kw">h2o.varimp_plot</span>(RFmodel)</code></pre></div>
<p>Grid search in h2o. Je kan makkelijk modellen fine-tunen, in een grid kan je verschillende waarden van hyperparameters proberen.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">RFmodelGrid =<span class="st"> </span><span class="kw">h2o.grid</span>(
  <span class="st">&quot;randomForest&quot;</span>,
  <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">5</span><span class="op">:</span><span class="dv">6</span>),
  <span class="dt">y =</span> <span class="st">&quot;Survived&quot;</span>,
  <span class="dt">training_frame  =</span> ttrain.h2o,
  <span class="dt">validation_frame =</span> ttest.h2o,
  <span class="dt">hyper_params =</span> <span class="kw">list</span>(
    <span class="dt">ntrees =</span><span class="kw">c</span>(<span class="dv">50</span>,<span class="dv">100</span>), 
    <span class="dt">mtries =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>)
  )
)

<span class="co">#overzicht van het grid, gesorteerd op logloss</span>
RFmodelGrid</code></pre></div>
<div id="h2o-automl" class="section level2">
<h2><span class="header-section-number">2.1</span> h2o automl</h2>
<p>De <code>automl</code> functionaliteit in h2o maakt het je helemaal makkelijk als je op zoek bent naar het beste voorspellende model. Deze functie traint en cross valideert random forests, extremely randomized forests, GBM’s, Neural Nets en stacked ensembles.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Geef het maximaal 30 seconden tijd
out =<span class="st"> </span><span class="kw">h2o.automl</span>(
    <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">5</span><span class="op">:</span><span class="dv">6</span>),
  <span class="dt">y =</span> <span class="st">&quot;Survived&quot;</span>,
  <span class="dt">training_frame  =</span> ttrain.h2o,
  <span class="dt">validation_frame =</span> ttest.h2o,
  <span class="dt">max_runtime_secs =</span> <span class="dv">30</span>
)

## out is nu een zgn H2OAutoML object met alle resultaten
out

## iets overzichtelijker output is de leaderboard
out<span class="op">@</span>leaderboard

WINNER =<span class="st"> </span>out<span class="op">@</span>leader
WINNER</code></pre></div>
<p>Overleef ik de titanic?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ik =<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">Pclass =</span> <span class="dv">1</span>, 
  <span class="dt">Sex =</span> <span class="st">&quot;female&quot;</span>, 
  <span class="dt">Age =</span> <span class="dv">26</span>
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>as.h2o

<span class="kw">predict</span>(WINNER, ik)</code></pre></div>
<p>Geef resources terug door h2o af te sluiten als je het niet meer nodig hebt.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">h2o.shutdown</span>(<span class="dt">prompt =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><br></p>
<hr />
<div id="oefening-4" class="section level3">
<h3><span class="header-section-number">2.1.1</span> <strong>OEFENING 4</strong></h3>
<ol style="list-style-type: decimal">
<li><p>Gebruik de jaap.RDs data set, lees hem in en maak een binaire target ‘Duur’: Y als prijs &gt; 800.000 en N als prijs &lt;= 800.000</p></li>
<li><p>train een h2o randomforest, splits in 80/20 train test set</p></li>
<li><p>Laat een variable importance plot zien</p></li>
<li><p>Op de test set: Wat voor AUROC zie je? Wat is de top10% lift?</p></li>
</ol>
<hr />
<p><br></p>
<p><br></p>
</div>
</div>
</div>
<div id="unsupervised-learning" class="section level1">
<h1><span class="header-section-number">3</span> Unsupervised learning</h1>
<hr />
<p>De bovenstaande code was gericht op predictive modeling, ook wel supervised learning genoemd: met input variabelen een target variable proberen te voorspellen. In deze sectie zullen we een tweetal technieken laten zien waar geen target variabele is, ook wel unsupervised learning genoemd.</p>
<div id="k-means-clustering" class="section level2">
<h2><span class="header-section-number">3.1</span> k-means Clustering</h2>
<p>Dit is een van de bekendste clustering methode. Je dient een aantal clusters van te voren op te geven, de <em>k</em>, het algoritme gaat dan elke observatie aan een van de k clusters toekennen.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mycars =<span class="st"> </span>mtcars <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span> (mpg, wt)
cars.cluster =<span class="st"> </span><span class="kw">kmeans</span>(mycars, <span class="dv">5</span>)
cars.cluster

<span class="co"># in het ouput object zit informatie over het gefitte kmeans algoritme</span>
mycars<span class="op">$</span>cluster =<span class="st"> </span>cars.cluster<span class="op">$</span>cluster
mycars

<span class="kw">plot</span>(mycars<span class="op">$</span>mpg, mycars<span class="op">$</span>wt, <span class="dt">col =</span> cars.cluster<span class="op">$</span>cluster )
<span class="kw">points</span>(cars.cluster<span class="op">$</span>centers, <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="dt">pch =</span> <span class="dv">8</span>, <span class="dt">cex =</span> <span class="dv">2</span>)</code></pre></div>
<p>Met het <code>h2o</code> package kan je ook k-means clustering doen, dit is niet alleen sneller maar kan ook meteen factor variabelen aan, in de <code>kmeans</code> van R kan dat niet. Start indien nodig h2o.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(h2o)
<span class="co">#h2o.init(nthreads=-1, port=54323, startH2O = FALSE)</span>
<span class="kw">h2o.init</span>()</code></pre></div>
<p>Breng data naar h2o, we gebruiken nu de sample data set mtcars in R maar we maken nog 1 extra factor kolom aan.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># am is de transimssie: 0 is automat en 1 is handgeschakeld, is eig</span>
mycars =<span class="st"> </span>mtcars <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">am =</span> <span class="kw">as.factor</span>(am))
cars.h2o =<span class="st"> </span><span class="kw">as.h2o</span>(mycars)</code></pre></div>
<p>Laat het algoritme zelf bepalen hoeveel clusters er in de data zijn.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cars_clustering =<span class="st"> </span><span class="kw">h2o.kmeans</span>(cars.h2o,  <span class="dt">k =</span> <span class="dv">10</span>, <span class="dt">estimate_k =</span> <span class="ot">TRUE</span>)
cars_clustering</code></pre></div>
<p>na het trainen heb je een h2o cluster object met diverse informatie</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cars_clustering<span class="op">@</span>model
<span class="kw">h2o.cluster_sizes</span>(cars_clustering)
<span class="kw">h2o.centers</span>(cars_clustering)

## met h2o.predict kan je data scoren: bepalen tot welk cluster een observatie hoort en weer terug naar R halen
cluster_membership =<span class="st"> </span><span class="kw">h2o.predict</span>(
  cars_clustering,
  <span class="dt">newdata =</span> cars.h2o
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">as.data.frame</span>()</code></pre></div>
</div>
<div id="dbscan" class="section level2">
<h2><span class="header-section-number">3.2</span> DBSCAN</h2>
<p>Density-based Spatial Clustering of Applications with Noise (DBSCAN), which does not make assumptions about spherical clusters like k-means, nor does it partition the dataset into hierarchies that require a manual cut-off point. As its name implies, density-based clustering assigns cluster labels based on dense regions of points.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dbscan)

## wat data, halve manen, hoeveel clusrers zijn dit?
x1 =<span class="st"> </span><span class="kw">runif</span>(<span class="dv">100</span>,<span class="dv">0</span>,pi)
y1 =<span class="st"> </span><span class="kw">sin</span>(x1) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>,<span class="dv">0</span>,<span class="fl">0.1</span>)

x2 =<span class="st"> </span><span class="kw">runif</span>(<span class="dv">100</span>,<span class="op">-</span>pi<span class="op">/</span><span class="dv">2</span>,pi<span class="op">/</span><span class="dv">2</span>)
y2 =<span class="st"> </span><span class="fl">0.4</span> <span class="op">+</span><span class="st"> </span><span class="kw">sin</span>(x2<span class="op">-</span>pi<span class="op">/</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>,<span class="dv">0</span>,<span class="fl">0.1</span>)

<span class="kw">plot</span>(<span class="kw">c</span>(x1,x2),<span class="kw">c</span>(y1,y2))</code></pre></div>
<p>k-means werkt niet echt hier….</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df  =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(x1,x2), <span class="dt">y =</span> <span class="kw">c</span>(y1,y2))
df.cluster =<span class="st"> </span><span class="kw">kmeans</span>(df, <span class="dv">2</span>)
df.cluster

<span class="kw">plot</span>(df<span class="op">$</span>x, df<span class="op">$</span>y, <span class="dt">col =</span> df.cluster<span class="op">$</span>cluster )
<span class="kw">points</span>(df.cluster<span class="op">$</span>centers, <span class="dt">col =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="dt">pch =</span> <span class="dv">8</span>, <span class="dt">cex =</span> <span class="dv">2</span>)</code></pre></div>
<p>De data moet in een matrix zitten om dbscan te kunnen runnen.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X =<span class="st"> </span><span class="kw">cbind</span>(
  <span class="kw">c</span>(x1,x2),
  <span class="kw">c</span>(y1,y2)
  )

db =<span class="st"> </span><span class="kw">dbscan</span>(X, <span class="dt">eps =</span> .<span class="dv">2</span>, <span class="dt">minPts =</span> <span class="dv">4</span>)
db

<span class="kw">pairs</span>(X, <span class="dt">col =</span> db<span class="op">$</span>cluster <span class="op">+</span><span class="st"> </span>1L)

### NOISE POINTS....
X =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(x1, x2,<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>), <span class="kw">c</span>(y1, y2, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>))
db =<span class="st"> </span><span class="kw">dbscan</span>(X, <span class="dt">eps =</span> .<span class="dv">2</span>, <span class="dt">minPts =</span> <span class="dv">4</span>)
db

<span class="kw">pairs</span>(X, <span class="dt">col =</span> db<span class="op">$</span>cluster <span class="op">+</span><span class="st"> </span>1L)</code></pre></div>
<p>Cluster analyse zou je ook voor outlier detecie kunnen gebruiken. Er is een afstand uit te rekenen tot andere punten.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## local outlier factor score
lof =<span class="st"> </span><span class="kw">lof</span>(X, <span class="dt">k =</span> <span class="dv">4</span>)
<span class="kw">pairs</span>(X, <span class="dt">cex =</span> lof)</code></pre></div>
</div>
<div id="hierarchisch-clusteren" class="section level2">
<h2><span class="header-section-number">3.3</span> Hierarchisch clusteren</h2>
<p>Een alternatief voor k-means en DBSCAN is hierarchisch clusteren. Je kan beginnen met 1 cluster waarin alle obeservaties zitten. Iteratief ga je deze cluster opsplitsen in sub clusters tot elke observatie 1 aparte cluster is (Divisive). Of je begint met de situatie dat elke observatie een cluster is en iteratief ga je clusters samenvoegen totdat alle observaties 1 cluster vormen (Aglomerative).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Agglomerative clusrting with hclust
<span class="co"># EEn distance matrix is eerst nodig voordat je het in hclust kan stoppen</span>
hc1 =<span class="st"> </span><span class="kw">dist</span>(
  mtcars, <span class="dt">method =</span> <span class="st">&quot;euclidean&quot;</span>
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">hclust</span>(
    <span class="dt">method =</span> <span class="st">&quot;complete&quot;</span> 
  )

<span class="co"># Plot the obtained dendrogram</span>
<span class="kw">plot</span>(hc1, <span class="dt">cex =</span> <span class="fl">0.6</span>, <span class="dt">hang =</span> <span class="op">-</span><span class="dv">1</span>)</code></pre></div>
<p>Naast een lelijk statisch old school plaatje kan je ook een interactive tree krijgen met visNetwork.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(visNetwork)
<span class="kw">visHclust</span>(iris, <span class="dt">cutree =</span> <span class="dv">3</span>, <span class="dt">colorEdges =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p>Zie voor een kort overzicht ook deze <a href="https://www.r-bloggers.com/how-to-perform-hierarchical-clustering-using-r/">blog</a></p>
<p><br></p>
</div>
</div>
<div id="market-basket-analyse" class="section level1">
<h1><span class="header-section-number">4</span> Market basket analyse</h1>
<p>Met market basket analyse (ook wel association rules mining genoemd) kan je uit “transacties van klanten” vaak voorkomende combinaties of juiste hele “sterke combinaties” van producten bepalen. Hieronder volgt een voorbeeldje op een fictief grocery (boodschappen transacties) data setje.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(arules)

## De meest simpele transactionele data set
trxDF =<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;data/boodschappen.RDs&quot;</span>)

## Transormeer naar een transaction object
Groceries =<span class="st"> </span><span class="kw">as</span>(
  <span class="kw">split</span>(
    trxDF<span class="op">$</span>item,
    trxDF<span class="op">$</span>id
    ),
  <span class="st">&quot;transactions&quot;</span>
)

Groceries

## Visuele Item informatie
<span class="kw">itemFrequencyPlot</span>(Groceries, <span class="dt">topN =</span> <span class="dv">35</span>, <span class="dt">cex.names =</span> <span class="fl">0.75</span>)</code></pre></div>
<p>Nu je de boodschappen als transaction object hebt kan je er market basket rules op los laten met behulp van het a-priori algoritme.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rules &lt;-<span class="st"> </span><span class="kw">apriori</span>(Groceries, <span class="dt">parameter =</span> <span class="kw">list</span>(<span class="dt">supp =</span> <span class="fl">0.001</span>, <span class="dt">conf =</span> <span class="fl">0.8</span>))
rules

## laat enkele regels zien
<span class="kw">inspect</span>(rules[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>])

<span class="kw">inspect</span>( <span class="kw">sort</span>(rules, <span class="dt">by =</span> <span class="st">&quot;support&quot;</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>])

## converteer de rules set naar een data frame
rulesDF =<span class="st"> </span><span class="kw">DATAFRAME</span>(rules)</code></pre></div>
<p>Nu je de regels hebt kan je filteren op regels. Welke regels bevatten bepaalde producten.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rules.subset =<span class="st"> </span><span class="kw">subset</span>(rules, lhs <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;cereals&quot;</span>, <span class="st">&quot;curd&quot;</span>))
rules.subset
<span class="kw">inspect</span>(<span class="kw">head</span>(rules.subset, <span class="dt">n=</span><span class="dv">15</span>))</code></pre></div>
<p>Of als iemand een bepaalde reeks transacties heeft welke regels horen daar bij en welk product kan je dan aanraden.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">PersoonA =<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">id =</span> <span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">3</span>),
  <span class="dt">item2 =</span> <span class="kw">c</span>(<span class="st">&quot;butter&quot;</span>,<span class="st">&quot;curd&quot;</span>,<span class="st">&quot;domestic eggs&quot;</span>)
)

trxs_trans =<span class="st"> </span><span class="kw">as</span>(
  <span class="kw">split</span>(
    PersoonA<span class="op">$</span>item2,
    PersoonA<span class="op">$</span>id
    ),
  <span class="st">&quot;transactions&quot;</span>
)
<span class="kw">inspect</span>(trxs_trans)

rulesMatch &lt;-<span class="st"> </span><span class="kw">is.subset</span>(rules<span class="op">@</span>lhs,trxs_trans)

## er zijn meerdere regels, je zou degene met de hoogste lift kunnen kiezen
<span class="kw">inspect</span>(rules[rulesMatch[,<span class="dv">1</span>]])
<span class="kw">inspect</span>(rules[rulesMatch[,<span class="dv">1</span>]]<span class="op">@</span>rhs)</code></pre></div>
<p>Een ander manier om regels weer te geven is in een network graph, de verzameling regels vormen in feite een netwerk. A –&gt; B, B –&gt; C, D –&gt; B bijvoorbeeld.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(arulesViz)
<span class="kw">plot</span>(<span class="kw">head</span>(<span class="kw">sort</span>(rules, <span class="dt">by =</span> <span class="st">&quot;lift&quot;</span>), <span class="dt">n=</span><span class="dv">50</span>), <span class="dt">method =</span> <span class="st">&quot;graph&quot;</span>, <span class="dt">control=</span><span class="kw">list</span>(<span class="dt">cex=</span>.<span class="dv">8</span>))</code></pre></div>
<div id="interactive-mba-graphs" class="section level2">
<h2><span class="header-section-number">4.1</span> interactive MBA graphs</h2>
<p>You can visualise rules in interactive plotly plots or interactive visNetwork plots. First, an interactive scatter plot of the rules can be made. Each rule is plotted as a point, where the x axis represents the support and the y axis represent the confidence of the rule.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rules &lt;-<span class="st"> </span><span class="kw">apriori</span>(Groceries, <span class="dt">parameter =</span> <span class="kw">list</span>(<span class="dt">supp =</span> <span class="fl">0.001</span>, <span class="dt">conf =</span> <span class="fl">0.8</span>) )
rulesDF =<span class="st"> </span>rules <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">DATAFRAME</span>()

<span class="kw">library</span>(plotly)
<span class="kw">plotly_arules</span>(rules, <span class="dt">max =</span> <span class="dv">2000</span>)
<span class="kw">plotly_arules</span>(rules, <span class="dt">method =</span> <span class="st">&quot;two-key plot&quot;</span>)</code></pre></div>
<p>Secondly, an interactive visNetwork can be created. We need to extract the nodes and edges from the rules object.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(visNetwork)

rules &lt;-<span class="st"> </span><span class="kw">apriori</span>(
  Groceries, 
  <span class="dt">parameter =</span> <span class="kw">list</span>(
    <span class="dt">supp =</span> <span class="fl">0.0001</span>, 
    <span class="dt">conf =</span> <span class="fl">0.1</span>, 
    <span class="dt">minlen =</span> <span class="dv">2</span>,
    <span class="dt">maxlen=</span><span class="dv">2</span>
    )
  )

rulesDF =<span class="st"> </span><span class="kw">head</span>(
  <span class="kw">sort</span>(rules, <span class="dt">by =</span> <span class="st">&quot;lift&quot;</span>),
  <span class="dt">n=</span><span class="dv">250</span>
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">DATAFRAME</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">from =</span> <span class="kw">as.character</span>(LHS),
    <span class="dt">to =</span> <span class="kw">as.character</span>(RHS),
    <span class="dt">value =</span> lift
  )

nodes =<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">id =</span> base<span class="op">::</span><span class="kw">unique</span>(<span class="kw">c</span>(rulesDF<span class="op">$</span>from, rulesDF<span class="op">$</span>to)),
  <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>
) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(
  <span class="dt">title =</span> id
)

<span class="kw">visNetwork</span>(nodes, rulesDF) <span class="op">%&gt;%</span>
<span class="st">   </span><span class="kw">visOptions</span>(<span class="dt">highlightNearest =</span> <span class="ot">TRUE</span>,  <span class="dt">nodesIdSelection =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span>
<span class="st">   </span><span class="kw">visEdges</span>(<span class="dt">smooth =</span> <span class="ot">FALSE</span>) </code></pre></div>
</div>
</div>
<div id="deeplearning" class="section level1">
<h1><span class="header-section-number">5</span> Deeplearning</h1>
<p>In R kan je deeplearning modellen trainen met Keras met een Tensorflow back-end, we geven hier een simpel voorbeeld. Je kan vanuit het <code>keras</code> package tensorflow installeren. Zie ook mijn slides op <a href="https://www.slideshare.net/LonghowLam/keras-on-tensorflow-in-r-python">slideshare</a> voor wat verdere uitleg en bekijk dit ‘fantastische’ boek: <a href="https://www.manning.com/books/deep-learning-with-r">deep learning in R</a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(keras)
## De library keras bevat een makkelijke functie om de tensorflow backend  installeren, 
## dit werkt makkelijk op een linux machine en is niet echt ondersteunt op een windows machine!

<span class="co"># keras::install_keras()</span></code></pre></div>
<div id="een-simpel-model" class="section level2">
<h2><span class="header-section-number">5.1</span> Een simpel model</h2>
<p>We maken met keras eerst een simpel model, 1 hidden layer, niks fancy nog. Download en prepareer eerst de data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">batch_size =<span class="st"> </span><span class="dv">128</span>
num_classes =<span class="st"> </span><span class="dv">10</span>
epochs =<span class="st"> </span><span class="dv">10</span>

<span class="co"># The data, shuffled and split between train and test sets</span>
<span class="kw">c</span>(<span class="kw">c</span>(x_train, y_train), <span class="kw">c</span>(x_test, y_test)) <span class="op">%&lt;-%</span><span class="st"> </span><span class="kw">dataset_mnist</span>()

<span class="kw">dim</span>(x_train)

<span class="co"># We hebben 60.000 plaatjes die 28 bij 28 matrices zijn, bekijk het eerste plaatje</span>
x_train[<span class="dv">1</span>,,]

<span class="co"># en de bijbehorende label</span>
y_train[<span class="dv">1</span>]</code></pre></div>
<p>Zo’n 28 bij 28 plaatje is ook te plotten</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># bekijk plaatje n</span>
n =<span class="st"> </span><span class="dv">2301</span>
m =<span class="st"> </span>x_train[n,,]

<span class="kw">image</span>(m)
## rotate 90 degrees clockwise
m =<span class="st"> </span><span class="kw">t</span>(m)[, <span class="kw">nrow</span>(m)<span class="op">:</span><span class="dv">1</span>]
<span class="kw">image</span>(m)

y_train[n]</code></pre></div>
<p>We gaan nu de data wat reshapen voor het model</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Sla de matrices plat tot een vector (array) van 784 = (28*28)</span>
x_train &lt;-<span class="st"> </span><span class="kw">array_reshape</span>(x_train, <span class="kw">c</span>(<span class="kw">nrow</span>(x_train), <span class="dv">784</span>))
x_test &lt;-<span class="st"> </span><span class="kw">array_reshape</span>(x_test, <span class="kw">c</span>(<span class="kw">nrow</span>(x_test), <span class="dv">784</span>))

<span class="kw">dim</span>(x_train)
x_train[<span class="dv">1</span>,]

<span class="co"># Transform RGB values into [0,1] range</span>
x_train &lt;-<span class="st"> </span>x_train <span class="op">/</span><span class="st"> </span><span class="dv">255</span>
x_test &lt;-<span class="st"> </span>x_test <span class="op">/</span><span class="st"> </span><span class="dv">255</span>

<span class="kw">cat</span>(<span class="kw">nrow</span>(x_train), <span class="st">'train samples</span><span class="ch">\n</span><span class="st">'</span>)
<span class="kw">cat</span>(<span class="kw">nrow</span>(x_test), <span class="st">'test samples</span><span class="ch">\n</span><span class="st">'</span>)

<span class="co"># Convert class vectors to binary class matrices</span>
y_train &lt;-<span class="st"> </span><span class="kw">to_categorical</span>(y_train, num_classes)
y_test &lt;-<span class="st"> </span><span class="kw">to_categorical</span>(y_test, num_classes)</code></pre></div>
<p>Defineer het model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>()
model <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">layer_dense</span>(
    <span class="dt">units =</span> <span class="dv">256</span>, <span class="dt">activation =</span> <span class="st">'relu'</span>, <span class="dt">input_shape =</span> <span class="kw">c</span>(<span class="dv">784</span>)
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">layer_dense</span>(
    <span class="dt">units =</span> <span class="dv">10</span>, <span class="dt">activation =</span> <span class="st">'softmax'</span>
  )

<span class="kw">summary</span>(model)

model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(
  <span class="dt">loss =</span> <span class="st">'categorical_crossentropy'</span>,
  <span class="dt">optimizer =</span> <span class="kw">optimizer_rmsprop</span>(),
  <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">'accuracy'</span>)
)</code></pre></div>
<p>Train en evalueer het model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit model to data</span>
history &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(
  x_train, y_train,
  <span class="dt">batch_size =</span> batch_size,
  <span class="dt">epochs =</span> epochs,
  <span class="dt">verbose =</span> <span class="dv">2</span>,
  <span class="dt">view_metrics =</span> <span class="ot">FALSE</span>,
  <span class="dt">validation_split =</span> <span class="fl">0.2</span>
)

<span class="kw">plot</span>(history)
  
score &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">evaluate</span>(
  x_test, y_test,
  <span class="dt">verbose =</span> <span class="dv">0</span>
)

<span class="co"># Output metrics</span>
<span class="kw">cat</span>(<span class="st">'Test loss:'</span>, score[[<span class="dv">1</span>]], <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)
<span class="kw">cat</span>(<span class="st">'Test accuracy:'</span>, score[[<span class="dv">2</span>]], <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</code></pre></div>
<p>We kunnen nu nog een tweede hidden layer toevoegen met drop outs om overfitten te voorkomen.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>()
model <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">layer_dense</span>(
    <span class="dt">units =</span> <span class="dv">512</span>, <span class="dt">activation =</span> <span class="st">'relu'</span>, <span class="dt">input_shape =</span> <span class="kw">c</span>(<span class="dv">784</span>)
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">layer_dropout</span>(<span class="dt">rate=</span><span class="fl">0.2</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">layer_dense</span>(
    <span class="dt">units =</span> <span class="dv">512</span>, <span class="dt">activation =</span> <span class="st">'relu'</span>, <span class="dt">input_shape =</span> <span class="kw">c</span>(<span class="dv">784</span>)
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">layer_dropout</span>(<span class="dt">rate=</span><span class="fl">0.2</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">10</span>, <span class="dt">activation =</span> <span class="st">'softmax'</span>)

<span class="kw">summary</span>(model)

model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(
  <span class="dt">loss =</span> <span class="st">'categorical_crossentropy'</span>,
  <span class="dt">optimizer =</span> <span class="kw">optimizer_rmsprop</span>(),
  <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">'accuracy'</span>)
)</code></pre></div>
</div>
<div id="convolutional-model." class="section level2">
<h2><span class="header-section-number">5.2</span> Convolutional model.</h2>
<p>Maak nu een meer fancy convolutional deeplearning model, input is nu een array van getallen ipv een plat geslagen vector zoals in vorige voorbeeld. En laten we nu de Fashion MNIST nemen, een leuk alternative voor de cijfers, het zijn kledingplaatjes door zalando beschikbaar gesteld.</p>
<p><a href="https://github.com/zalandoresearch/fashion-mnist">Fashion MNIST</a></p>
<p>Data kan je krijgen door de git repo te clonen. en in data dir zitten gz files die je moet uitpakken, dit zijn binaire files die je mer R kan inlezen, gebruikmakend van de functie <code>readBin</code>.</p>
<p>Er zijn 60.000 train en 10.000 test plaatjes, elk plaatje heeft een label (0-9)</p>
<p>Label Description 0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">### MNIST FASHION helper functie
load_mnist &lt;-<span class="st"> </span><span class="cf">function</span>() {
  load_image_file &lt;-<span class="st"> </span><span class="cf">function</span>(filename) {
    ret =<span class="st"> </span><span class="kw">list</span>()
    f =<span class="st"> </span><span class="kw">file</span>(filename,<span class="st">'rb'</span>)
    <span class="kw">readBin</span>(f,<span class="st">'integer'</span>,<span class="dt">n=</span><span class="dv">1</span>,<span class="dt">size=</span><span class="dv">4</span>,<span class="dt">endian=</span><span class="st">'big'</span>)
    ret<span class="op">$</span>n =<span class="st"> </span><span class="kw">readBin</span>(f,<span class="st">'integer'</span>,<span class="dt">n=</span><span class="dv">1</span>,<span class="dt">size=</span><span class="dv">4</span>,<span class="dt">endian=</span><span class="st">'big'</span>)
    nrow =<span class="st"> </span><span class="kw">readBin</span>(f,<span class="st">'integer'</span>,<span class="dt">n=</span><span class="dv">1</span>,<span class="dt">size=</span><span class="dv">4</span>,<span class="dt">endian=</span><span class="st">'big'</span>)
    ncol =<span class="st"> </span><span class="kw">readBin</span>(f,<span class="st">'integer'</span>,<span class="dt">n=</span><span class="dv">1</span>,<span class="dt">size=</span><span class="dv">4</span>,<span class="dt">endian=</span><span class="st">'big'</span>)
    x =<span class="st"> </span><span class="kw">readBin</span>(f,<span class="st">'integer'</span>,<span class="dt">n=</span>ret<span class="op">$</span>n<span class="op">*</span>nrow<span class="op">*</span>ncol,<span class="dt">size=</span><span class="dv">1</span>,<span class="dt">signed=</span>F)
    ret<span class="op">$</span>x =<span class="st"> </span><span class="kw">matrix</span>(x, <span class="dt">ncol=</span>nrow<span class="op">*</span>ncol, <span class="dt">byrow=</span>T)
    <span class="kw">close</span>(f)
    ret
  }

    load_label_file &lt;-<span class="st"> </span><span class="cf">function</span>(filename) {
    f =<span class="st"> </span><span class="kw">file</span>(filename,<span class="st">'rb'</span>)
    <span class="kw">readBin</span>(f,<span class="st">'integer'</span>,<span class="dt">n=</span><span class="dv">1</span>,<span class="dt">size=</span><span class="dv">4</span>,<span class="dt">endian=</span><span class="st">'big'</span>)
    n =<span class="st"> </span><span class="kw">readBin</span>(f,<span class="st">'integer'</span>,<span class="dt">n=</span><span class="dv">1</span>,<span class="dt">size=</span><span class="dv">4</span>,<span class="dt">endian=</span><span class="st">'big'</span>)
    y =<span class="st"> </span><span class="kw">readBin</span>(f,<span class="st">'integer'</span>,<span class="dt">n=</span>n,<span class="dt">size=</span><span class="dv">1</span>,<span class="dt">signed=</span>F)
    <span class="kw">close</span>(f)
    y
  }
  
  train &lt;&lt;-<span class="st"> </span><span class="kw">load_image_file</span>(<span class="st">'/home/longhowlam/datasets/fashion/train-images-idx3-ubyte'</span>)
  test &lt;&lt;-<span class="st"> </span><span class="kw">load_image_file</span>(<span class="st">'/home/longhowlam/datasets/fashion/t10k-images-idx3-ubyte'</span>)
  
  train<span class="op">$</span>y &lt;&lt;-<span class="st"> </span><span class="kw">load_label_file</span>(<span class="st">'/home/longhowlam/datasets/fashion/train-labels-idx1-ubyte'</span>)
  test<span class="op">$</span>y &lt;&lt;-<span class="st"> </span><span class="kw">load_label_file</span>(<span class="st">'/home/longhowlam/datasets/fashion/t10k-labels-idx1-ubyte'</span>)  
}</code></pre></div>
<p>Nu kan je de data inlezen</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">### Helper functie om een digit (een zalando plaatje) te laten zien
show_digit &lt;-<span class="st"> </span><span class="cf">function</span>(arr784, <span class="dt">col=</span><span class="kw">gray</span>(<span class="dv">12</span><span class="op">:</span><span class="dv">1</span><span class="op">/</span><span class="dv">12</span>), ...) {
  <span class="kw">image</span>(<span class="kw">matrix</span>(arr784, <span class="dt">nrow=</span><span class="dv">28</span>)[,<span class="dv">28</span><span class="op">:</span><span class="dv">1</span>], <span class="dt">col=</span>col, ...)
}

<span class="kw">load_mnist</span>()

<span class="co"># vier plaatjes</span>
<span class="kw">show_digit</span>(train<span class="op">$</span>x[<span class="dv">1816</span>,])
train<span class="op">$</span>y[<span class="dv">1816</span>]

<span class="kw">show_digit</span>(train<span class="op">$</span>x[<span class="dv">116</span>,])
train<span class="op">$</span>y[<span class="dv">116</span>]

<span class="kw">show_digit</span>(train<span class="op">$</span>x[<span class="dv">3116</span>,])
train<span class="op">$</span>y[<span class="dv">3116</span>]

<span class="kw">show_digit</span>(train<span class="op">$</span>x[<span class="dv">1316</span>,])
train<span class="op">$</span>y[<span class="dv">1316</span>]</code></pre></div>
<p>Nu kan je het model trainen, ik heb alleen 1 epoch gezet, dit model duurt een uur op dit laptopje om te trainen. Eerst weer de data goed zetten en wat algemene settings, zoals batch size, dimensie en epochs specificeren….</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">batch_size &lt;-<span class="st"> </span><span class="dv">128</span>
num_classes &lt;-<span class="st"> </span><span class="dv">10</span>
epochs &lt;-<span class="st"> </span><span class="dv">1</span>

<span class="co"># input image dimensions</span>
img_rows &lt;-<span class="st"> </span><span class="dv">28</span>
img_cols &lt;-<span class="st"> </span><span class="dv">28</span>

<span class="co"># the data, shuffled and split between train and test sets</span>
x_train &lt;-<span class="st"> </span>train<span class="op">$</span>x
y_train &lt;-<span class="st"> </span>train<span class="op">$</span>y
x_test &lt;-<span class="st"> </span>test<span class="op">$</span>x
y_test &lt;-<span class="st"> </span>test<span class="op">$</span>y

<span class="kw">dim</span>(x_train) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">nrow</span>(x_train), img_rows, img_cols, <span class="dv">1</span>) 
<span class="kw">dim</span>(x_test) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">nrow</span>(x_test), img_rows, img_cols, <span class="dv">1</span>)
input_shape &lt;-<span class="st"> </span><span class="kw">c</span>(img_rows, img_cols, <span class="dv">1</span>)

x_train &lt;-<span class="st"> </span>x_train <span class="op">/</span><span class="st"> </span><span class="dv">255</span>
x_test &lt;-<span class="st"> </span>x_test <span class="op">/</span><span class="st"> </span><span class="dv">255</span>

<span class="kw">cat</span>(<span class="st">'x_train_shape:'</span>, <span class="kw">dim</span>(x_train), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)
<span class="kw">cat</span>(<span class="kw">nrow</span>(x_train), <span class="st">'train samples</span><span class="ch">\n</span><span class="st">'</span>)
<span class="kw">cat</span>(<span class="kw">nrow</span>(x_test), <span class="st">'test samples</span><span class="ch">\n</span><span class="st">'</span>)

<span class="co"># convert class vectors to binary class matrices</span>
y_train &lt;-<span class="st"> </span><span class="kw">to_categorical</span>(y_train, num_classes)
y_test &lt;-<span class="st"> </span><span class="kw">to_categorical</span>(y_test, num_classes)</code></pre></div>
<p>Maak in keras nu het meer complexere convolutional model, verschillende lagen en pas ook dropout toe.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># define a two layer conv with max pooling  model</span>
model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>()
model <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">layer_conv_2d</span>(
    <span class="dt">filters     =</span> <span class="dv">32</span>, 
    <span class="dt">kernel_size =</span> <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>),
    <span class="dt">activation  =</span> <span class="st">'relu'</span>,
    <span class="dt">input_shape =</span> input_shape
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">layer_conv_2d</span>(
    <span class="dt">filters     =</span> <span class="dv">64</span>,
    <span class="dt">kernel_size =</span> <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>),
    <span class="dt">activation  =</span> <span class="st">'relu'</span>
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">layer_max_pooling_2d</span>(
    <span class="dt">pool_size =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>)
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">layer_dropout</span>(<span class="dt">rate =</span> <span class="fl">0.25</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">layer_conv_2d</span>(
    <span class="dt">filters     =</span> <span class="dv">64</span>,
    <span class="dt">kernel_size =</span> <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>),
    <span class="dt">activation  =</span> <span class="st">'relu'</span>
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">layer_max_pooling_2d</span>(
    <span class="dt">pool_size =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>)
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">layer_flatten</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">layer_dense</span>(
    <span class="dt">units      =</span> <span class="dv">256</span>, 
    <span class="dt">activation =</span> <span class="st">'relu'</span>
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">layer_dropout</span>(<span class="dt">rate =</span> <span class="fl">0.5</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">layer_dense</span>(
    <span class="dt">units =</span> num_classes,
    <span class="dt">activation =</span> <span class="st">'softmax'</span>
  )</code></pre></div>
<p>Bekijk de beknopte smanevatting van het model</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(model)</code></pre></div>
<p>Compileer en train het model</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compile model</span>
model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(
  <span class="dt">loss =</span> loss_categorical_crossentropy,
  <span class="dt">optimizer =</span> <span class="kw">optimizer_adadelta</span>(),
  <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">'accuracy'</span>)
)
<span class="kw">lm</span>()
<span class="co"># train and evaluate</span>
model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(
  x_train, y_train,
  <span class="dt">batch_size =</span> batch_size,
  <span class="dt">epochs =</span> epochs,
  <span class="dt">verbose =</span> <span class="dv">1</span>,
  <span class="dt">validation_data =</span> <span class="kw">list</span>(x_test, y_test)
)

scores &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">evaluate</span>(
  x_test, y_test, <span class="dt">verbose =</span> <span class="dv">0</span>
)

<span class="kw">cat</span>(<span class="st">'Test loss:'</span>, scores[[<span class="dv">1</span>]], <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)
<span class="kw">cat</span>(<span class="st">'Test accuracy:'</span>, scores[[<span class="dv">2</span>]], <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</code></pre></div>
<p><br></p>
</div>
</div>
<div id="tiijdreeks-modellen-met-prophet" class="section level1">
<h1><span class="header-section-number">6</span> Tiijdreeks modellen met prophet</h1>
<hr />
<p>Prophet is een facebook package voor tijdreeks data, die zij zelf ook intern gebruiken. In plaats van de traditionale manier van tijdreeks modellen maken met ARIMA, worden zogenaamde additieve regressie modellen gefit. Zie hier de <a href="https://peerj.com/preprints/3190.pdf">paper</a>.</p>
<p>Hier een voorbeeld op gescrapte Billy verkopen van de Ikea website</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(prophet)

## dagelijke ikea billy verkoop data
DailyBilly =<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;data/DailyBilly.RDs&quot;</span>)
<span class="kw">ggplot</span>(DailyBilly, <span class="kw">aes</span>(<span class="dt">x=</span>ds, <span class="dt">y =</span> y)) <span class="op">+</span><span class="st">  </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">col=</span><span class="st">&quot;grey&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.85</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_x_date</span>(<span class="dt">breaks =</span> <span class="st">&quot;1 month&quot;</span>)

## billy forcast, fit eerst het model
BillyF =<span class="st"> </span><span class="kw">prophet</span>(DailyBilly)

## dan aantal dagen vooruit maken waarop je gaat forecastem
future =<span class="st"> </span><span class="kw">make_future_dataframe</span>(BillyF, <span class="dt">periods =</span> <span class="dv">90</span>)
forecast =<span class="st"> </span><span class="kw">predict</span>(BillyF, future)

<span class="kw">plot</span>(BillyF, forecast)</code></pre></div>
<p>Bovenstaande werkt nog niet echt….. We zetten seasonality aan.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## billy forcast, fit eerst het model
BillyF =<span class="st"> </span><span class="kw">prophet</span>(
  DailyBilly,
  <span class="dt">weekly.seasonality =</span> <span class="ot">TRUE</span> ,
  <span class="dt">yearly.seasonality =</span> <span class="ot">TRUE</span>, 
  <span class="dt">daily.seasonality =</span> <span class="ot">FALSE</span>
)

## dan aantal dagen vooruit maken waarop je gaat forecastem
future =<span class="st"> </span><span class="kw">make_future_dataframe</span>(BillyF, <span class="dt">periods =</span> <span class="dv">90</span>)
forecast =<span class="st"> </span><span class="kw">predict</span>(BillyF, future)

<span class="kw">plot</span>(BillyF, forecast)
<span class="kw">prophet_plot_components</span>(BillyF, forecast)</code></pre></div>
<p>Met prohet kan je ook externe regressors toevoegen aan het model. Laten we eens wat KNMI data bekijken (Wind, temperatauur, zonnestraling en neerslag)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(readr)
<span class="kw">library</span>(ggplot2)

KNMI_<span class="dv">20180101</span> =<span class="st"> </span><span class="kw">read_csv</span>(
   <span class="st">&quot;data/KNMI_20180101.txt&quot;</span>, 
   <span class="dt">col_types =</span> <span class="kw">cols</span>(<span class="dt">YYYYMMDD =</span> <span class="kw">col_date</span>(<span class="dt">format =</span> <span class="st">&quot;%Y%m%d&quot;</span>))
)

DailyBilly2 =<span class="st"> </span>DailyBilly <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">left_join</span>(
    KNMI_<span class="dv">20180101</span>, 
    <span class="dt">by =</span> <span class="kw">c</span>(<span class="st">&quot;ds&quot;</span> =<span class="st"> &quot;YYYYMMDD&quot;</span>)
  )</code></pre></div>
<p>Check eerst even of er wat correlatie is tussen y (de billy verkopen) en KNMI variabelen.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(corrplot)
M =<span class="st"> </span><span class="kw">cor</span>(
  DailyBilly2 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">select</span>(y, FHVEC, FHX, TG, TN, Q, RH)
  )

<span class="kw">corrplot</span>(
  M, 
  <span class="dt">method =</span> <span class="st">&quot;ellipse&quot;</span>,
  <span class="dt">diag =</span> <span class="ot">FALSE</span>,
  <span class="dt">addCoef.col =</span> <span class="st">&quot;black&quot;</span>
)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## het model moet nog niet gefit worden voordat je de regressor er aan toevoegt.
BillyF =<span class="st"> </span><span class="kw">prophet</span>(
  <span class="dt">weekly.seasonality =</span> <span class="ot">TRUE</span> ,
  <span class="dt">yearly.seasonality =</span> <span class="ot">TRUE</span>, 
  <span class="dt">daily.seasonality =</span> <span class="ot">TRUE</span>,
  <span class="dt">fit =</span> <span class="ot">FALSE</span>
)  <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_regressor</span>(<span class="st">&quot;TG&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">fit.prophet</span>(<span class="dt">df =</span> DailyBilly2)

## voorspel Billy Verkopen voor 10 graden C en 25 graden C.
future =<span class="st"> </span><span class="kw">make_future_dataframe</span>(BillyF, <span class="dt">periods =</span> <span class="dv">30</span>)
future<span class="op">$</span>TG =<span class="st"> </span><span class="dv">100</span>
future =<span class="st"> </span>future <span class="op">%&gt;%</span><span class="st">  </span><span class="kw">filter</span>(ds <span class="op">&gt;</span><span class="st"> &quot;2017-10-01&quot;</span>)
forecast1 =<span class="st"> </span><span class="kw">predict</span>(BillyF, future)
future<span class="op">$</span>TG =<span class="st"> </span><span class="dv">250</span>
forecast2 =<span class="st"> </span><span class="kw">predict</span>(BillyF, future)

### zet de twee voorspellingen uit.
<span class="kw">ggplot</span>(forecast1, <span class="kw">aes</span>(ds,yhat)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> forecast2, <span class="kw">aes</span>(ds,yhat), <span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
</div>
<div id="the-mlr-package" class="section level1">
<h1><span class="header-section-number">7</span> The mlr package</h1>
<hr />
<p>Met het <code>mlr</code> package kan je makkelijk verschillende modellen trainen en testen op een meer uniforme manier. In R hebben alle machine learning technieken net weer verschillende aanroepen en de uitkomst is vaak een object met net steeds weer andere componenten. Dit zagen we bijvoorbeeld in de bovenstaande code voor ranger en xgboost.</p>
<p>Met het <code>mlr</code> package kan je dit uniform stroomlijnen. Het maken van een predictive model (welk model dan ook) bestaat altijd uit een aantal stappen. Bijvoorbeeld:</p>
<ul>
<li>specificeren van de target,</li>
<li>specificeren van inputs,</li>
<li>specificeren van variabelen die je niet wilt gebruiken,</li>
<li>splitsen data,</li>
<li>het model / algoritme</li>
</ul>
<p>Deze zijn te beschrijven en uit te voeren in mlr.</p>
<p>We gebruiken de <code>titanic</code> data set als test in <code>mlr</code> en doorlopen een aantal stappen on een aantal modellen te benchmarken. De modellen die we willen benchmarken zijn:</p>
<ul>
<li>neuraal netwerk,</li>
<li>gradient boosting,</li>
<li>random forest</li>
<li>xgboost</li>
<li>decision tree,</li>
<li>logistic regression via glmnet</li>
</ul>
<div id="specificeren-van-technieken-en-hun-opties" class="section level2">
<h2><span class="header-section-number">7.1</span> specificeren van technieken en hun opties</h2>
<p>In mlr kan je een aantal algemene parameters weergeven.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## parameters die geen beschrijving hebben willen we ook kunnen opgeven
<span class="kw">configureMlr</span>(<span class="dt">on.par.without.desc =</span> <span class="st">&quot;warn&quot;</span>)

## we kijken naar maximaal dertig variabelen
n.importance =<span class="st"> </span><span class="dv">30</span>

## voorspel type, we willen kansen uitrekenen
ptype =<span class="st"> &quot;prob&quot;</span>

## aantal crossvalidation splitsingen
N_CV_iter =<span class="st"> </span><span class="dv">10</span></code></pre></div>
<p>Naast algemene parameters, heeft elk model bepaalde parameters die je kan zetten. Dit hoeft niet, dan worden default waarden gekozen.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">parameters_rf =<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">num.trees  =</span> <span class="dv">500</span>
)

parameters_rpart =<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">cp=</span><span class="fl">0.0001</span>
)

parameters_glmnet =<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">alpha  =</span> <span class="dv">1</span>
)

parameters_NN =<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">hidden =</span> <span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">15</span>)
)

parameters_xgboost =<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">nrounds  =</span> <span class="dv">5</span>,
  <span class="dt">max.depth =</span> <span class="dv">7</span>
)</code></pre></div>
<p>Maak nu een Lijst van modelen (ook wel learners genomed) die je wilt trainen op je data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">RF_Learner =<span class="st"> </span><span class="kw">makeLearner</span>(
  <span class="st">&quot;classif.ranger&quot;</span>,
  <span class="dt">predict.type =</span> ptype,
  <span class="dt">par.vals =</span> parameters_rf
)

xgboost_Learner =<span class="st"> </span><span class="kw">makeLearner</span>(
  <span class="st">&quot;classif.xgboost&quot;</span>,
  <span class="dt">predict.type =</span> ptype,
  <span class="dt">par.vals =</span> parameters_xgboost
)

rpart_Learner =<span class="st"> </span><span class="kw">makeLearner</span>(
  <span class="st">&quot;classif.rpart&quot;</span>,
  <span class="dt">predict.type =</span> ptype,
  <span class="dt">par.vals =</span> parameters_rpart
)

binomial_Learner =<span class="st"> </span><span class="kw">makeLearner</span>(
  <span class="st">&quot;classif.binomial&quot;</span>,
  <span class="dt">predict.type =</span> ptype
)

glmnet_Learner =<span class="st"> </span><span class="kw">makeLearner</span>(
  <span class="st">&quot;classif.cvglmnet&quot;</span>, 
  <span class="dt">predict.type =</span> ptype,
  <span class="dt">par.vals =</span> parameters_glmnet
)

h2ogbm_Learner =<span class="st"> </span><span class="kw">makeLearner</span>(
  <span class="st">&quot;classif.h2o.gbm&quot;</span>, 
  <span class="dt">predict.type =</span> <span class="st">&quot;prob&quot;</span>
)

h2oNN_Learner =<span class="st"> </span><span class="kw">makeLearner</span>(
  <span class="st">&quot;classif.h2o.deeplearning&quot;</span>,
  <span class="dt">predict.type =</span> ptype,
  <span class="dt">par.vals =</span> parameters_NN
)

## lijst van de learners
learners =<span class="st"> </span><span class="kw">list</span>(
  rpart_Learner,
  RF_Learner,
  binomial_Learner,
  glmnet_Learner,
  h2ogbm_Learner,
  h2oNN_Learner
)</code></pre></div>
<p>Als je categorische variabelen in je voorspel model wilt gebruiken eist het mlr package dat ze <code>factor</code> zijn. En in het geval van een classificatie probleem moet de target variabele ook een factor variabele zijn.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ABT =<span class="st"> </span>titanic_train
ABT<span class="op">$</span>Target =<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">ifelse</span>(ABT<span class="op">$</span>Survived <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;N&quot;</span>, <span class="st">&quot;Y&quot;</span>))
ABT =<span class="st"> </span>ABT <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate_if</span>(is.character, as.factor)</code></pre></div>
</div>
<div id="imputeren-van-missende-waarden" class="section level2">
<h2><span class="header-section-number">7.2</span> Imputeren van missende waarden</h2>
<p>Als er missende waarden zijn kan je mlr deze laten imputeren door een bepaalde waarde.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">impObject =<span class="st"> </span><span class="kw">impute</span>(
  ABT, 
  <span class="dt">classes =</span> <span class="kw">list</span>(
    <span class="dt">integer =</span> <span class="kw">imputeMean</span>(),
    <span class="dt">numeric =</span> <span class="kw">imputeMean</span>(),
    <span class="dt">factor =</span> <span class="kw">imputeMode</span>()
    ),
  <span class="dt">dummy.classes =</span> <span class="st">&quot;integer&quot;</span>
)

ABT =<span class="st"> </span>impObject<span class="op">$</span>data</code></pre></div>
</div>
<div id="het-aanmaken-van-een-task" class="section level2">
<h2><span class="header-section-number">7.3</span> Het aanmaken van een task</h2>
<p>Maak nu een ‘task’ aan waarin je de data, de inputs en de target specificeert. Een classificatie taak voor categorische target of een regressie task voor een numerieke target.</p>
<p>Wat wil je modelleren: Kans op level “Y”, dan dien je positive = “Y” op te geven. Bij een binair target met Y en N levels wordt namelijk standaard “N”gebruik (alfabetisch)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">classify.task =<span class="st"> </span><span class="kw">makeClassifTask</span>(<span class="dt">id =</span> <span class="st">&quot;Titanic&quot;</span>, <span class="dt">data =</span> ABT, <span class="dt">target =</span> <span class="st">&quot;Target&quot;</span>, <span class="dt">positive =</span> <span class="st">&quot;Y&quot;</span>)

## Overzicht van de taak en kolom informatie

<span class="kw">print</span>(classify.task)
<span class="kw">getTaskDescription</span>(classify.task)
<span class="kw">summarizeColumns</span>(classify.task)</code></pre></div>
</div>
<div id="variablen-hard-uitsluiten" class="section level2">
<h2><span class="header-section-number">7.4</span> Variablen hard uitsluiten</h2>
<p>Soms zijn er variabelen die je niet wilt meenemen in je model. Deze kan je hard uitsluiten.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">vars.to.drop =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Name&quot;</span>, <span class="st">&quot;Survived&quot;</span>, <span class="st">&quot;Ticket&quot;</span>)

classify.task =<span class="st"> </span><span class="kw">dropFeatures</span>(classify.task, vars.to.drop )

## Weghalen van (bijna) constante variabelen 

## Je kan ook 'bijna' constante variabelen weghalen: perc iets hoger zetten
classify.task =<span class="st"> </span><span class="kw">removeConstantFeatures</span>(classify.task, <span class="dt">perc =</span> <span class="fl">0.01</span>)
classify.task</code></pre></div>
<p>Zeldzame levels van factors samenvoegen. Het is gebruikelijk om zeldzame levels te verwijderen of te mergen</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">classify.task =<span class="st"> </span><span class="kw">mergeSmallFactorLevels</span> (classify.task, <span class="dt">min.perc =</span> <span class="fl">0.02</span>,  <span class="dt">new.level =</span> <span class="st">&quot;.merged&quot;</span>)
<span class="kw">summarizeColumns</span>(classify.task)</code></pre></div>
<p>Welke features hebben een effect op de target? Je kan de predictive power meten per input variable. Univariate, dus per feature kan je met mlr de relatie met de target berekenen.</p>
<p>Onderliggend heb je Rweka en Rjava dingen nodig daar kan je op linux wat issues mee krijgen. Dingen de je kan doen:</p>
<ul>
<li>run in een shell: <code>sudo R CMD javareconf</code> en doe</li>
<li>sudo rstudio-server stop</li>
<li>export LD_LIBRARY_PATH=/usr/lib/jvm/jre/lib/amd64:/usr/lib/jvm/jre/lib/amd64/default</li>
<li>sudo rstudio-server start</li>
</ul>
<p>Zie ook <a href="https://stackoverflow.com/questions/28462302/libjvm-so-cannot-open-shared-object-file-no-such-file-or-directory">stack overflow</a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Feature selection 
fv =<span class="st"> </span><span class="kw">generateFilterValuesData</span>(classify.task,  <span class="dt">method =</span> <span class="kw">c</span>(<span class="st">&quot;information.gain&quot;</span>, <span class="st">&quot;chi.squared&quot;</span>))

## display en plot importance

importance =<span class="st"> </span>fv<span class="op">$</span>data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(<span class="kw">desc</span>(information.gain))
<span class="kw">head</span>(importance, <span class="dt">n =</span> n.importance)
<span class="kw">plotFilterValues</span>(fv, <span class="dt">n.show =</span> <span class="dv">2</span><span class="op">*</span>n.importance)</code></pre></div>
<p>laat nog eens variabelen weg die helemaal niks doen.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">vars.to.drop =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;PassengerId&quot;</span>, <span class="st">&quot;Parch&quot;</span>, <span class="st">&quot;SibSp&quot;</span>)
classify.task =<span class="st"> </span><span class="kw">dropFeatures</span>(classify.task, vars.to.drop )</code></pre></div>
</div>
<div id="sample-schema" class="section level2">
<h2><span class="header-section-number">7.5</span> Sample schema</h2>
<p>Met mlr kan je data splitsen, niet alleen in train / test maar ook cross validation. Dit heet een sample schema.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">SampleStrageyHO =<span class="st"> </span><span class="kw">makeResampleDesc</span>(<span class="st">&quot;Holdout&quot;</span>, <span class="dt">split=</span><span class="fl">0.75</span>)
SampleStrageyCV =<span class="st"> </span><span class="kw">makeResampleDesc</span>(<span class="st">&quot;CV&quot;</span>, <span class="dt">iters =</span> N_CV_iter)</code></pre></div>
</div>
<div id="uitvoeren-machine-learning-becnhamrk" class="section level2">
<h2><span class="header-section-number">7.6</span> uitvoeren machine learning becnhamrk</h2>
<p>Nu heb je de diverse stappen gespecificeerd en kan je een benchmark uitvoeren voor de verschillende learners,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">br1 =<span class="st"> </span>mlr<span class="op">::</span><span class="kw">benchmark</span>(learners, classify.task, SampleStrageyHO, <span class="dt">measures =</span> <span class="kw">list</span>(mlr<span class="op">::</span>mmce, mlr<span class="op">::</span>auc, mlr<span class="op">::</span>f1))</code></pre></div>
</div>
<div id="vergelijking-machine-learning-modellen" class="section level2">
<h2><span class="header-section-number">7.7</span> Vergelijking machine learning modellen</h2>
<p>Na het trainen van de modellen met mlr heb je een zogenaamde benchmark object, die kan je printen en plotten om wat meer info te krijgen.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data.frame</span>(br1) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(<span class="kw">desc</span>(auc))
<span class="kw">plotBMRSummary</span>(br1, <span class="dt">measure =</span> mlr<span class="op">::</span>auc)</code></pre></div>
<div id="roc-curves" class="section level3">
<h3><span class="header-section-number">7.7.1</span> ROC curves</h3>
<p>In het benchmark object zit eigenlijk nog veel meer data. Met onderstaande code pluk je alle stukjes data per model uit om deze vervolgens in een ROC grafiek te zetten.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">NModels =<span class="st"> </span><span class="kw">length</span>(br1<span class="op">$</span>results<span class="op">$</span>Titanic)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>NModels)
{
  tmp2  =<span class="st"> </span>br1<span class="op">$</span>results<span class="op">$</span>Titanic[[i]]<span class="op">$</span>pred<span class="op">$</span>data
  rocResultTEST =<span class="st"> </span><span class="kw">roc</span>(truth  <span class="op">~</span><span class="st"> </span>prob.Y, <span class="dt">data =</span> tmp2 )
  <span class="cf">if</span>(i<span class="op">==</span><span class="dv">1</span>)
  {
    <span class="kw">plot</span>(rocResultTEST, <span class="dt">col=</span>i)
  }<span class="cf">else</span>{
    <span class="kw">plot</span>(rocResultTEST, <span class="dt">col=</span>i, <span class="dt">add=</span><span class="ot">TRUE</span>)
  }
}

<span class="kw">legend</span>( <span class="fl">0.6</span>,<span class="fl">0.6</span>, <span class="kw">names</span>(br1<span class="op">$</span>results<span class="op">$</span>Titanic), <span class="dt">col=</span><span class="dv">1</span><span class="op">:</span>NModels,<span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">title</span>(<span class="st">&quot;Titanic model&quot;</span>)</code></pre></div>
</div>
<div id="model-gebruiken-om-te-scoren" class="section level3">
<h3><span class="header-section-number">7.7.2</span> model gebruiken om te scoren</h3>
<p>Als je een benchmark hebt gedaan heb je al de getrainde modellen in het benchmark object zitten. Die kan je al gebruiken om een data set te scoren. je dient dit model er wel ‘eerst uit te halen’.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## haal model er uit
titanmodel =<span class="st"> </span>br1<span class="op">$</span>results<span class="op">$</span>Titanic<span class="op">$</span>classif.ranger<span class="op">$</span>models[[<span class="dv">1</span>]]

## dit zijn de feauteures in het model
FT =<span class="st"> </span>titanmodel<span class="op">$</span>features

## Maak even een score set van de ABT met alleen de features
ScoreSet =<span class="st"> </span>ABT[, FT]

outpredict =<span class="st"> </span><span class="kw">predict</span>(titanmodel, <span class="dt">newdata =</span> ScoreSet)
outpredict</code></pre></div>
<p><br></p>
<p>Let’s test our learned R skills on www.kahoot.it</p>
<p>EINDE SESSIE</p>
</div>
</div>
</div>
</section>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
